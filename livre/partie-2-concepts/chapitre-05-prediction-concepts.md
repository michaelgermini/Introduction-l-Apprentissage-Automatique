# Chapitre 5 : Pr√©diction - Concepts de Base

## üìö Introduction

Ce chapitre √©tablit les fondements th√©oriques de la pr√©diction statistique, qui est au c≈ìur du machine learning supervis√©.

---

## 5.1 Cadre G√©n√©ral

### Probl√®me de Pr√©diction

Nous avons :
- **X** : variable d'entr√©e (features)
- **Y** : variable de sortie (target)
- **Distribution conjointe** P(X, Y)

**Objectif** : Construire un pr√©dicteur f : X ‚Üí Y qui minimise une fonction de perte L.

**Risque** :
```
R(f) = ùîº[L(Y, f(X))]
```

### Fonctions de Perte Courantes

**R√©gression** :
- Erreur quadratique : L(y, ≈∑) = (y - ≈∑)¬≤
- Erreur absolue : L(y, ≈∑) = |y - ≈∑|

**Classification** :
- Perte 0-1 : L(y, ≈∑) = 1_{y ‚â† ≈∑}
- Log-loss : L(y, ≈∑) = -log P(Y = y|X)

---

## 5.2 Pr√©dicteur de Bayes

### Th√©or√®me

Le **pr√©dicteur de Bayes** minimise le risque :

**R√©gression** (L quadratique) :
```
f*(x) = ùîº[Y|X = x]
```

**Classification** (perte 0-1) :
```
f*(x) = argmax_y P(Y = y|X = x)
```

### Risque de Bayes

Le **risque de Bayes** R* = R(f*) est le risque minimal atteignable.

**Erreur irr√©ductible** : M√™me le meilleur mod√®le ne peut pas faire mieux que R*.

---

## 5.3 Exemples : Approche Bas√©e sur des Mod√®les

### 5.3.1 Mod√®les Gaussiens et Na√Øve Bayes

#### Classification avec Loi Normale

Si X|Y = k ~ N(Œº‚Çñ, Œ£) :
```
P(Y = k|X = x) ‚àù œÄ‚Çñ ¬∑ exp(-¬Ω(x - Œº‚Çñ)·µÄŒ£‚Åª¬π(x - Œº‚Çñ))
```

**Na√Øve Bayes** : Suppose l'ind√©pendance conditionnelle :
```
P(X|Y = k) = ‚àè‚±º P(X‚±º|Y = k)
```

```python
from sklearn.naive_bayes import GaussianNB

# Donn√©es
X_train, y_train = ...  # Features et labels

# Mod√®le
model = GaussianNB()
model.fit(X_train, y_train)

# Pr√©diction
y_pred = model.predict(X_test)
```

### 5.3.2 R√©gression par Noyaux

```
fÃÇ(x) = Œ£·µ¢ w·µ¢(x) y·µ¢
```

o√π les poids d√©pendent de la distance :
```
w·µ¢(x) = K((x - x·µ¢)/h) / Œ£‚±º K((x - x‚±º)/h)
```

---

## 5.4 Minimisation du Risque Empirique

### 5.4.1 Principes G√©n√©raux

On n'a pas acc√®s √† P(X, Y), mais aux donn√©es (x‚ÇÅ, y‚ÇÅ), ..., (x‚Çô, y‚Çô).

**Risque empirique** :
```
RÃÇ(f) = (1/n) Œ£·µ¢ L(y·µ¢, f(x·µ¢))
```

**ERM** (Empirical Risk Minimization) :
```
fÃÇ = argmin_{f ‚àà ‚Ñ±} RÃÇ(f)
```

### 5.4.2 Biais et Variance

**D√©composition de l'erreur** :
```
ùîº[L(Y, fÃÇ(X))] = R* + Approximation + Estimation
```

- **R*** : Risque de Bayes (irr√©ductible)
- **Approximation** : inf_{f ‚àà ‚Ñ±} R(f) - R*
- **Estimation** : R(fÃÇ) - inf_{f ‚àà ‚Ñ±} R(f)

---

## 5.5 √âvaluation de l'Erreur

### 5.5.1 Erreur de G√©n√©ralisation

**Probl√®me** : RÃÇ(fÃÇ) est optimiste car fÃÇ est ajust√© sur les m√™mes donn√©es.

**Erreur de g√©n√©ralisation** : √âvaluer sur de nouvelles donn√©es.

### 5.5.2 Validation Crois√©e

#### Hold-out

```
Training set (70%) ‚Üí Ajuster le mod√®le
Test set (30%) ‚Üí √âvaluer
```

#### K-Fold Cross-Validation

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print(f"CV Score: {scores.mean():.3f} ¬± {scores.std():.3f}")
```

#### Leave-One-Out (LOO)

```
Pour i = 1 √† n:
    Entra√Æner sur {1, ..., n} \ {i}
    Tester sur i
```

**Erreur LOOCV** :
```
CV = (1/n) Œ£·µ¢ L(y·µ¢, fÃÇ^{(-i)}(x·µ¢))
```

---

## üí° Points Cl√©s

1. **Pr√©dicteur de Bayes** : Optimal en th√©orie
2. **ERM** : Minimiser l'erreur empirique
3. **Compromis biais-variance** : Choix de la classe de fonctions ‚Ñ±
4. **Validation crois√©e** : Estimation non biais√©e de l'erreur

---

[‚¨ÖÔ∏è Chapitre pr√©c√©dent](./chapitre-04-biais-variance.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](./chapitre-06-noyaux.md)

