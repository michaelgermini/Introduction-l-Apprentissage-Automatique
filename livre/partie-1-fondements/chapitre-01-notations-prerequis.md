# Chapitre 1 : Notations GÃ©nÃ©rales et PrÃ©requis MathÃ©matiques

## ğŸ“š Introduction

Ce chapitre Ã©tablit les fondations mathÃ©matiques nÃ©cessaires pour comprendre l'apprentissage automatique. Nous couvrons les notations utilisÃ©es tout au long du livre et rÃ©visons les concepts essentiels d'algÃ¨bre linÃ©aire, de topologie, de calcul diffÃ©rentiel et de thÃ©orie des probabilitÃ©s.

---

## 1.1 AlgÃ¨bre LinÃ©aire

### 1.1.1 Ensembles et Fonctions

#### Notations de Base

- **â„** : Ensemble des nombres rÃ©els
- **â„â¿** : Espace vectoriel euclidien de dimension n
- **â„áµË£â¿** : Ensemble des matrices rÃ©elles mÃ—n
- **â„•** : Ensemble des entiers naturels {0, 1, 2, ...}
- **â„¤** : Ensemble des entiers relatifs

#### Fonctions

Une **fonction** f : A â†’ B associe Ã  chaque Ã©lÃ©ment de A un unique Ã©lÃ©ment de B.

- **Domaine** : L'ensemble A
- **Codomaine** : L'ensemble B
- **Image** : L'ensemble {f(x) | x âˆˆ A}

**Exemple** :
```
f : â„Â² â†’ â„
f(xâ‚, xâ‚‚) = xâ‚Â² + xâ‚‚Â²
```

### 1.1.2 Vecteurs

Un **vecteur** dans â„â¿ est un n-uplet de nombres rÃ©els :

```
v = [vâ‚, vâ‚‚, ..., vâ‚™]áµ€
```

#### OpÃ©rations sur les Vecteurs

1. **Addition** : u + v = [uâ‚+vâ‚, uâ‚‚+vâ‚‚, ..., uâ‚™+vâ‚™]áµ€

2. **Multiplication scalaire** : Î±v = [Î±vâ‚, Î±vâ‚‚, ..., Î±vâ‚™]áµ€

3. **Produit scalaire** :
   ```
   âŸ¨u, vâŸ© = uáµ€v = Î£áµ¢ uáµ¢váµ¢
   ```

4. **Norme euclidienne** :
   ```
   â€–vâ€– = âˆš(váµ€v) = âˆš(Î£áµ¢ váµ¢Â²)
   ```

#### PropriÃ©tÃ©s Importantes

- **InÃ©galitÃ© de Cauchy-Schwarz** : |âŸ¨u, vâŸ©| â‰¤ â€–uâ€– Â· â€–vâ€–
- **InÃ©galitÃ© triangulaire** : â€–u + vâ€– â‰¤ â€–uâ€– + â€–vâ€–

**Exemple Pratique** :
```python
import numpy as np

# Vecteurs
u = np.array([1, 2, 3])
v = np.array([4, 5, 6])

# Produit scalaire
dot_product = np.dot(u, v)  # 32

# Norme
norm_u = np.linalg.norm(u)  # âˆš14 â‰ˆ 3.74
```

### 1.1.3 Matrices

Une **matrice** A âˆˆ â„áµË£â¿ est un tableau rectangulaire de m lignes et n colonnes :

```
A = [aâ‚â‚  aâ‚â‚‚  ...  aâ‚â‚™]
    [aâ‚‚â‚  aâ‚‚â‚‚  ...  aâ‚‚â‚™]
    [...  ...  ...  ...]
    [aáµâ‚  aáµâ‚‚  ...  aáµâ‚™]
```

#### OpÃ©rations Matricielles

1. **Multiplication matricielle** : Si A âˆˆ â„áµË£â¿ et B âˆˆ â„â¿Ë£áµ–, alors C = AB âˆˆ â„áµË£áµ– avec :
   ```
   cáµ¢â±¼ = Î£â‚– aáµ¢â‚– bâ‚–â±¼
   ```

2. **TransposÃ©e** : (Aáµ€)áµ¢â±¼ = Aâ±¼áµ¢

3. **Trace** : tr(A) = Î£áµ¢ aáµ¢áµ¢ (somme des Ã©lÃ©ments diagonaux)

4. **DÃ©terminant** : det(A) pour les matrices carrÃ©es

#### Matrices SpÃ©ciales

- **Matrice identitÃ©** I : Iáµ¢â±¼ = 1 si i=j, 0 sinon
- **Matrice diagonale** : Aáµ¢â±¼ = 0 si iâ‰ j
- **Matrice symÃ©trique** : A = Aáµ€
- **Matrice orthogonale** : Aáµ€A = I

#### Valeurs Propres et Vecteurs Propres

Pour une matrice carrÃ©e A âˆˆ â„â¿Ë£â¿ :
- Î» est une **valeur propre** de A
- v est un **vecteur propre** associÃ© Ã  Î»
- Si Av = Î»v

**PropriÃ©tÃ©s** :
- Une matrice symÃ©trique a des valeurs propres rÃ©elles
- Les vecteurs propres d'une matrice symÃ©trique sont orthogonaux

**Exemple** :
```python
import numpy as np

A = np.array([[4, 2], [2, 3]])

# Calcul des valeurs et vecteurs propres
eigenvalues, eigenvectors = np.linalg.eig(A)
print("Valeurs propres:", eigenvalues)
print("Vecteurs propres:\n", eigenvectors)
```

### 1.1.4 Applications MultilinÃ©aires

Une **application bilinÃ©aire** B : â„â¿ Ã— â„â¿ â†’ â„ est linÃ©aire en chaque argument :

```
B(Î±u + Î²v, w) = Î±B(u,w) + Î²B(v,w)
B(u, Î±v + Î²w) = Î±B(u,v) + Î²B(u,w)
```

**Exemple** : Le produit scalaire âŸ¨u, vâŸ© = uáµ€v est bilinÃ©aire.

---

## 1.2 Topologie

### 1.2.1 Ensembles Ouverts et FermÃ©s dans â„áµˆ

#### Boule Ouverte

La **boule ouverte** de centre x et rayon r > 0 :
```
B(x, r) = {y âˆˆ â„áµˆ : â€–y - xâ€– < r}
```

#### Ensemble Ouvert

Un ensemble U âŠ‚ â„áµˆ est **ouvert** si pour tout x âˆˆ U, il existe r > 0 tel que B(x, r) âŠ‚ U.

#### Ensemble FermÃ©

Un ensemble F âŠ‚ â„áµˆ est **fermÃ©** si son complÃ©ment â„áµˆ \ F est ouvert.

**Exemples** :
- (a, b) est ouvert dans â„
- [a, b] est fermÃ© dans â„
- â„â¿ et âˆ… sont Ã  la fois ouverts et fermÃ©s

### 1.2.2 Ensembles Compacts

Un ensemble K âŠ‚ â„áµˆ est **compact** s'il est fermÃ© et bornÃ©.

**ThÃ©orÃ¨me de Heine-Borel** : Dans â„áµˆ, un ensemble est compact si et seulement si il est fermÃ© et bornÃ©.

**Importance en ML** : Les ensembles compacts garantissent l'existence de minima pour les fonctions continues (thÃ©orÃ¨me de Weierstrass).

### 1.2.3 Espaces MÃ©triques

Un **espace mÃ©trique** est un ensemble X muni d'une distance d : X Ã— X â†’ â„â‚Š telle que :

1. d(x, y) = 0 âŸº x = y
2. d(x, y) = d(y, x) (symÃ©trie)
3. d(x, z) â‰¤ d(x, y) + d(y, z) (inÃ©galitÃ© triangulaire)

**Exemples de distances** :
- Distance euclidienne : d(x, y) = â€–x - yâ€–
- Distance de Manhattan : d(x, y) = Î£áµ¢ |xáµ¢ - yáµ¢|
- Distance de Tchebychev : d(x, y) = maxáµ¢ |xáµ¢ - yáµ¢|

---

## 1.3 Calcul DiffÃ©rentiel

### 1.3.1 DiffÃ©rentielles

#### DÃ©rivÃ©e Directionnelle

La **dÃ©rivÃ©e directionnelle** de f : â„â¿ â†’ â„ en x dans la direction v est :

```
Dáµ¥f(x) = lim[hâ†’0] [f(x + hv) - f(x)] / h
```

#### Gradient

Le **gradient** de f : â„â¿ â†’ â„ en x est le vecteur :

```
âˆ‡f(x) = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€
```

**PropriÃ©tÃ©s** :
- Le gradient pointe dans la direction de plus grande augmentation
- â€–âˆ‡f(x)â€– donne le taux de variation maximal

**Application en ML** : Le gradient est au cÅ“ur de l'algorithme de descente de gradient.

#### Matrice Jacobienne

Pour f : â„â¿ â†’ â„áµ, la **matrice Jacobienne** est :

```
Jf(x) = [âˆ‚fâ‚/âˆ‚xâ‚  ...  âˆ‚fâ‚/âˆ‚xâ‚™]
        [...       ...  ...]
        [âˆ‚fâ‚˜/âˆ‚xâ‚  ...  âˆ‚fâ‚˜/âˆ‚xâ‚™]
```

### 1.3.2 Exemples Importants

#### Fonction Quadratique

Pour f(x) = Â½xáµ€Ax - báµ€x :
```
âˆ‡f(x) = Ax - b
```

#### Fonction Exponentielle

Pour f(x) = exp(aáµ€x) :
```
âˆ‡f(x) = a exp(aáµ€x)
```

#### Fonction Logistique

Pour f(x) = log(1 + exp(aáµ€x)) :
```
âˆ‡f(x) = a / (1 + exp(-aáµ€x))
```

### 1.3.3 DÃ©rivÃ©es d'Ordre SupÃ©rieur

#### Matrice Hessienne

La **matrice Hessienne** de f : â„â¿ â†’ â„ est :

```
Hf(x) = [âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼]
```

**PropriÃ©tÃ©s** :
- Si f est deux fois continÃ»ment diffÃ©rentiable, H est symÃ©trique
- H dÃ©finit la courbure locale de f

**ConvexitÃ©** : f est convexe si H est semi-dÃ©finie positive partout.

### 1.3.4 ThÃ©orÃ¨me de Taylor

Le **dÃ©veloppement de Taylor** d'ordre 2 de f autour de x est :

```
f(x + h) â‰ˆ f(x) + âˆ‡f(x)áµ€h + Â½háµ€Hf(x)h
```

**Application** : UtilisÃ© pour l'analyse de convergence des algorithmes d'optimisation.

---

## 1.4 ThÃ©orie des ProbabilitÃ©s

### 1.4.1 HypothÃ¨ses GÃ©nÃ©rales et Notations

#### Espace de ProbabilitÃ©

Un **espace de probabilitÃ©** est un triplet (Î©, â„±, P) oÃ¹ :
- Î© est l'ensemble des rÃ©sultats possibles (univers)
- â„± est une Ïƒ-algÃ¨bre d'Ã©vÃ©nements
- P : â„± â†’ [0, 1] est une mesure de probabilitÃ©

#### Variable AlÃ©atoire

Une **variable alÃ©atoire** X est une fonction mesurable X : Î© â†’ â„.

**Notation** : X ~ P signifie que X suit la loi de probabilitÃ© P.

#### EspÃ©rance

L'**espÃ©rance** d'une variable alÃ©atoire X est :

```
ğ”¼[X] = âˆ« x dP(x)
```

**PropriÃ©tÃ©s** :
- LinÃ©aritÃ© : ğ”¼[Î±X + Î²Y] = Î±ğ”¼[X] + Î²ğ”¼[Y]
- Si X â‰¥ 0, alors ğ”¼[X] â‰¥ 0

#### Variance

La **variance** de X est :

```
Var(X) = ğ”¼[(X - ğ”¼[X])Â²] = ğ”¼[XÂ²] - (ğ”¼[X])Â²
```

**Ã‰cart-type** : Ïƒ(X) = âˆšVar(X)

### 1.4.2 ProbabilitÃ©s et EspÃ©rances Conditionnelles

#### ProbabilitÃ© Conditionnelle

La **probabilitÃ© conditionnelle** de A sachant B est :

```
P(A|B) = P(A âˆ© B) / P(B)
```

**Formule de Bayes** :
```
P(A|B) = P(B|A)P(A) / P(B)
```

**Application en ML** : La classification bayÃ©sienne utilise cette formule pour calculer P(classe|donnÃ©es).

#### EspÃ©rance Conditionnelle

L'**espÃ©rance conditionnelle** de X sachant Y est :

```
ğ”¼[X|Y] = g(Y) oÃ¹ g(y) = ğ”¼[X|Y=y]
```

**PropriÃ©tÃ© importante** :
```
ğ”¼[ğ”¼[X|Y]] = ğ”¼[X]  (Loi de l'espÃ©rance totale)
```

### 1.4.3 ThÃ©orie des ProbabilitÃ©s Mesurables

#### Ïƒ-AlgÃ¨bre

Une **Ïƒ-algÃ¨bre** â„± sur Î© est une famille d'ensembles telle que :
1. Î© âˆˆ â„±
2. Si A âˆˆ â„±, alors Aá¶œ âˆˆ â„±
3. Si Aâ‚, Aâ‚‚, ... âˆˆ â„±, alors â‹ƒáµ¢ Aáµ¢ âˆˆ â„±

#### Mesure de ProbabilitÃ©

Une **mesure de probabilitÃ©** P sur (Î©, â„±) satisfait :
1. P(Î©) = 1
2. P(A) â‰¥ 0 pour tout A âˆˆ â„±
3. Pour Aâ‚, Aâ‚‚, ... disjoints : P(â‹ƒáµ¢ Aáµ¢) = Î£áµ¢ P(Aáµ¢)

### 1.4.4 Produit de Mesures

Le **produit de mesures** Pâ‚ âŠ— Pâ‚‚ sur Î©â‚ Ã— Î©â‚‚ satisfait :

```
(Pâ‚ âŠ— Pâ‚‚)(A Ã— B) = Pâ‚(A) Â· Pâ‚‚(B)
```

**ThÃ©orÃ¨me de Fubini** : Pour une fonction intÃ©grable f :
```
âˆ«âˆ« f(x,y) d(Pâ‚âŠ—Pâ‚‚)(x,y) = âˆ«[âˆ« f(x,y) dPâ‚‚(y)] dPâ‚(x)
```

### 1.4.5 ContinuitÃ© Absolue et DensitÃ©s

Une mesure Q est **absolument continue** par rapport Ã  P (notÃ© Q << P) si :
```
P(A) = 0 âŸ¹ Q(A) = 0
```

**ThÃ©orÃ¨me de Radon-Nikodym** : Si Q << P, il existe une fonction f (densitÃ©) telle que :
```
Q(A) = âˆ«â‚ f dP
```

On note : f = dQ/dP

**Exemple** : La loi normale N(Î¼, ÏƒÂ²) a pour densitÃ© par rapport Ã  la mesure de Lebesgue :
```
f(x) = (1/âˆš(2Ï€ÏƒÂ²)) exp(-(x-Î¼)Â²/(2ÏƒÂ²))
```

### 1.4.6 ProbabilitÃ©s ThÃ©oriques Mesurables

#### IndÃ©pendance

Deux Ã©vÃ©nements A et B sont **indÃ©pendants** si :
```
P(A âˆ© B) = P(A) Â· P(B)
```

Deux variables alÃ©atoires X et Y sont **indÃ©pendantes** si :
```
P(X âˆˆ A, Y âˆˆ B) = P(X âˆˆ A) Â· P(Y âˆˆ B)
```
pour tous ensembles A et B.

### 1.4.7 EspÃ©rances Conditionnelles (Cas GÃ©nÃ©ral)

L'**espÃ©rance conditionnelle** ğ”¼[X|ğ’¢] est la projection de X sur l'espace LÂ²(ğ’¢) :

**PropriÃ©tÃ©s** :
1. ğ”¼[ğ”¼[X|ğ’¢]] = ğ”¼[X]
2. Si Y est ğ’¢-mesurable : ğ”¼[XY|ğ’¢] = Yğ”¼[X|ğ’¢]
3. InÃ©galitÃ© de Jensen : ğ”¼[Ï†(X)|ğ’¢] â‰¥ Ï†(ğ”¼[X|ğ’¢]) pour Ï† convexe

### 1.4.8 ProbabilitÃ©s Conditionnelles (Cas GÃ©nÃ©ral)

La **probabilitÃ© conditionnelle** P(A|ğ’¢) est dÃ©finie comme :
```
P(A|ğ’¢) = ğ”¼[1â‚|ğ’¢]
```

oÃ¹ 1â‚ est la fonction indicatrice de A.

---

## ğŸ’¡ Applications en Machine Learning

Les concepts de ce chapitre sont utilisÃ©s partout en ML :

1. **AlgÃ¨bre linÃ©aire** : ReprÃ©sentation des donnÃ©es, transformations
2. **Optimisation** : Descente de gradient, minimisation de fonctions de coÃ»t
3. **ProbabilitÃ©s** : ModÃ¨les gÃ©nÃ©ratifs, classification bayÃ©sienne
4. **Calcul diffÃ©rentiel** : RÃ©tropropagation dans les rÃ©seaux de neurones

---

## ğŸ“ Exercices

### Exercice 1 : AlgÃ¨bre LinÃ©aire
Soit A = [[2, 1], [1, 3]]. Calculez ses valeurs propres et vecteurs propres.

### Exercice 2 : Calcul DiffÃ©rentiel
Pour f(x) = â€–xâ€–Â², calculez âˆ‡f(x) et Hf(x).

### Exercice 3 : ProbabilitÃ©s
DÃ©montrez que Var(X) = ğ”¼[XÂ²] - (ğ”¼[X])Â².

### Exercice 4 : Formule de Bayes
Un test mÃ©dical dÃ©tecte une maladie avec 95% de sensibilitÃ© et 90% de spÃ©cificitÃ©. Si 1% de la population a la maladie, quelle est la probabilitÃ© qu'une personne testÃ©e positive ait rÃ©ellement la maladie ?

---

## ğŸ”— RÃ©fÃ©rences et Lectures ComplÃ©mentaires

- **AlgÃ¨bre linÃ©aire** : Strang, G. "Linear Algebra and Its Applications"
- **Calcul** : Spivak, M. "Calculus on Manifolds"
- **ProbabilitÃ©s** : Billingsley, P. "Probability and Measure"

---

[â¬…ï¸ Retour Ã  la table des matiÃ¨res](../README.md) | [Chapitre suivant â¡ï¸](./chapitre-02-analyse-matricielle.md)

