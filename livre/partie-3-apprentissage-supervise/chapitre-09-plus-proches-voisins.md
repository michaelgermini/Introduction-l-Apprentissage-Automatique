# Chapitre 9 : M√©thodes des Plus Proches Voisins

## üìö Introduction

Les m√©thodes des k plus proches voisins (k-NN) sont des algorithmes simples mais puissants bas√©s sur la proximit√© locale.

---

## 9.1 Plus Proches Voisins pour la R√©gression

### Algorithme k-NN

**Pr√©diction** :
```
fÃÇ(x) = (1/k) Œ£_{i ‚àà N_k(x)} y·µ¢
```

o√π N_k(x) sont les k plus proches voisins de x.

```python
from sklearn.neighbors import KNeighborsRegressor

# R√©gression k-NN
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
```

### 9.1.1 Consistance

**Th√©or√®me** : Si k ‚Üí ‚àû et k/n ‚Üí 0, alors :
```
fÃÇ(x) ‚Üí ùîº[Y|X = x]  en probabilit√©
```

**Condition** : k cro√Æt, mais plus lentement que n.

### 9.1.2 Optimalit√©

**Taux de convergence** : O(n^{-4/(4+d)}) o√π d est la dimension.

**Mal√©diction de la dimensionalit√©** : Performance se d√©grade rapidement avec d.

---

## 9.2 Classification k-NN

**R√®gle de d√©cision** :
```
≈∑ = argmax_c Œ£_{i ‚àà N_k(x)} 1_{y·µ¢ = c}
```

(vote majoritaire parmi les k voisins)

```python
from sklearn.neighbors import KNeighborsClassifier

# Classification k-NN
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train, y_train)

# Pr√©diction
y_pred = knn_clf.predict(X_test)

# Probabilit√©s
y_proba = knn_clf.predict_proba(X_test)
```

### Choix de k

**Validation crois√©e** :
```python
from sklearn.model_selection import cross_val_score

scores = []
k_values = range(1, 31)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X_train, y_train, cv=5).mean()
    scores.append(score)

best_k = k_values[np.argmax(scores)]
print(f"Meilleur k: {best_k}")
```

---

## 9.3 Conception de la Distance

### Distances Courantes

**Euclidienne** :
```
d(x, x') = ‚Äñx - x'‚Äñ‚ÇÇ = ‚àö(Œ£·µ¢ (x·µ¢ - x'·µ¢)¬≤)
```

**Manhattan** :
```
d(x, x') = ‚Äñx - x'‚Äñ‚ÇÅ = Œ£·µ¢ |x·µ¢ - x'·µ¢|
```

**Minkowski** :
```
d(x, x') = (Œ£·µ¢ |x·µ¢ - x'·µ¢|·µñ)^{1/p}
```

### Distance de Mahalanobis

```
d(x, x') = ‚àö((x - x')·µÄŒ£‚Åª¬π(x - x'))
```

Prend en compte la corr√©lation entre variables.

```python
from scipy.spatial.distance import mahalanobis

# Matrice de covariance
cov = np.cov(X_train.T)
cov_inv = np.linalg.inv(cov)

# Distance
dist = mahalanobis(x1, x2, cov_inv)
```

### Apprentissage de M√©triques

**Objectif** : Apprendre une matrice M telle que :
```
d(x, x') = ‚àö((x - x')·µÄM(x - x'))
```

**LMNN** (Large Margin Nearest Neighbor)

---

## üí° Avantages et Inconv√©nients

**Avantages** :
- Simple √† comprendre et impl√©menter
- Pas de phase d'entra√Ænement
- Non param√©trique

**Inconv√©nients** :
- Co√ªt de pr√©diction √©lev√© (O(n))
- Sensible √† la dimension
- N√©cessite beaucoup de m√©moire

**Optimisations** :
- KD-Tree pour recherche rapide
- Ball Tree
- LSH (Locality Sensitive Hashing)

```python
# Avec KD-Tree (plus rapide pour d < 20)
knn_tree = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')
knn_tree.fit(X_train, y_train)
```

---

[‚¨ÖÔ∏è Chapitre pr√©c√©dent](./chapitre-08-classification-lineaire.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](./chapitre-10-arbres.md)

