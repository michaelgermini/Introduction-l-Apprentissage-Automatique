# Chapitre 11 : RÃ©seaux de Neurones

## ğŸ“š Introduction

Les rÃ©seaux de neurones sont la base du deep learning moderne. Ce chapitre couvre les fondements architecturaux et algorithmiques.

## ğŸ—ºï¸ Carte Mentale : Deep Learning

```
                    RÃ‰SEAUX DE NEURONES
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
   ARCHITECTURES        TRAINING            RÃ‰GULARISATION
        â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚           â”‚       â”‚           â”‚       â”‚
  MLP     CNN        Forward  Backward   Dropout  BatchNorm
  RNN   Transformer     â”‚       â”‚           â”‚
        â”‚           Loss    Backprop    Early Stop
    Attention                             L1/L2
```

## ğŸ“ Architecture Visuelle d'un MLP

```
INPUT        HIDDEN 1        HIDDEN 2        OUTPUT
Layer         Layer          Layer           Layer

  xâ‚ â—â”€â”€â”€â”€â”€â”
           â”œâ”€â”€â†’â— zâ‚â½Â¹â¾         
  xâ‚‚ â—â”€â”€â”¬â”€â”€â”¤      â†“          
        â”‚  â””â”€â”€â†’â— Ïƒ(zâ‚â½Â¹â¾)â”€â”€â”
  xâ‚ƒ â—â”€â”€â”¼â”€â”€â”   aâ‚â½Â¹â¾      â”œâ”€â”€â†’â— zâ‚â½Â²â¾  
        â”‚  â”‚                â”‚      â†“
  ...   â”‚  â””â”€â”€â†’â— zâ‚‚â½Â¹â¾      â”‚   Ïƒ(zâ‚â½Â²â¾)â”€â”€â”
        â”‚          â†“        â”‚   aâ‚â½Â²â¾      â”œâ”€â”€â†’â— Å·
  xâ‚™ â—â”€â”€â”´â”€â”€â”€â”€â”€â”€â†’â— Ïƒ(zâ‚‚â½Â¹â¾)â”€â”€â”˜              â”‚
                 aâ‚‚â½Â¹â¾      â””â”€â”€â†’â— zâ‚‚â½Â²â¾    â”‚
                                 â†“         â”‚
                              Ïƒ(zâ‚‚â½Â²â¾)â”€â”€â”€â”€â”€â”˜
                              aâ‚‚â½Â²â¾

Notations :
  záµ¢â½Ë¡â¾ = Î£â±¼ wáµ¢â±¼â½Ë¡â¾ aâ±¼â½Ë¡â»Â¹â¾ + báµ¢â½Ë¡â¾  (prÃ©-activation)
  aáµ¢â½Ë¡â¾ = Ïƒ(záµ¢â½Ë¡â¾)                 (activation)
```

## ğŸ“Š Tableau des Fonctions d'Activation

| **Fonction** | **Formule** | **DÃ©rivÃ©e** | **Range** | **Usage** | **Avantages** | **InconvÃ©nients** |
|-------------|------------|------------|----------|-----------|--------------|------------------|
| **Sigmoid** | Ïƒ(z) = 1/(1+eâ»á¶») | Ïƒ(1-Ïƒ) | (0, 1) | Sortie binaire | ProbabilitÃ© | Vanishing gradient |
| **Tanh** | tanh(z) | 1-tanhÂ² | (-1, 1) | Hidden layers | CentrÃ© en 0 | Vanishing gradient |
| **ReLU** | max(0, z) | 1 si z>0 | [0, âˆ) | Hidden (standard) | Rapide, simple | Dying ReLU |
| **Leaky ReLU** | max(Î±z, z) | 1 ou Î± | (-âˆ, âˆ) | Hidden (robuste) | Pas de mort | HyperparamÃ¨tre Î± |
| **ELU** | z si z>0, Î±(eá¶»-1) si zâ‰¤0 | 1 ou Î±Â·eá¶» | (-Î±, âˆ) | Hidden (smooth) | Smooth | Exponentielle |
| **Softmax** | eá¶»â±/Î£â±¼eá¶»Ê² | Complexe | (0, 1), Î£=1 | Sortie multi-classe | ProbabilitÃ©s | Uniquement sortie |

## ğŸ“ˆ Visualisation des Activations

```
    Ïƒ(z)
     â”‚
 1   â”‚      Sigmoid        ReLU          Tanh
     â”‚       â•±â”€â”€â”€â”€â”€         â•±           â•±â”€â”€â”€â”€
     â”‚      â•±              â•±           â•±
 0.5 â”‚     â•±              â•±           â”€
     â”‚    â•±              â•±           â•±
 0   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€ z    â”€â”€â”€â”€â•±â”€â”€â”€â”€   â”€â”€â”€â”€â•±â”€â”€â”€â”€ z
     â”‚                             â•²
-0.5 â”‚                              â•²
     â”‚                               â•²â”€â”€â”€â”€
-1   â”‚
```

---

## 11.1 DÃ©finitions de Base

### Neurone Artificiel

Un **neurone** calcule :
```
z = wâ‚xâ‚ + wâ‚‚xâ‚‚ + ... + wâ‚šxâ‚š + b
a = Ïƒ(z)
```

oÃ¹ Ïƒ est une **fonction d'activation**.

### Fonctions d'Activation

**SigmoÃ¯de** :
```
Ïƒ(z) = 1 / (1 + eâ»á¶»)
```

**Tanh** :
```
tanh(z) = (e^z - eâ»á¶») / (e^z + eâ»á¶»)
```

**ReLU** (Rectified Linear Unit) :
```
ReLU(z) = max(0, z)
```

**Leaky ReLU** :
```
LeakyReLU(z) = max(Î±Â·z, z)  oÃ¹ Î± â‰ˆ 0.01
```

---

## 11.2 Architecture des RÃ©seaux

### 11.2.1 RÃ©seau Multicouche

**Couche cachÃ©e l** :
```
zâ½Ë¡â¾ = Wâ½Ë¡â¾aâ½Ë¡â»Â¹â¾ + bâ½Ë¡â¾
aâ½Ë¡â¾ = Ïƒ(zâ½Ë¡â¾)
```

### 11.2.2 Couche de Sortie

**RÃ©gression** : Sortie linÃ©aire
**Classification binaire** : SigmoÃ¯de
**Classification multi-classe** : Softmax
```
P(Y = k|x) = exp(z_k) / Î£â±¼ exp(z_j)
```

### 11.2.3 RÃ©seaux Convolutifs (CNN)

**Convolution** :
```
(f * g)[i] = Î£â±¼ f[j] Â· g[i - j]
```

**Pooling** : Max pooling, average pooling

```python
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(32 * 14 * 14, 10)
    
    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = x.view(-1, 32 * 14 * 14)
        x = self.fc(x)
        return x
```

---

## 11.3 Fonction Objectif

### Perte pour la RÃ©gression

**MSE** :
```
L = (1/n) Î£áµ¢ (yáµ¢ - Å·áµ¢)Â²
```

### Perte pour la Classification

**Cross-entropie** :
```
L = -(1/n) Î£áµ¢ Î£â‚– yáµ¢â‚– log(Å·áµ¢â‚–)
```

**Avec PyTorch** :
```python
import torch
import torch.nn as nn

criterion = nn.CrossEntropyLoss()
loss = criterion(outputs, targets)
```

---

## 11.4 RÃ©tropropagation

### Algorithme

**Forward pass** : Calculer les activations
**Backward pass** : Calculer les gradients

**RÃ¨gle de la chaÃ®ne** :
```
âˆ‚L/âˆ‚Wâ½Ë¡â¾ = (âˆ‚L/âˆ‚zâ½Ë¡â¾) Â· (âˆ‚zâ½Ë¡â¾/âˆ‚Wâ½Ë¡â¾)
```

**ImplÃ©mentation** :
```python
import torch

# RÃ©seau simple
class SimpleNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# EntraÃ®nement
model = SimpleNN(784, 128, 10)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        # Forward
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

## 11.5 Techniques d'EntraÃ®nement

### 11.5.1 Mini-Batches

**SGD par mini-batches** :
```python
batch_size = 32
dataloader = torch.utils.data.DataLoader(
    dataset, 
    batch_size=batch_size,
    shuffle=True
)
```

### 11.5.2 Dropout

**RÃ©gularisation** : DÃ©sactiver alÃ©atoirement des neurones pendant l'entraÃ®nement.

```python
class NNWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)  # DÃ©sactivÃ© en mode eval()
        x = self.fc2(x)
        return x
```

### Batch Normalization

```python
class NNWithBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.bn = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x
```

---

## 11.6 Architectures Modernes

### ResNet (Residual Networks)

**Connexions rÃ©siduelles** :
```
F(x) = H(x) - x
H(x) = F(x) + x
```

### Transformers

**Attention** :
```
Attention(Q, K, V) = softmax(QK^T / âˆšd_k) V
```

---

## ğŸ’¡ Best Practices

1. **Initialisation** : Xavier/He initialization
2. **Learning rate** : Learning rate schedule
3. **RÃ©gularisation** : Dropout, L2, Batch norm
4. **Architecture** : Commencer simple, complexifier progressivement
5. **Monitoring** : TensorBoard, visualisation des pertes

```python
# Exemple complet avec TensorBoard
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment_1')

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader)
    val_loss = validate(model, val_loader)
    
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)

writer.close()
```

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-10-arbres.md) | [Retour](../README.md) | [Suite â¡ï¸](../partie-4-modeles-probabilistes/chapitre-12-comparaison-distributions.md)

