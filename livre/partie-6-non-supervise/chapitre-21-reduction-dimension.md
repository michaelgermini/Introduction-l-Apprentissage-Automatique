# Chapitre 21 : R√©duction de Dimension et Analyse Factorielle

## üìö Introduction

La r√©duction de dimension projette les donn√©es dans un espace de dimension r√©duite tout en pr√©servant l'information.

---

## 21.1 Analyse en Composantes Principales (PCA)

### Principe

**Maximiser la variance** ou **minimiser l'erreur de reconstruction**.

**Solution** : Vecteurs propres de la matrice de covariance.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# Variance expliqu√©e
print(pca.explained_variance_ratio_)

# Composantes principales
print(pca.components_)
```

---

## 21.2 Kernel PCA

**PCA non lin√©aire** avec le kernel trick.

```python
from sklearn.decomposition import KernelPCA

kpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.1)
X_reduced = kpca.fit_transform(X)
```

---

## 21.3 PCA Probabiliste

**Mod√®le** : x = Wz + Œº + Œµ o√π z ~ N(0, I)

**EM Algorithm** pour estimer W.

---

## 21.4 Robust PCA

**D√©composition** : X = L + S
- L : matrice de rang faible
- S : matrice creuse (outliers)

---

## 21.5 Analyse en Composantes Ind√©pendantes (ICA)

### Objectif

Trouver sources ind√©pendantes : x = As o√π s sont ind√©pendants.

```python
from sklearn.decomposition import FastICA

ica = FastICA(n_components=3)
S = ica.fit_transform(X)  # Sources
A = ica.mixing_  # Matrice de m√©lange
```

**Applications** : S√©paration de sources (audio, EEG).

---

## 21.6 Factorisation Matricielle Non N√©gative (NMF)

**Contrainte** : X ‚âà WH avec W, H ‚â• 0

```python
from sklearn.decomposition import NMF

nmf = NMF(n_components=10, init='random')
W = nmf.fit_transform(X)
H = nmf.components_
```

**Applications** : Traitement d'images, analyse de texte.

---

[‚¨ÖÔ∏è Chapitre pr√©c√©dent](./chapitre-20-clustering.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](./chapitre-22-visualisation.md)

