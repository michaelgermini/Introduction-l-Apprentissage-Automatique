# Chapitre 6 : Produits Internes et Noyaux Reproduisants

## ğŸ“š Introduction

Les noyaux sont un outil puissant qui permet d'Ã©tendre les mÃ©thodes linÃ©aires Ã  des espaces de dimension infinie. Ce chapitre introduit la thÃ©orie des noyaux reproduisants.

## ğŸ—ºï¸ Carte Mentale : Le Kernel Trick

```
                    NOYAUX (KERNELS)
                           â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                  â”‚                  â”‚
    THÃ‰ORIE           NOYAUX             APPLICATIONS
        â”‚              POPULAIRES             â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”            â”‚              â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚       â”‚            â”‚              â”‚           â”‚
 Espace  Produit     â”Œâ”€â”€â”€â”´â”€â”€â”€â”       SVM      Kernel
 Hilbert Scalaire    â”‚       â”‚        â”‚       Ridge/PCA
    â”‚       â”‚     LinÃ©aire  RBF    Polynomial  â”‚
  Ï†(x)   K(x,x')      â”‚       â”‚        â”‚    K-Means
         =âŸ¨Ï†(x),Ï†(x')âŸ© xáµ€x'  Gaussien DegrÃ© d  Spectral
```

## ğŸ¯ Le Kernel Trick : Concept ClÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              LE KERNEL TRICK EN ACTION                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

APPROCHE NAÃVE (CoÃ»teuse) :
  1. Transformer : x â†’ Ï†(x)     [Dimension peut Ãªtre âˆ!]
  2. Calculer : âŸ¨Ï†(x), Ï†(x')âŸ©   [TrÃ¨s coÃ»teux]

KERNEL TRICK (Efficace) :
  Directement : K(x, x') = âŸ¨Ï†(x), Ï†(x')âŸ©
  
SchÃ©ma :

    Espace Original            Espace TransformÃ©
         (â„áµˆ)                      (â„‹, dim >> d)
    
    xâ‚ â—  â— xâ‚‚                    Ï†(xâ‚) â—
      â—  â—        â”€â”€â”€â”€â”€Ï†â”€â”€â†’              â•²
    xâ‚ƒâ—  â—xâ‚„                             â—â”€â— SÃ©parable
         â—                              â•±     linÃ©airement!
    Non-linÃ©airement              Ï†(xâ‚„) â—
    sÃ©parable                     
                                  Ï†(xâ‚ƒ) â—

Exemple : XOR problem
  Original : (xâ‚,xâ‚‚) non-sÃ©parable linÃ©airement
  AprÃ¨s Ï†  : SÃ©parable avec un hyperplan !
```

## ğŸ“Š Tableau Comparatif : Noyaux Populaires

| **Noyau** | **Formule** | **Dimension â„‹** | **ParamÃ¨tres** | **Avantages** | **InconvÃ©nients** |
|-----------|-----------|----------------|---------------|--------------|------------------|
| **LinÃ©aire** | xáµ€x' | d | Aucun | Rapide, simple | LinÃ©aire seulement |
| **Polynomial** | (xáµ€x' + c)^p | C(d+p,p) | p (degrÃ©), c | Flexible | NumÃ©rique instable |
| **RBF (Gaussien)** | exp(-Î³â€–x-x'â€–Â²) | âˆ | Î³ (ou Ïƒ) | Universel, smooth | Peut overfit |
| **Sigmoid** | tanh(Î± xáµ€x' + c) | âˆ | Î±, c | Comme rÃ©seau | Pas toujours PD |
| **Laplacien** | exp(-Î³â€–x-x'â€–â‚) | âˆ | Î³ | Moins smooth | Rare |

## ğŸ“ Visualisation : Effet des Noyaux

### Noyau RBF avec diffÃ©rents Î³

```
Î³ = 0.1 (large Ïƒ)         Î³ = 1.0 (moyen)         Î³ = 10 (petit Ïƒ)

    â•­â”€â”€â”€â”€â”€â”€â”€â•®                  â•­â”€â”€â•®                     â•­â•®
   â•±         â•²                â•±    â•²                   â•±â•²
  â•±           â•²              â•±      â•²                 â•±  â•²
 â•±             â•²            â•±        â•²               â•±    â•²
â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€            â”€â”€â—â”€â”€â”€â”€â”€â”€
  Influence large       Influence moyenne       Influence locale
  Smooth, underfit      Ã‰quilibrÃ©               Peut overfit
```

### Transformation par Noyau Polynomial

```
Espace Original (2D)          Noyau Poly(deg=2)

    xâ‚‚                         zâ‚ = xâ‚Â²
     â”‚                              â•±
   1 â”‚ â—‹ â—‹ â—                      â•±  â— â—
     â”‚ â—‹ â— â— â—                   â•±   â—
   0 â”‚ â— â— â—         â”€â”€â”€Ï†â”€â”€â†’   zâ‚‚= âˆš2xâ‚xâ‚‚
     â”‚   â—                      â”‚    â—
  -1 â”‚                          â”‚   â— â—‹ â—‹
     â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ xâ‚               â””â”€â”€â”€â”€â”€â”€â”€â”€â†’ zâ‚ƒ = xâ‚‚Â²
    -1  0  1                         SÃ©parable !

Non-linÃ©airement              LinÃ©airement sÃ©parable
sÃ©parable                     dans l'espace transformÃ©
```

## ğŸ”¢ Calcul Explicite : Noyau Polynomial

### Exemple DÃ©taillÃ©

```
Soit x = [xâ‚, xâ‚‚]áµ€ âˆˆ â„Â²
Noyau polynomial : K(x, x') = (xáµ€x')Â²

Ã‰TAPE 1 : Calcul direct du noyau
  K(x, x') = (xâ‚xâ‚' + xâ‚‚xâ‚‚')Â²

Ã‰TAPE 2 : Expansion
  K(x, x') = xâ‚Â²xâ‚'Â² + 2xâ‚xâ‚‚xâ‚'xâ‚‚' + xâ‚‚Â²xâ‚‚'Â²

Ã‰TAPE 3 : Identification de Ï†
  Ï†(x) = [xâ‚Â², âˆš2xâ‚xâ‚‚, xâ‚‚Â²]áµ€ âˆˆ â„Â³
  
  VÃ©rification :
  Ï†(x)áµ€Ï†(x') = xâ‚Â²xâ‚'Â² + âˆš2xâ‚xâ‚‚Â·âˆš2xâ‚'xâ‚‚' + xâ‚‚Â²xâ‚‚'Â²
              = xâ‚Â²xâ‚'Â² + 2xâ‚xâ‚‚xâ‚'xâ‚‚' + xâ‚‚Â²xâ‚‚'Â²
              = K(x, x') âœ“

GAIN : Au lieu de calculer Ï†(x) puis âŸ¨Ï†(x),Ï†(x')âŸ©
       On calcule directement (xáµ€x')Â² !
```

## ğŸ¨ Matrice de Gram

```
Pour n points : X = [xâ‚, xâ‚‚, ..., xâ‚™]

Matrice de Gram K :
    
    K = â”Œ                                      â”
        â”‚ K(xâ‚,xâ‚)  K(xâ‚,xâ‚‚)  ...  K(xâ‚,xâ‚™)  â”‚
        â”‚ K(xâ‚‚,xâ‚)  K(xâ‚‚,xâ‚‚)  ...  K(xâ‚‚,xâ‚™)  â”‚
        â”‚    â‹®         â‹®       â‹±       â‹®      â”‚
        â”‚ K(xâ‚™,xâ‚)  K(xâ‚™,xâ‚‚)  ...  K(xâ‚™,xâ‚™)  â”‚
        â””                                      â”˜

PropriÃ©tÃ©s :
  â€¢ SymÃ©trique : K = Káµ€
  â€¢ Semi-dÃ©finie positive : K âª° 0
  â€¢ Káµ¢â±¼ = âŸ¨Ï†(xáµ¢), Ï†(xâ±¼)âŸ©

Python :
  from sklearn.metrics.pairwise import rbf_kernel
  K = rbf_kernel(X, gamma=0.5)
```

---

## 6.1 Introduction

Les **noyaux** permettent de calculer des produits scalaires dans des espaces de haute dimension sans calculer explicitement la transformation.

---

## 6.2 DÃ©finitions de Base

### 6.2.1 Espaces Ã  Produit Interne

Un **espace Ã  produit interne** (H, âŸ¨Â·,Â·âŸ©) est un espace vectoriel avec un produit scalaire.

**PropriÃ©tÃ©s** :
- LinÃ©aritÃ© : âŸ¨Î±u + Î²v, wâŸ© = Î±âŸ¨u,wâŸ© + Î²âŸ¨v,wâŸ©
- SymÃ©trie : âŸ¨u,vâŸ© = âŸ¨v,uâŸ©
- PositivitÃ© : âŸ¨u,uâŸ© â‰¥ 0

### 6.2.2 Espaces de CaractÃ©ristiques et Noyaux

**Fonction noyau** K : ğ’³ Ã— ğ’³ â†’ â„ est **dÃ©finie positive** si :
```
Î£áµ¢â±¼ Î±áµ¢Î±â±¼ K(xáµ¢, xâ±¼) â‰¥ 0
```
pour tous xâ‚, ..., xâ‚™ et Î±â‚, ..., Î±â‚™.

**ThÃ©orÃ¨me de Moore-Aronszajn** : Tout noyau dÃ©fini positif correspond Ã  un espace de Hilbert â„‹ et une carte Ï† : ğ’³ â†’ â„‹ tels que :
```
K(x, x') = âŸ¨Ï†(x), Ï†(x')âŸ©_â„‹
```

---

## 6.3 Exemples

### 6.3.1 Produit Scalaire

Le noyau linÃ©aire :
```
K(x, x') = xáµ€x'
```

### 6.3.2 Noyaux Polynomiaux

```
K(x, x') = (xáµ€x' + c)^d
```

**Exemple** : Pour d = 2, c = 0 en â„Â² :
```
K(x, x') = (xâ‚xâ‚' + xâ‚‚xâ‚‚')Â²
         = xâ‚Â²xâ‚'Â² + 2xâ‚xâ‚‚xâ‚'xâ‚‚' + xâ‚‚Â²xâ‚‚'Â²
```

Espace de caractÃ©ristiques : Ï†(x) = [xâ‚Â², âˆš2xâ‚xâ‚‚, xâ‚‚Â²]

### 6.3.3 Noyau Gaussien (RBF)

```
K(x, x') = exp(-â€–x - x'â€–Â²/(2ÏƒÂ²))
```

**PropriÃ©tÃ©** : Correspond Ã  un espace de dimension infinie !

```python
from sklearn.metrics.pairwise import rbf_kernel

K = rbf_kernel(X, gamma=1/(2*sigma**2))
```

### 6.3.4 ThÃ©orÃ¨mes de Construction

**ThÃ©orÃ¨me** : Si Kâ‚ et Kâ‚‚ sont des noyaux, alors :
- Î±Kâ‚ + Î²Kâ‚‚ (Î±, Î² â‰¥ 0)
- Kâ‚ Â· Kâ‚‚
- f(Kâ‚) oÃ¹ f est une sÃ©rie entiÃ¨re Ã  coefficients positifs

### 6.3.5 OpÃ©rations sur les Noyaux

**Somme** :
```
K(x, x') = Kâ‚(x, x') + Kâ‚‚(x, x')
```

**Produit** :
```
K(x, x') = Kâ‚(x, x') Â· Kâ‚‚(x, x')
```

**Composition avec fonction** :
```
K(x, x') = f(x)áµ€f(x')
```

---

## 6.4 Projection sur Sous-Espace

Dans un espace â„‹, la projection de f sur span{Ï†(xâ‚), ..., Ï†(xâ‚™)} est :

```
fÌ‚ = Î£áµ¢ Î±áµ¢ Ï†(xáµ¢)
```

**ThÃ©orÃ¨me du reprÃ©sentant** : La solution de minimiser
```
â€–fâ€–Â²_â„‹ + Î» Î£áµ¢ L(yáµ¢, f(xáµ¢))
```
est de la forme f = Î£áµ¢ Î±áµ¢ K(Â·, xáµ¢).

---

## ğŸ’¡ Applications

1. **SVM** : Classification dans espace de caractÃ©ristiques
2. **Kernel Ridge Regression**
3. **Kernel PCA** : PCA non linÃ©aire
4. **Kernel K-means** : Clustering non linÃ©aire

```python
from sklearn.svm import SVC
from sklearn.decomposition import KernelPCA

# SVM avec noyau RBF
svm = SVC(kernel='rbf', gamma=0.1)
svm.fit(X_train, y_train)

# Kernel PCA
kpca = KernelPCA(n_components=2, kernel='rbf')
X_transformed = kpca.fit_transform(X)
```

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-05-prediction-concepts.md) | [Retour](../README.md) | [Suite â¡ï¸](../partie-3-apprentissage-supervise/chapitre-07-regression-lineaire.md)

