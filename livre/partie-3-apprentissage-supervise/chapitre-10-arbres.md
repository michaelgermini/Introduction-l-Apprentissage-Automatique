# Chapitre 10 : Algorithmes Bas√©s sur les Arbres

## üìö Introduction

Les arbres de d√©cision et leurs extensions (for√™ts al√©atoires, boosting) sont parmi les algorithmes les plus populaires en ML.

---

## 10.1 Partitionnement R√©cursif

### 10.1.1 Arbres de D√©cision Binaires

**Structure** : Arbre binaire o√π :
- N≈ìuds internes : tests sur les features
- Feuilles : pr√©dictions

### 10.1.2 Algorithme d'Entra√Ænement

**CART** (Classification And Regression Trees) :

1. Choisir la meilleure division (feature + seuil)
2. Diviser les donn√©es
3. R√©p√©ter r√©cursivement

**Crit√®re de division** :

**R√©gression** (MSE) :
```
Impurity = Œ£_{samples} (y - »≥)¬≤
```

**Classification** (Gini) :
```
Gini = 1 - Œ£_c p_c¬≤
```

```python
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

# Classification
tree_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=20)
tree_clf.fit(X_train, y_train)

# Visualisation
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(tree_clf, filled=True, feature_names=feature_names)
plt.show()
```

### 10.1.3 R√®gle d'Arr√™t

**Crit√®res** :
- Profondeur maximale
- Nombre minimal d'√©chantillons par n≈ìud
- Am√©lioration minimale

### 10.1.4 √âlagage (Pruning)

**Cost-complexity pruning** :
```
R_Œ±(T) = R(T) + Œ±|T|
```

o√π |T| est le nombre de feuilles.

```python
# Avec √©lagage
tree = DecisionTreeClassifier(ccp_alpha=0.01)
tree.fit(X_train, y_train)
```

---

## 10.2 For√™ts Al√©atoires

### 10.2.1 Bagging

**Bootstrap Aggregating** :
1. G√©n√©rer B √©chantillons bootstrap
2. Entra√Æner un arbre sur chaque √©chantillon
3. Pr√©diction = moyenne/vote majoritaire

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

# Classification
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    max_features='sqrt',
    random_state=42
)
rf.fit(X_train, y_train)

# Importance des variables
importances = rf.feature_importances_
```

### 10.2.2 Randomisation des Features

√Ä chaque division, consid√©rer seulement ‚àöp features al√©atoires (classification) ou p/3 (r√©gression).

**Avantages** :
- R√©duit la corr√©lation entre arbres
- Am√©liore la g√©n√©ralisation

---

## 10.3 Adaboost

### 10.3.1 Principe

**Boosting** : Combiner s√©quentiellement des apprenants faibles.

### 10.3.2 Algorithme Adaboost

```
Pour m = 1 √† M:
    1. Entra√Æner h_m avec poids w_i
    2. Calculer erreur Œµ_m = Œ£ w_i ¬∑ 1_{y_i ‚â† h_m(x_i)} / Œ£ w_i
    3. Œ±_m = ¬Ω log((1 - Œµ_m) / Œµ_m)
    4. Mettre √† jour: w_i ‚Üê w_i ¬∑ exp(-Œ±_m y_i h_m(x_i))

Pr√©diction: sign(Œ£_m Œ±_m h_m(x))
```

```python
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0
)
ada.fit(X_train, y_train)
```

---

## 10.4 Gradient Boosting

### Principe

**Id√©e** : Ajuster s√©quentiellement la r√©siduelle.

```
f_0(x) = 0
Pour m = 1 √† M:
    r_i = y_i - f_{m-1}(x_i)
    h_m = fit(X, r)
    f_m(x) = f_{m-1}(x) + Œ∑ ¬∑ h_m(x)
```

```python
from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8
)
gbr.fit(X_train, y_train)
```

### XGBoost

**Optimisations** :
- R√©gularisation L1/L2
- Gestion des valeurs manquantes
- Parall√©lisation

```python
import xgboost as xgb

xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0
)
xgb_model.fit(X_train, y_train)
```

---

## üí° Comparaison

| M√©thode | Type | Force | Faiblesse |
|---------|------|-------|-----------|
| Arbre | Non param√©trique | Interpr√©table | Instable |
| Random Forest | Ensemble | Robuste | Bo√Æte noire |
| Adaboost | Boosting | Pr√©cis | Sensible au bruit |
| Gradient Boosting | Boosting | Tr√®s pr√©cis | Temps d'entra√Ænement |

---

[‚¨ÖÔ∏è Chapitre pr√©c√©dent](./chapitre-09-plus-proches-voisins.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](./chapitre-11-reseaux-neurones.md)

