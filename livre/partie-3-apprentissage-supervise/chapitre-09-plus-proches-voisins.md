# Chapitre 9 : MÃ©thodes des Plus Proches Voisins

## ğŸ“š Introduction

Les mÃ©thodes des k plus proches voisins (k-NN) sont des algorithmes simples mais puissants basÃ©s sur la proximitÃ© locale.

## ğŸ—ºï¸ Carte Mentale : k-NN

```
                    k-NEAREST NEIGHBORS
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
    RÃ‰GRESSION        CLASSIFICATION        PARAMÃˆTRES
        â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚           â”‚       â”‚           â”‚       â”‚
 Moyenne  Poids      Vote     Poids       k    Distance
 simple  distance  majoritaire probabiliste â”‚        â”‚
    â”‚       â”‚           â”‚       â”‚        CV    Euclidienne
  Å·=1/k Î£y  Inverse    argmax   Î£1_c/k       Manhattan
         distanceÂ²              â”‚         Minkowski
                            ProbabilitÃ©s
```

## ğŸ“ Visualisation : Fonctionnement de k-NN

### Classification avec k = 1, k = 3, k = 7

```
k = 1 (1-NN)              k = 3 (3-NN)              k = 7 (7-NN)

  â— â—                      â— â—                      â— â—
 â— ? â—                    â— ? â—                    â— ? â—
  â— â—                      â— â—                      â— â—
   â—                        â—                        â—
                           â—                        â— â—

Plus proche:             3 voisins:               7 voisins:
  1 bleu                   2 bleus, 1 rouge         4 bleus, 3 rouges
â†’ Classe: Bleu          â†’ Classe: Bleu           â†’ Classe: Bleu

FrontiÃ¨re:              FrontiÃ¨re:               FrontiÃ¨re:
TrÃ¨s irrÃ©guliÃ¨re        ModÃ©rÃ©e                  Lisse
(overfit)               (Ã©quilibrÃ©e)             (peut underfit)
```

### Effet de k sur la FrontiÃ¨re de DÃ©cision

```
    k = 1                    k = 5                    k = 15

   â•±â•²â•±â•²â•±â•²                   â•±â”€â”€â•²                     â•±â”€â”€â”€â”€â•²
  â•±  â—  â•²                  â•± â—  â•²                   â•±  â—   â•²
 â•±   â—   â•²                â•±   â—  â•²                 â•±    â—   â•²
â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â—              â—â”€â”€â”€â”€â”€â”€â”€â”€â—               â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â—
â•²    â—‹    â•±              â•²   â—‹   â•±               â•²     â—‹     â•±
 â•²   â—‹   â•±                â•²  â—‹  â•±                 â•²    â—‹    â•±
  â•²_â—‹__â—‹_â•±                  â•²__â—‹_â•±                   â•²___â—‹___â•±

Complexe                 Ã‰quilibrÃ©                Lisse
Variance â†‘               Compromis                Biais â†‘
Overfitting              Optimal                  Underfitting
```

## ğŸ“Š Tableau : Effet de k

| **k** | **Biais** | **Variance** | **FrontiÃ¨re** | **Overfitting** | **Usage** |
|-------|----------|-------------|--------------|----------------|-----------|
| **k = 1** | âœ“ Faible | â¬†ï¸ TrÃ¨s Ã©levÃ©e | TrÃ¨s irrÃ©guliÃ¨re | âœ— Oui | Test rapide |
| **k = 3-5** | Moyen | Moyen | ModÃ©rÃ©e | âš ï¸ Possible | Standard |
| **k = 10-20** | â¬†ï¸ Ã‰levÃ© | âœ“ Faible | Lisse | âœ“ Non | Datasets bruitÃ©s |
| **k = âˆšn** | Ã‰levÃ© | TrÃ¨s faible | TrÃ¨s lisse | âœ“ Non | Heuristique |

## ğŸ¯ Algorithme k-NN : Ã‰tapes DÃ©taillÃ©es

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              ALGORITHME k-NN (Classification)           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

EntrÃ©e : 
  â€¢ Training set : {(xâ‚,yâ‚), ..., (xâ‚™,yâ‚™)}
  â€¢ Point de requÃªte : x_query
  â€¢ Nombre de voisins : k

Ã‰tape 1 : CALCULER DISTANCES
  Pour chaque point d'entraÃ®nement xáµ¢ :
    dáµ¢ = distance(x_query, xáµ¢)
    
  [x_query] â”€â”€dâ‚â”€â”€â†’ [xâ‚, yâ‚]
     â”‚
     â”œâ”€â”€â”€â”€dâ‚‚â”€â”€â†’ [xâ‚‚, yâ‚‚]
     â”‚
     â”œâ”€â”€â”€â”€dâ‚ƒâ”€â”€â†’ [xâ‚ƒ, yâ‚ƒ]
     â‹®

Ã‰tape 2 : TRIER & SÃ‰LECTIONNER k plus proches
  Trier : dâ‚ â‰¤ dâ‚‚ â‰¤ ... â‰¤ dâ‚™
  Garder les k premiers
  
  Exemple (k=3) :
    dâ‚ƒ = 0.5  [xâ‚ƒ, yâ‚ƒ=â—‹]  âœ“ Voisin 1
    dâ‚‡ = 0.8  [xâ‚‡, yâ‚‡=â—]  âœ“ Voisin 2
    dâ‚ = 0.9  [xâ‚, yâ‚=â—‹]  âœ“ Voisin 3
    dâ‚… = 1.2  [xâ‚…, yâ‚…=â—]  âœ— Trop loin

Ã‰tape 3 : VOTE MAJORITAIRE
  Compter les classes :
    Classe â—‹ : 2 votes
    Classe â— : 1 vote
    
  Å· = argmax (votes) = â—‹

Sortie : Å· = Classe prÃ©dite
```

## ğŸ“ Tableau : MÃ©triques de Distance

| **Distance** | **Formule** | **p** | **PropriÃ©tÃ©s** | **Usage** |
|-------------|-----------|------|---------------|-----------|
| **Euclidienne** | âˆš(Î£(xáµ¢-x'áµ¢)Â²) | p=2 | Isotrope, sensible Ã©chelle | Standard |
| **Manhattan** | Î£\|xáµ¢-x'áµ¢\| | p=1 | Robuste outliers | Grilles, City-block |
| **Minkowski** | (Î£\|xáµ¢-x'áµ¢\|áµ–)^(1/p) | pâ‰¥1 | GÃ©nÃ©ralise Lâ‚, Lâ‚‚ | Flexible |
| **Chebyshev** | max\|xáµ¢-x'áµ¢\| | p=âˆ | Dimension dominante | Jeux (Ã©checs) |
| **Mahalanobis** | âˆš((x-x')áµ€Î£â»Â¹(x-x')) | - | CorrÃ©lations prises en compte | Features corrÃ©lÃ©es |

## ğŸ” Visualisation : Distances

```
Point de rÃ©fÃ©rence : â—

     Manhattan (Lâ‚)           Euclidienne (Lâ‚‚)         Chebyshev (Lâˆ)
         
    â”‚     â”‚     â”‚                 â•±â”‚â•²                     â”Œâ”€â”€â”€â”€â”€â”
  â”€â”€â”¼â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”¼â”€â”€            â•±  â”‚  â•²                   â”‚     â”‚
    â”‚     â”‚     â”‚            â•±    â—    â•²                 â”‚  â—  â”‚
                           â•±      â”‚      â•²               â”‚     â”‚
  Distance = 4          Distance = 2âˆš2            Distance = 2
  (Losange)             (Cercle)                  (CarrÃ©)
```

## âš ï¸ MalÃ©diction de la DimensionalitÃ©

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         CURSE OF DIMENSIONALITY pour k-NN            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Dimension d        Volume requis        DonnÃ©es pour
                  pour 1% voisins       densitÃ© uniforme
                  
d = 2              10% du rayon         n = 100
    â—â—â—                                  
    â—xâ—            â•­â”€â”€â•®                 Faisable
    â—â—â—            â”‚  â”‚
                   
d = 10             80% du rayon         n = 10Â¹â°
    â—              â•­â”€â”€â”€â”€â•®               
    xâ—â—            â”‚    â”‚               Impossible !
    â—              â•°â”€â”€â”€â”€â•¯
                   
d = 100            99.5% du rayon       n = 10Â¹â°â°
    xâ—                                   
    â—              â•­â”€â”€â”€â”€â”€â”€â•®             Astronomique
                   â”‚      â”‚

ProblÃ¨me : En haute dimension
  â€¢ Tous les points sont "loin"
  â€¢ Notion de "proximitÃ©" perd son sens
  â€¢ k-NN devient inefficace

Solution :
  â€¢ RÃ©duction de dimension (PCA)
  â€¢ SÃ©lection de features
  â€¢ MÃ©thodes paramÃ©triques
```

## ğŸ’¡ Conseils Pratiques

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GUIDE D'UTILISATION k-NN             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

âœ“ QUAND UTILISER k-NN :
  â€¢ Dataset petit/moyen (n < 10,000)
  â€¢ Faible dimension (d < 20)
  â€¢ FrontiÃ¨re de dÃ©cision complexe
  â€¢ Baseline rapide
  â€¢ Pas besoin d'interprÃ©tabilitÃ©

âœ— Ã‰VITER k-NN SI :
  â€¢ Big Data (n > 100,000) â†’ trop lent
  â€¢ Haute dimension (d > 50) â†’ curse
  â€¢ DonnÃ©es dÃ©sÃ©quilibrÃ©es â†’ biais
  â€¢ Temps rÃ©el requis â†’ latence Ã©levÃ©e

âš™ï¸ OPTIMISATIONS :
  â€¢ KD-Tree, Ball-Tree pour recherche rapide
  â€¢ Normalisation des features (StandardScaler)
  â€¢ Poids par distance inverse
  â€¢ Cross-validation pour k optimal
```

---

## 9.1 Plus Proches Voisins pour la RÃ©gression

### Algorithme k-NN

**PrÃ©diction** :
```
fÌ‚(x) = (1/k) Î£_{i âˆˆ N_k(x)} yáµ¢
```

oÃ¹ N_k(x) sont les k plus proches voisins de x.

```python
from sklearn.neighbors import KNeighborsRegressor

# RÃ©gression k-NN
knn = KNeighborsRegressor(n_neighbors=5)
knn.fit(X_train, y_train)
y_pred = knn.predict(X_test)
```

### 9.1.1 Consistance

**ThÃ©orÃ¨me** : Si k â†’ âˆ et k/n â†’ 0, alors :
```
fÌ‚(x) â†’ ğ”¼[Y|X = x]  en probabilitÃ©
```

**Condition** : k croÃ®t, mais plus lentement que n.

### 9.1.2 OptimalitÃ©

**Taux de convergence** : O(n^{-4/(4+d)}) oÃ¹ d est la dimension.

**MalÃ©diction de la dimensionalitÃ©** : Performance se dÃ©grade rapidement avec d.

---

## 9.2 Classification k-NN

**RÃ¨gle de dÃ©cision** :
```
Å· = argmax_c Î£_{i âˆˆ N_k(x)} 1_{yáµ¢ = c}
```

(vote majoritaire parmi les k voisins)

```python
from sklearn.neighbors import KNeighborsClassifier

# Classification k-NN
knn_clf = KNeighborsClassifier(n_neighbors=3)
knn_clf.fit(X_train, y_train)

# PrÃ©diction
y_pred = knn_clf.predict(X_test)

# ProbabilitÃ©s
y_proba = knn_clf.predict_proba(X_test)
```

### Choix de k

**Validation croisÃ©e** :
```python
from sklearn.model_selection import cross_val_score

scores = []
k_values = range(1, 31)

for k in k_values:
    knn = KNeighborsClassifier(n_neighbors=k)
    score = cross_val_score(knn, X_train, y_train, cv=5).mean()
    scores.append(score)

best_k = k_values[np.argmax(scores)]
print(f"Meilleur k: {best_k}")
```

---

## 9.3 Conception de la Distance

### Distances Courantes

**Euclidienne** :
```
d(x, x') = â€–x - x'â€–â‚‚ = âˆš(Î£áµ¢ (xáµ¢ - x'áµ¢)Â²)
```

**Manhattan** :
```
d(x, x') = â€–x - x'â€–â‚ = Î£áµ¢ |xáµ¢ - x'áµ¢|
```

**Minkowski** :
```
d(x, x') = (Î£áµ¢ |xáµ¢ - x'áµ¢|áµ–)^{1/p}
```

### Distance de Mahalanobis

```
d(x, x') = âˆš((x - x')áµ€Î£â»Â¹(x - x'))
```

Prend en compte la corrÃ©lation entre variables.

```python
from scipy.spatial.distance import mahalanobis

# Matrice de covariance
cov = np.cov(X_train.T)
cov_inv = np.linalg.inv(cov)

# Distance
dist = mahalanobis(x1, x2, cov_inv)
```

### Apprentissage de MÃ©triques

**Objectif** : Apprendre une matrice M telle que :
```
d(x, x') = âˆš((x - x')áµ€M(x - x'))
```

**LMNN** (Large Margin Nearest Neighbor)

---

## ğŸ’¡ Avantages et InconvÃ©nients

**Avantages** :
- Simple Ã  comprendre et implÃ©menter
- Pas de phase d'entraÃ®nement
- Non paramÃ©trique

**InconvÃ©nients** :
- CoÃ»t de prÃ©diction Ã©levÃ© (O(n))
- Sensible Ã  la dimension
- NÃ©cessite beaucoup de mÃ©moire

**Optimisations** :
- KD-Tree pour recherche rapide
- Ball Tree
- LSH (Locality Sensitive Hashing)

```python
# Avec KD-Tree (plus rapide pour d < 20)
knn_tree = KNeighborsClassifier(n_neighbors=5, algorithm='kd_tree')
knn_tree.fit(X_train, y_train)
```

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-08-classification-lineaire.md) | [Retour](../README.md) | [Suite â¡ï¸](./chapitre-10-arbres.md)

