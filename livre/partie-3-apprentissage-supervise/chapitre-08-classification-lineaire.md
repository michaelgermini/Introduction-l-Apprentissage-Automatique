# Chapitre 8 : Mod√®les de Classification Lin√©aire

## üìö Introduction

Ce chapitre couvre les principales m√©thodes de classification lin√©aire : r√©gression logistique, analyse discriminante lin√©aire, et machines √† vecteurs de support.

---

## 8.1 R√©gression Logistique

### 8.1.1 Cadre G√©n√©ral

**Mod√®le** : Pour classification binaire (Y ‚àà {0, 1}) :
```
P(Y = 1|X = x) = œÉ(w·µÄx + b)
```

o√π œÉ(z) = 1/(1 + e‚Åª·∂ª) est la fonction sigmo√Øde.

### 8.1.2 Log-Vraisemblance Conditionnelle

**Fonction objectif** :
```
‚Ñì(w, b) = Œ£·µ¢ [y·µ¢ log p(x·µ¢) + (1-y·µ¢) log(1-p(x·µ¢))]
```

o√π p(x) = œÉ(w·µÄx + b)

### 8.1.3 Algorithme d'Entra√Ænement

**Gradient** :
```
‚àá‚Ñì = Œ£·µ¢ (y·µ¢ - p(x·µ¢)) x·µ¢
```

```python
from sklearn.linear_model import LogisticRegression

# Classification binaire
lr = LogisticRegression()
lr.fit(X_train, y_train)

# Pr√©diction
y_pred = lr.predict(X_test)
y_proba = lr.predict_proba(X_test)
```

### 8.1.4 R√©gression Logistique P√©nalis√©e

```
minimize  -‚Ñì(w, b) + Œª‚Äñw‚Äñ¬≤   (Ridge)
minimize  -‚Ñì(w, b) + Œª‚Äñw‚Äñ‚ÇÅ   (Lasso)
```

```python
# L2 penalty
lr_ridge = LogisticRegression(penalty='l2', C=1.0)

# L1 penalty (s√©lection de variables)
lr_lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)
```

### 8.1.5 R√©gression Logistique √† Noyaux

```
f(x) = Œ£·µ¢ Œ±·µ¢ K(x, x·µ¢)
P(Y = 1|X = x) = œÉ(f(x))
```

---

## 8.2 Analyse Discriminante Lin√©aire (LDA)

### 8.2.1 Mod√®le G√©n√©ratif

**Hypoth√®ses** :
- X|Y = k ~ N(Œº‚Çñ, Œ£)  (m√™me covariance)
- P(Y = k) = œÄ‚Çñ

**R√®gle de classification** :
```
≈∑ = argmax_k [log œÄ‚Çñ - ¬Ω(x - Œº‚Çñ)·µÄŒ£‚Åª¬π(x - Œº‚Çñ)]
```

**Fronti√®re de d√©cision** : Lin√©aire !

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)
y_pred = lda.predict(X_test)
```

### 8.2.2 R√©duction de Dimension

LDA projette sur l'espace de dimension K-1 (K classes) qui maximise :
```
J(W) = |W·µÄS_B W| / |W·µÄS_W W|
```

o√π :
- S_B : matrice de covariance between-class
- S_W : matrice de covariance within-class

```python
# LDA pour r√©duction de dimension
lda = LinearDiscriminantAnalysis(n_components=2)
X_reduced = lda.fit_transform(X_train, y_train)
```

### 8.2.3 LDA de Fisher

Maximiser :
```
J(w) = (w·µÄ(Œº‚ÇÅ - Œº‚ÇÇ))¬≤ / (w·µÄ(Œ£‚ÇÅ + Œ£‚ÇÇ)w)
```

**Solution** :
```
w = (Œ£‚ÇÅ + Œ£‚ÇÇ)‚Åª¬π(Œº‚ÇÅ - Œº‚ÇÇ)
```

---

## 8.3 Notation Optimale

Trouver une transformation Y ‚Üí Z qui rend la classification plus facile.

**Objectif** : Maximiser la corr√©lation entre Z et les indicatrices de classes.

---

## 8.4 SVM (Support Vector Machines)

### 8.4.1 Perceptron et Marge

**Perceptron** : Trouver w tel que y·µ¢(w·µÄx·µ¢) > 0

**Marge** : Distance du point le plus proche √† l'hyperplan :
```
m = min_i |w·µÄx·µ¢| / ‚Äñw‚Äñ
```

### 8.4.2 Maximisation de la Marge

**Probl√®me primal** :
```
minimize    ¬Ω‚Äñw‚Äñ¬≤
subject to  y·µ¢(w·µÄx·µ¢ + b) ‚â• 1  pour tout i
```

**Version soft-margin** :
```
minimize    ¬Ω‚Äñw‚Äñ¬≤ + C Œ£·µ¢ Œæ·µ¢
subject to  y·µ¢(w·µÄx·µ¢ + b) ‚â• 1 - Œæ·µ¢
            Œæ·µ¢ ‚â• 0
```

### 8.4.3 Probl√®me Dual et Conditions KKT

**Dual** :
```
maximize    Œ£·µ¢ Œ±·µ¢ - ¬ΩŒ£·µ¢‚±º Œ±·µ¢Œ±‚±ºy·µ¢y‚±º(x·µ¢·µÄx‚±º)
subject to  0 ‚â§ Œ±·µ¢ ‚â§ C
            Œ£·µ¢ Œ±·µ¢y·µ¢ = 0
```

**Solution** :
```
w = Œ£·µ¢ Œ±·µ¢y·µ¢x·µ¢
```

**Vecteurs de support** : Points avec Œ±·µ¢ > 0

### 8.4.4 Version √† Noyaux

Remplacer x·µ¢·µÄx‚±º par K(x·µ¢, x‚±º) :

```
maximize    Œ£·µ¢ Œ±·µ¢ - ¬ΩŒ£·µ¢‚±º Œ±·µ¢Œ±‚±ºy·µ¢y‚±ºK(x·µ¢, x‚±º)
```

```python
from sklearn.svm import SVC

# SVM lin√©aire
svm_linear = SVC(kernel='linear', C=1.0)
svm_linear.fit(X_train, y_train)

# SVM avec noyau RBF
svm_rbf = SVC(kernel='rbf', gamma=0.1, C=1.0)
svm_rbf.fit(X_train, y_train)

# Vecteurs de support
n_support = svm_rbf.n_support_
print(f"Nombre de vecteurs de support: {n_support}")
```

---

## üí° Comparaison des M√©thodes

| M√©thode | Type | Fronti√®re | Probabilit√©s |
|---------|------|-----------|--------------|
| Logistic | Discriminatif | Lin√©aire | Oui |
| LDA | G√©n√©ratif | Lin√©aire | Oui |
| SVM | Discriminatif | Non lin√©aire (kernel) | Non (direct) |

---

[‚¨ÖÔ∏è Chapitre pr√©c√©dent](./chapitre-07-regression-lineaire.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](./chapitre-09-plus-proches-voisins.md)

