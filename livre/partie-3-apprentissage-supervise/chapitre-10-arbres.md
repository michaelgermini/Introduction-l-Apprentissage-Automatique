# Chapitre 10 : Algorithmes BasÃ©s sur les Arbres

## ğŸ“š Introduction

Les arbres de dÃ©cision et leurs extensions (forÃªts alÃ©atoires, boosting) sont parmi les algorithmes les plus populaires en ML.

## ğŸ—ºï¸ Carte Mentale : MÃ©thodes Ã  Base d'Arbres

```
                ALGORITHMES BASÃ‰S SUR LES ARBRES
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
    ARBRE SIMPLE        BAGGING            BOOSTING
        â”‚                   â”‚                   â”‚
    CART                Random           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               Forest           â”‚               â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”              â”‚          AdaBoost      Gradient
    â”‚       â”‚          Ensemble        â”‚            Boosting
Classif. RÃ©gres.      parallÃ¨le    SÃ©quentiel         â”‚
    â”‚       â”‚         (varianceâ†“)    reweight      XGBoost
  Gini    MSE            â”‚          samples       LightGBM
Entropy  MAE         Bootstrap         â”‚           CatBoost
                     + Random       Focus on
                      Features      errors
```

## ğŸ“Š Tableau Comparatif : Arbres et Ensembles

| **MÃ©thode** | **Type** | **Biais** | **Variance** | **Vitesse** | **InterprÃ©tabilitÃ©** | **Usage** |
|------------|---------|----------|-------------|-----------|-------------------|-----------|
| **Arbre Simple** | Base | Moyen | â¬†ï¸ Ã‰levÃ©e | âš¡âš¡âš¡ Rapide | âœ“âœ“âœ“ Excellente | Exploration |
| **Random Forest** | Bagging | Moyen | âœ“ Faible | âš¡âš¡ Moyen | âš ï¸ Moyenne | Production standard |
| **AdaBoost** | Boosting | âœ“ Faible | Moyen | âš¡âš¡ Moyen | âš ï¸ Difficile | Perf. Ã©levÃ©e |
| **XGBoost** | Gradient Boosting | âœ“âœ“ TrÃ¨s faible | âœ“ Faible | âš¡âš¡ Moyen | âš ï¸ Difficile | CompÃ©titions |
| **LightGBM** | Gradient Boosting | âœ“âœ“ TrÃ¨s faible | âœ“ Faible | âš¡âš¡âš¡ Rapide | âš ï¸ Difficile | Big Data |

## ğŸ“ Visualisation : Arbre de DÃ©cision

### Structure d'un Arbre Binaire

```
                    [Racine]
                   Feature_3 â‰¤ 2.5 ?
                    /          \
                  OUI           NON
                  /              \
          [NÅ“ud gauche]      [NÅ“ud droit]
         Feature_1 â‰¤ 5.0 ?  Feature_2 â‰¤ 3.5 ?
            /      \           /        \
          OUI      NON       OUI        NON
          /         \        /           \
    [Feuille]  [Feuille] [Feuille]  [Feuille]
     Classe A   Classe B  Classe A   Classe C
     (50/50)    (30/32)   (45/47)    (80/80)
     PuretÃ©:     PuretÃ©:   PuretÃ©:    PuretÃ©:
      100%        93.8%     95.7%      100%

Notation :
  - NÅ“ud : Condition de split (feature + seuil)
  - Feuille : PrÃ©diction finale (classe majoritaire ou moyenne)
  - (n_correct/n_total) : Ã‰chantillons dans la feuille
```

### Partitionnement de l'Espace

```
    Feature_2
        â”‚
    10  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  â”‚        â”‚         â”‚
     8  â”‚  â”‚   C    â”‚    B    â”‚  Split 1: Feature_2 = 7
        â”‚  â”‚        â”‚         â”‚
     6  â”‚  â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
        â”‚  â”‚                  â”‚  Split 2: Feature_1 = 5
     4  â”‚  â”‚        A         â”‚
        â”‚  â”‚                  â”‚
     2  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚
     0  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Feature_1
        0    5         10

Chaque rÃ©gion = une feuille de l'arbre
FrontiÃ¨res de dÃ©cision = orthogonales aux axes
```

## ğŸ¯ CritÃ¨res de Division

### Pour Classification

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  GINI IMPURITY                                         â”‚
â”‚  Gini = 1 - Î£â‚– pâ‚–Â²                                    â”‚
â”‚                                                        â”‚
â”‚  Exemple : NÅ“ud avec [50 Class A, 30 Class B]         â”‚
â”‚  p_A = 50/80 = 0.625                                  â”‚
â”‚  p_B = 30/80 = 0.375                                  â”‚
â”‚  Gini = 1 - (0.625Â² + 0.375Â²) = 0.469                â”‚
â”‚                                                        â”‚
â”‚  NÅ“ud pur (une seule classe) â†’ Gini = 0              â”‚
â”‚  NÅ“ud Ã©quilibrÃ© (50/50) â†’ Gini = 0.5                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ENTROPY (Information Gain)                            â”‚
â”‚  H = -Î£â‚– pâ‚– logâ‚‚(pâ‚–)                                  â”‚
â”‚                                                        â”‚
â”‚  MÃªme exemple :                                        â”‚
â”‚  H = -(0.625 logâ‚‚(0.625) + 0.375 logâ‚‚(0.375))        â”‚
â”‚    â‰ˆ 0.954                                            â”‚
â”‚                                                        â”‚
â”‚  Information Gain = H_parent - Î£ (n_child/n) H_child  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Pour RÃ©gression

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MEAN SQUARED ERROR (MSE)                              â”‚
â”‚  MSE = (1/n) Î£áµ¢ (yáµ¢ - È³)Â²                            â”‚
â”‚                                                        â”‚
â”‚  Exemple : NÅ“ud avec valeurs [1, 2, 5, 7, 9]         â”‚
â”‚  È³ = (1+2+5+7+9)/5 = 4.8                             â”‚
â”‚  MSE = [(1-4.8)Â² + (2-4.8)Â² + ... + (9-4.8)Â²]/5      â”‚
â”‚      = [14.44 + 7.84 + 0.04 + 4.84 + 17.64]/5        â”‚
â”‚      = 8.96                                            â”‚
â”‚                                                        â”‚
â”‚  On cherche le split qui minimise : MSE_total         â”‚
â”‚  MSE_total = (n_L/n)Â·MSE_L + (n_R/n)Â·MSE_R           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸŒ³ Comparaison Visuelle : Ensemble Methods

```
    ARBRE SIMPLE                RANDOM FOREST (Bagging)
    
         ğŸŒ³                     ğŸŒ³  ğŸŒ³  ğŸŒ³  ğŸŒ³  ğŸŒ³
      Un arbre                  100 arbres
      profond                   indÃ©pendants
         â”‚                            â”‚
    Variance Ã©levÃ©e            Moyenne/Vote â†’ Variance rÃ©duite
    Overfitting                Meilleure gÃ©nÃ©ralisation


    ADABOOST (Boosting)         GRADIENT BOOSTING
    
    ğŸŒ± â†’ ğŸŒ± â†’ ğŸŒ± â†’ ğŸŒ±           ğŸŒ± â†’ ğŸŒ± â†’ ğŸŒ± â†’ ğŸŒ±
    t=1  t=2  t=3  t=4         t=1  t=2  t=3  t=4
    â”‚    â”‚    â”‚    â”‚           â”‚    â”‚    â”‚    â”‚
  Focus  sur   les  erreurs    Fit residuals Ã  chaque Ã©tape
  Reweight samples             Apprend gradient de la loss
  SÃ©quentiel                   SÃ©quentiel
```

## ğŸ“ˆ Ã‰volution de l'Erreur

```
    Erreur
      â”‚
      â”‚  â”€â”€â”€â”€ Arbre simple (test)
      â”‚ â•²
  1.0 â”‚  â•²     â”€â”€â”€â”€ Random Forest
      â”‚   â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚         â”€â”€â”€â”€ XGBoost
  0.5 â”‚          â•²â•²
      â”‚            â•²â•²â•²____________
      â”‚               â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²â•²
  0.0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Nombre d'arbres
       1    10    50   100   500

Random Forest : Converge rapidement, plateau
XGBoost : Continue Ã  amÃ©liorer, risque overfit si trop d'arbres
```

---

## 10.1 Partitionnement RÃ©cursif

### 10.1.1 Arbres de DÃ©cision Binaires

**Structure** : Arbre binaire oÃ¹ :
- NÅ“uds internes : tests sur les features
- Feuilles : prÃ©dictions

### 10.1.2 Algorithme d'EntraÃ®nement

**CART** (Classification And Regression Trees) :

1. Choisir la meilleure division (feature + seuil)
2. Diviser les donnÃ©es
3. RÃ©pÃ©ter rÃ©cursivement

**CritÃ¨re de division** :

**RÃ©gression** (MSE) :
```
Impurity = Î£_{samples} (y - È³)Â²
```

**Classification** (Gini) :
```
Gini = 1 - Î£_c p_cÂ²
```

```python
from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor

# Classification
tree_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=20)
tree_clf.fit(X_train, y_train)

# Visualisation
from sklearn.tree import plot_tree
import matplotlib.pyplot as plt

plt.figure(figsize=(20, 10))
plot_tree(tree_clf, filled=True, feature_names=feature_names)
plt.show()
```

### 10.1.3 RÃ¨gle d'ArrÃªt

**CritÃ¨res** :
- Profondeur maximale
- Nombre minimal d'Ã©chantillons par nÅ“ud
- AmÃ©lioration minimale

### 10.1.4 Ã‰lagage (Pruning)

**Cost-complexity pruning** :
```
R_Î±(T) = R(T) + Î±|T|
```

oÃ¹ |T| est le nombre de feuilles.

```python
# Avec Ã©lagage
tree = DecisionTreeClassifier(ccp_alpha=0.01)
tree.fit(X_train, y_train)
```

---

## 10.2 ForÃªts AlÃ©atoires

### 10.2.1 Bagging

**Bootstrap Aggregating** :
1. GÃ©nÃ©rer B Ã©chantillons bootstrap
2. EntraÃ®ner un arbre sur chaque Ã©chantillon
3. PrÃ©diction = moyenne/vote majoritaire

```python
from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor

# Classification
rf = RandomForestClassifier(
    n_estimators=100,
    max_depth=10,
    min_samples_split=5,
    max_features='sqrt',
    random_state=42
)
rf.fit(X_train, y_train)

# Importance des variables
importances = rf.feature_importances_
```

### 10.2.2 Randomisation des Features

Ã€ chaque division, considÃ©rer seulement âˆšp features alÃ©atoires (classification) ou p/3 (rÃ©gression).

**Avantages** :
- RÃ©duit la corrÃ©lation entre arbres
- AmÃ©liore la gÃ©nÃ©ralisation

---

## 10.3 Adaboost

### 10.3.1 Principe

**Boosting** : Combiner sÃ©quentiellement des apprenants faibles.

### 10.3.2 Algorithme Adaboost

```
Pour m = 1 Ã  M:
    1. EntraÃ®ner h_m avec poids w_i
    2. Calculer erreur Îµ_m = Î£ w_i Â· 1_{y_i â‰  h_m(x_i)} / Î£ w_i
    3. Î±_m = Â½ log((1 - Îµ_m) / Îµ_m)
    4. Mettre Ã  jour: w_i â† w_i Â· exp(-Î±_m y_i h_m(x_i))

PrÃ©diction: sign(Î£_m Î±_m h_m(x))
```

```python
from sklearn.ensemble import AdaBoostClassifier

ada = AdaBoostClassifier(
    base_estimator=DecisionTreeClassifier(max_depth=1),
    n_estimators=50,
    learning_rate=1.0
)
ada.fit(X_train, y_train)
```

---

## 10.4 Gradient Boosting

### Principe

**IdÃ©e** : Ajuster sÃ©quentiellement la rÃ©siduelle.

```
f_0(x) = 0
Pour m = 1 Ã  M:
    r_i = y_i - f_{m-1}(x_i)
    h_m = fit(X, r)
    f_m(x) = f_{m-1}(x) + Î· Â· h_m(x)
```

```python
from sklearn.ensemble import GradientBoostingRegressor

gbr = GradientBoostingRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=3,
    subsample=0.8
)
gbr.fit(X_train, y_train)
```

### XGBoost

**Optimisations** :
- RÃ©gularisation L1/L2
- Gestion des valeurs manquantes
- ParallÃ©lisation

```python
import xgboost as xgb

xgb_model = xgb.XGBRegressor(
    n_estimators=100,
    learning_rate=0.1,
    max_depth=5,
    subsample=0.8,
    colsample_bytree=0.8,
    reg_alpha=0.1,
    reg_lambda=1.0
)
xgb_model.fit(X_train, y_train)
```

---

## ğŸ’¡ Comparaison

| MÃ©thode | Type | Force | Faiblesse |
|---------|------|-------|-----------|
| Arbre | Non paramÃ©trique | InterprÃ©table | Instable |
| Random Forest | Ensemble | Robuste | BoÃ®te noire |
| Adaboost | Boosting | PrÃ©cis | Sensible au bruit |
| Gradient Boosting | Boosting | TrÃ¨s prÃ©cis | Temps d'entraÃ®nement |

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-09-plus-proches-voisins.md) | [Retour](../README.md) | [Suite â¡ï¸](./chapitre-11-reseaux-neurones.md)

