# Chapitre 2 : Analyse Matricielle

## ğŸ“š Introduction

L'analyse matricielle est fondamentale en machine learning. Les matrices permettent de reprÃ©senter les donnÃ©es, les transformations linÃ©aires et les modÃ¨les. Ce chapitre couvre les outils matriciels essentiels pour comprendre les algorithmes d'apprentissage.

## ğŸ—ºï¸ Carte Mentale : Analyse Matricielle

```
                    ANALYSE MATRICIELLE
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
   DÃ‰COMPOSITIONS        NORMES            APPLICATIONS
        â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚           â”‚       â”‚           â”‚       â”‚
  SVD    Valeurs     Frobenius  Spectrale  PCA   Compression
         Propres       â”‚         â”‚               d'Images
    â”‚       â”‚          Lâ‚‚        Ïƒ_max          â”‚
  A=UÎ£Váµ€  A=QÎ›Qáµ€     Matrix     â€–Aâ€–â‚‚       RÃ©duction
                      Norm                  Dimension
```

## ğŸ“Š Tableau Comparatif : DÃ©compositions Matricielles

| **DÃ©composition** | **Formule** | **Conditions** | **ComplexitÃ©** | **Applications ML** |
|------------------|------------|---------------|---------------|-------------------|
| **Valeurs propres** | A = QÎ›Qáµ€ | A symÃ©trique | O(nÂ³) | PCA, Spectral clustering |
| **SVD** | A = UÎ£Váµ€ | Toute matrice | O(mnÂ²) | Recommandation, compression |
| **QR** | A = QR | Toute matrice | O(mnÂ²) | RÃ©gression, moindres carrÃ©s |
| **Cholesky** | A = LLáµ€ | A dÃ©finie positive | O(nÂ³/3) | Simulation, optimisation |
| **LU** | A = LU | A inversible | O(nÂ³/3) | SystÃ¨mes linÃ©aires |

## ğŸ“ Visualisation : SVD (DÃ©composition en Valeurs SinguliÃ¨res)

```
                DÃ‰COMPOSITION SVD : A = U Î£ Váµ€
                
    A           =       U          Ã—      Î£        Ã—       Váµ€
  (mÃ—n)               (mÃ—m)            (mÃ—n)             (nÃ—n)
                                                    
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         â”‚     â”‚          â”‚     â”‚Ïƒâ‚  0   0 â€¦â”‚    â”‚          â”‚
â”‚    A    â”‚  =  â”‚    U     â”‚  Ã—  â”‚0  Ïƒâ‚‚  0  â”‚ Ã—  â”‚    Váµ€    â”‚
â”‚         â”‚     â”‚          â”‚     â”‚0   0  Ïƒâ‚ƒ â”‚    â”‚          â”‚
â”‚         â”‚     â”‚          â”‚     â”‚â‹®   â‹®   â‹® â”‚    â”‚          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚0   0   0 â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
DonnÃ©es         Rotation         Ã‰tirement      Rotation
originales      gauche           (scaling)      droite

InterprÃ©tation gÃ©omÃ©trique :
  1. Váµ€ : Rotation dans l'espace d'entrÃ©e
  2. Î£  : Ã‰tirement selon les directions principales
  3. U  : Rotation dans l'espace de sortie
```

## ğŸ”¢ Tableau des Normes Matricielles

| **Norme** | **DÃ©finition** | **Formule** | **InterprÃ©tation** | **Usage ML** |
|-----------|---------------|------------|-------------------|-------------|
| **Frobenius** | â€–Aâ€–_F | âˆš(Î£áµ¢â±¼ aáµ¢â±¼Â²) | Norme Lâ‚‚ des Ã©lÃ©ments | RÃ©gularisation |
| **Spectrale** | â€–Aâ€–â‚‚ | Ïƒ_max(A) | Plus grande valeur singuliÃ¨re | StabilitÃ© |
| **NuclÃ©aire** | â€–Aâ€–_* | Î£áµ¢ Ïƒáµ¢(A) | Somme val. singuliÃ¨res | Rang faible |
| **Lâ‚** | â€–Aâ€–â‚ | max_j Î£áµ¢ \|aáµ¢â±¼\| | Max somme colonnes | SparsitÃ© |
| **Lâˆ** | â€–Aâ€–âˆ | max_i Î£â±¼ \|aáµ¢â±¼\| | Max somme lignes | Analyse erreur |

---

## 2.1 Notation et Faits de Base

### Matrices et OpÃ©rations

#### Notation
- **A âˆˆ â„áµË£â¿** : matrice mÃ—n
- **Aáµ€** : transposÃ©e de A
- **Aâ»Â¹** : inverse de A (si elle existe)
- **tr(A)** : trace de A (somme des Ã©lÃ©ments diagonaux)
- **det(A)** : dÃ©terminant de A
- **rank(A)** : rang de A

#### Matrices DÃ©finies Positives

Une matrice symÃ©trique A est **dÃ©finie positive** (A â‰» 0) si :
```
xáµ€Ax > 0  pour tout x â‰  0
```

**Semi-dÃ©finie positive** (A âª° 0) si :
```
xáµ€Ax â‰¥ 0  pour tout x
```

**PropriÃ©tÃ©s importantes** :
- A â‰» 0 âŸº toutes les valeurs propres de A sont strictement positives
- A âª° 0 âŸº toutes les valeurs propres de A sont non-nÃ©gatives

### DÃ©composition en Valeurs Propres

Pour une matrice symÃ©trique A âˆˆ â„â¿Ë£â¿ :
```
A = QÎ›Qáµ€
```
oÃ¹ :
- Q est orthogonale (Qáµ€Q = I)
- Î› = diag(Î»â‚, Î»â‚‚, ..., Î»â‚™) contient les valeurs propres

**Exemple Python** :
```python
import numpy as np

A = np.array([[4, 2], [2, 3]])
eigenvalues, Q = np.linalg.eigh(A)  # Pour matrices symÃ©triques
Lambda = np.diag(eigenvalues)

# VÃ©rification
reconstructed = Q @ Lambda @ Q.T
print(np.allclose(A, reconstructed))  # True
```

### DÃ©composition en Valeurs SinguliÃ¨res (SVD)

Pour toute matrice A âˆˆ â„áµË£â¿ :
```
A = UÎ£Váµ€
```
oÃ¹ :
- U âˆˆ â„áµË£áµ orthogonale
- V âˆˆ â„â¿Ë£â¿ orthogonale  
- Î£ âˆˆ â„áµË£â¿ diagonale avec Ïƒâ‚ â‰¥ Ïƒâ‚‚ â‰¥ ... â‰¥ 0

**Valeurs singuliÃ¨res** : Ïƒáµ¢ = âˆšÎ»áµ¢ oÃ¹ Î»áµ¢ sont les valeurs propres de Aáµ€A.

**Application en ML** : La SVD est utilisÃ©e dans :
- La rÃ©duction de dimension (PCA)
- La recommandation (factorisation matricielle)
- Le dÃ©bruitage de donnÃ©es

---

## 2.2 L'InÃ©galitÃ© de Trace

### ThÃ©orÃ¨me Fondamental

Pour des matrices A, B âˆˆ â„â¿Ë£â¿ symÃ©triques dÃ©finies positives :

```
tr(AB) â‰¤ tr(A)tr(B)
```

avec Ã©galitÃ© si et seulement si A et B commutent (AB = BA).

### InÃ©galitÃ© de von Neumann

Pour A, B âˆˆ â„â¿Ë£â¿ :

```
tr(AB) â‰¤ Î£áµ¢ Ïƒáµ¢(A)Ïƒáµ¢(B)
```

oÃ¹ Ïƒáµ¢(A) dÃ©signent les valeurs singuliÃ¨res de A ordonnÃ©es par ordre dÃ©croissant.

### InÃ©galitÃ© d'Araki-Lieb-Thirring

Pour A âª° 0 et B âª° 0 :

```
tr(Aáµ–Báµ–) â‰¥ tr((AB)áµ–)  pour 0 < p < 1
tr(Aáµ–Báµ–) â‰¤ tr((AB)áµ–)  pour p â‰¥ 1
```

**Application** : Ces inÃ©galitÃ©s sont utilisÃ©es dans l'analyse de convergence des algorithmes d'optimisation.

### Lemme de Trace

Pour A âˆˆ â„áµË£â¿ et B âˆˆ â„â¿Ë£áµ :

```
tr(AB) = tr(BA)
```

**PropriÃ©tÃ©s utiles** :
- tr(ABC) = tr(CAB) = tr(BCA)
- tr(Aáµ€B) = âŸ¨A, BâŸ©_F (produit scalaire de Frobenius)

---

## 2.3 Applications

### RÃ©gression Ridge

Dans la rÃ©gression ridge, on minimise :

```
â€–XÎ² - yâ€–Â² + Î»â€–Î²â€–Â²
```

La solution est donnÃ©e par :

```
Î²Ì‚ = (Xáµ€X + Î»I)â»Â¹Xáµ€y
```

**Analyse matricielle** : 
- Xáµ€X est semi-dÃ©finie positive
- Xáµ€X + Î»I est dÃ©finie positive pour Î» > 0
- L'inverse existe toujours

### Analyse en Composantes Principales (PCA)

Pour des donnÃ©es X âˆˆ â„â¿Ë£áµˆ (n observations, d variables) :

1. Centrer les donnÃ©es : XÌƒ = X - Î¼ oÃ¹ Î¼ est la moyenne
2. Calculer la matrice de covariance : C = (1/n)XÌƒáµ€XÌƒ
3. DÃ©composer : C = QÎ›Qáµ€
4. Projeter sur les k premiers vecteurs propres

**RÃ©duction de dimension** :
```
X_rÃ©duite = XÌƒ Â· Q[:, :k]
```

**Exemple Python** :
```python
from sklearn.decomposition import PCA
import numpy as np

# DonnÃ©es
X = np.random.randn(100, 10)

# PCA Ã  3 composantes
pca = PCA(n_components=3)
X_reduced = pca.fit_transform(X)

print("Variance expliquÃ©e:", pca.explained_variance_ratio_)
```

### DÃ©composition de Cholesky

Pour A âª° 0, il existe une matrice triangulaire infÃ©rieure L telle que :

```
A = LLáµ€
```

**UtilitÃ©** :
- RÃ©solution efficace de systÃ¨mes linÃ©aires
- Simulation de variables gaussiennes multivariÃ©es
- Optimisation numÃ©rique

**Exemple** : GÃ©nÃ©rer X ~ N(Î¼, Î£)
```python
import numpy as np

mu = np.array([1, 2])
Sigma = np.array([[2, 0.5], [0.5, 1]])

# DÃ©composition de Cholesky
L = np.linalg.cholesky(Sigma)

# GÃ©nÃ©rer Ã©chantillon
z = np.random.randn(2)  # N(0, I)
x = mu + L @ z          # N(Î¼, Î£)
```

---

## 2.4 Normes Matricielles

### Norme de Frobenius

```
â€–Aâ€–_F = âˆš(Î£áµ¢â±¼ aáµ¢â±¼Â²) = âˆštr(Aáµ€A)
```

**PropriÃ©tÃ©s** :
- â€–Aâ€–_FÂ² = Î£áµ¢ Ïƒáµ¢Â² (somme des carrÃ©s des valeurs singuliÃ¨res)
- â€–ABâ€–_F â‰¤ â€–Aâ€–_F â€–Bâ€–_F

### Norme Spectrale

```
â€–Aâ€–â‚‚ = max_{â€–xâ€–=1} â€–Axâ€– = Ïƒâ‚(A)
```

La norme spectrale est la plus grande valeur singuliÃ¨re.

**PropriÃ©tÃ© importante** :
```
â€–ABâ€–â‚‚ â‰¤ â€–Aâ€–â‚‚ â€–Bâ€–â‚‚
```

### Norme NuclÃ©aire

```
â€–Aâ€–_* = Î£áµ¢ Ïƒáµ¢(A)
```

La norme nuclÃ©aire est la somme des valeurs singuliÃ¨res.

**Application en ML** : UtilisÃ©e pour la rÃ©gularisation dans la complÃ©tion de matrices :

```
minimize  â€–Aâ€–_*  subject to  A_Î© = M_Î©
```

oÃ¹ Î© reprÃ©sente les entrÃ©es observÃ©es.

### Comparaison des Normes

Pour A âˆˆ â„áµË£â¿ avec r = min(m, n) :

```
â€–Aâ€–â‚‚ â‰¤ â€–Aâ€–_F â‰¤ âˆšr â€–Aâ€–â‚‚
â€–Aâ€–â‚‚ â‰¤ â€–Aâ€–_* â‰¤ âˆšr â€–Aâ€–_F
```

**Tableau rÃ©capitulatif** :

| Norme | DÃ©finition | Valeurs singuliÃ¨res |
|-------|------------|---------------------|
| Frobenius | âˆšÎ£áµ¢â±¼ aáµ¢â±¼Â² | âˆšÎ£áµ¢ Ïƒáµ¢Â² |
| Spectrale | max â€–Axâ€–/â€–xâ€– | Ïƒâ‚ |
| NuclÃ©aire | - | Î£áµ¢ Ïƒáµ¢ |

---

## 2.5 Approximation de Rang Faible

### ThÃ©orÃ¨me de Eckart-Young-Mirsky

La meilleure approximation de rang k de A (au sens de Frobenius ou spectral) est :

```
A_k = Î£áµ¢â‚Œâ‚áµ Ïƒáµ¢ uáµ¢váµ¢áµ€
```

oÃ¹ A = Î£áµ¢ Ïƒáµ¢ uáµ¢váµ¢áµ€ est la SVD de A.

**Erreur d'approximation** :

- Norme de Frobenius : â€–A - A_kâ€–_F = âˆš(Î£áµ¢â‚Œâ‚–â‚Šâ‚â¿ Ïƒáµ¢Â²)
- Norme spectrale : â€–A - A_kâ€–â‚‚ = Ïƒâ‚–â‚Šâ‚

### ImplÃ©mentation Python

```python
import numpy as np

def low_rank_approximation(A, k):
    """
    Approximation de rang k de la matrice A
    """
    U, s, Vt = np.linalg.svd(A, full_matrices=False)
    
    # Garder les k premiÃ¨res composantes
    U_k = U[:, :k]
    s_k = s[:k]
    Vt_k = Vt[:k, :]
    
    # Reconstruction
    A_k = U_k @ np.diag(s_k) @ Vt_k
    
    # Erreur
    error = np.linalg.norm(A - A_k, 'fro')
    
    return A_k, error

# Exemple
A = np.random.randn(100, 50)
A_k, error = low_rank_approximation(A, k=10)
print(f"Erreur de reconstruction: {error:.4f}")
```

### Application : Compression d'Images

Une image en niveaux de gris peut Ãªtre vue comme une matrice. L'approximation de rang faible permet de la compresser :

```python
from PIL import Image
import numpy as np
import matplotlib.pyplot as plt

# Charger image
img = Image.open('image.jpg').convert('L')
A = np.array(img)

# Approximations de diffÃ©rents rangs
for k in [5, 10, 20, 50]:
    A_k, _ = low_rank_approximation(A, k)
    
    # Taux de compression
    m, n = A.shape
    compression = (k * (m + n)) / (m * n) * 100
    
    print(f"Rang {k}: compression {compression:.1f}%")
    
    # Afficher
    plt.figure()
    plt.imshow(A_k, cmap='gray')
    plt.title(f'Rang {k}')
    plt.show()
```

### Factorisation Matricielle en ML

De nombreuses mÃ©thodes ML utilisent la factorisation matricielle :

**SystÃ¨mes de recommandation** :
```
R â‰ˆ UV^T
```
oÃ¹ :
- R : matrice utilisateurÃ—item
- U : facteurs latents utilisateurs
- V : facteurs latents items

**Algorithme gradient descent** :
```python
def matrix_factorization(R, k, steps=1000, lr=0.01, reg=0.01):
    m, n = R.shape
    U = np.random.randn(m, k)
    V = np.random.randn(n, k)
    
    for step in range(steps):
        # Gradient par rapport Ã  U
        grad_U = (U @ V.T - R) @ V + reg * U
        U -= lr * grad_U
        
        # Gradient par rapport Ã  V
        grad_V = (U @ V.T - R).T @ U + reg * V
        V -= lr * grad_V
        
        if step % 100 == 0:
            loss = np.linalg.norm(R - U @ V.T, 'fro')**2
            print(f"Step {step}, Loss: {loss:.4f}")
    
    return U, V
```

---

## ğŸ’¡ Points ClÃ©s Ã  Retenir

1. **SVD** : Outil fondamental pour l'analyse de matrices
2. **Normes matricielles** : Mesures de "taille" et rÃ©gularisation
3. **Approximation de rang faible** : Compression et rÃ©duction de dimension
4. **Matrices dÃ©finies positives** : Garantissent la convexitÃ©
5. **InÃ©galitÃ©s de trace** : Outils d'analyse thÃ©orique

---

## ğŸ“ Exercices

### Exercice 1
DÃ©montrez que pour une matrice symÃ©trique A, â€–Aâ€–â‚‚ = max |Î»áµ¢(A)|.

### Exercice 2
ImplÃ©mentez la PCA from scratch en utilisant la SVD.

### Exercice 3
Pour une matrice de covariance Î£, montrez que Î£ âª° 0.

### Exercice 4
Ã‰crivez un algorithme de complÃ©tion de matrice en utilisant la rÃ©gularisation nuclÃ©aire.

---

## ğŸ”— Applications en Machine Learning

- **PCA** : RÃ©duction de dimension
- **SVD** : SystÃ¨mes de recommandation, traitement du langage naturel
- **Factorisation matricielle** : Apprentissage de reprÃ©sentations latentes
- **RÃ©gularisation** : Ridge regression, Lasso
- **Compression** : Approximation de rang faible

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-01-notations-prerequis.md) | [Retour Ã  la table des matiÃ¨res](../README.md) | [Chapitre suivant â¡ï¸](./chapitre-03-optimisation.md)

