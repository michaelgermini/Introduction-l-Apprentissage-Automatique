# Chapitre 5 : PrÃ©diction - Concepts de Base

## ğŸ“š Introduction

Ce chapitre Ã©tablit les fondements thÃ©oriques de la prÃ©diction statistique, qui est au cÅ“ur du machine learning supervisÃ©.

## ğŸ—ºï¸ Carte Mentale : PrÃ©diction Statistique

```
                    PRÃ‰DICTION STATISTIQUE
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
    THÃ‰ORIE            PRATIQUE            Ã‰VALUATION
        â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚           â”‚       â”‚           â”‚       â”‚
 Bayes    Risque     ERM      Model      Train/Test  Cross
Optimal  Empirique    â”‚      Selection      Split   Validation
    â”‚       â”‚         â”‚           â”‚           â”‚          â”‚
  f*(x)   RÌ‚(f)   argmin    Complexity   Holdout    k-Fold
         = 1/n Î£ L   RÌ‚(f)     Control      Set       CV
```

## ğŸ“ Workflow de PrÃ©diction

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  PIPELINE DE PRÃ‰DICTION ML                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    1. DONNÃ‰ES                2. MODÃˆLE              3. Ã‰VALUATION
         â”‚                        â”‚                        â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
    â”‚  (X, Y) â”‚              â”‚  f(X)   â”‚            â”‚   Risque  â”‚
    â”‚ n samplesâ”‚  â”€â”€â”€â”€â”€â†’     â”‚PrÃ©dicteurâ”‚  â”€â”€â”€â”€â”€â†’   â”‚   RÌ‚(f)    â”‚
    â”‚ p featuresâ”‚            â”‚         â”‚            â”‚           â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                        â”‚                        â”‚
         â”‚                        â”‚                        â”‚
    [Split Data]            [Training]            [Validation]
         â”‚                        â”‚                        â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”              â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”            â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚ Train   â”‚              â”‚ Minimizeâ”‚            â”‚  Compare  â”‚
    â”‚  80%    â”‚              â”‚  RÌ‚(f)   â”‚            â”‚  Models   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜            â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Test   â”‚                                     â”‚  Select   â”‚
    â”‚  20%    â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’  â”‚   Best    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ“Š Tableau : Fonctions de Perte

| **Type** | **Nom** | **Formule** | **PropriÃ©tÃ©s** | **Usage** |
|----------|---------|------------|---------------|-----------|
| **RÃ©gression** | MSE | (y - Å·)Â² | Sensible outliers | Standard |
| **RÃ©gression** | MAE | \|y - Å·\| | Robuste | Outliers prÃ©sents |
| **RÃ©gression** | Huber | {Â½(y-Å·)Â² si \|y-Å·\|â‰¤Î´; Î´\|y-Å·\|-Â½Î´Â² sinon} | Compromis | Robustesse |
| **Classification** | 0-1 Loss | 1_{yâ‰ Å·} | Non diffÃ©rentiable | ThÃ©orique |
| **Classification** | Log-Loss | -log P(y\|x) | Probabiliste | Pratique |
| **Classification** | Hinge | max(0, 1-yÂ·Å·) | Marge | SVM |

## ğŸ¯ HiÃ©rarchie des Risques

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              DÃ‰COMPOSITION DU RISQUE                          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Risque RÃ©el R(f)
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚                                     â”‚
    Risque de Bayes R*              ExcÃ¨s de Risque
    (IrrÃ©ductible)                  R(f) - R*
         â”‚                                â”‚
    Bruit dans les                       â”‚
    donnÃ©es                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    Variance ÏƒÂ²              â”‚                      â”‚
                        Approximation           Estimation
                        Error                   Error
                             â”‚                      â”‚
                        Biais du modÃ¨le        Variance de
                        (â„± trop simple)        l'estimateur
                                              (n trop petit)

Objectif : Minimiser R(f) - R*
  â€¢ Choisir â„± assez riche (â†“ biais)
  â€¢ Avoir assez de donnÃ©es (â†“ variance)
  â€¢ RÃ©gulariser si besoin
```

## ğŸ“ˆ Visualisation : Erreur de PrÃ©diction

```
    Erreur
      â”‚
      â”‚     â•±â•² Risque rÃ©el R(f)
      â”‚    â•±  â•²
  1.0 â”‚   â•±    â•²
      â”‚  â•±      â•²â”€â”€â”€â”€â”€â”€ Risque empirique RÌ‚(f)
      â”‚ â•±        â•²â•²
      â”‚â•±__________â•²â•²___ Risque de Bayes R*
  0.5 â”‚            â•²â•²
      â”‚             â•²â•²
      â”‚              â•²â•²
  0   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ComplexitÃ©
      Simple       Optimal    Complexe

Observations :
  â€¢ RÌ‚(f) â‰¤ R(f) en gÃ©nÃ©ral (optimisme du risque empirique)
  â€¢ Gap = R(f) - RÌ‚(f) (erreur de gÃ©nÃ©ralisation)
  â€¢ R* = meilleur risque possible
```

---

## 5.1 Cadre GÃ©nÃ©ral

### ProblÃ¨me de PrÃ©diction

Nous avons :
- **X** : variable d'entrÃ©e (features)
- **Y** : variable de sortie (target)
- **Distribution conjointe** P(X, Y)

**Objectif** : Construire un prÃ©dicteur f : X â†’ Y qui minimise une fonction de perte L.

**Risque** :
```
R(f) = ğ”¼[L(Y, f(X))]
```

### Fonctions de Perte Courantes

**RÃ©gression** :
- Erreur quadratique : L(y, Å·) = (y - Å·)Â²
- Erreur absolue : L(y, Å·) = |y - Å·|

**Classification** :
- Perte 0-1 : L(y, Å·) = 1_{y â‰  Å·}
- Log-loss : L(y, Å·) = -log P(Y = y|X)

---

## 5.2 PrÃ©dicteur de Bayes

### ThÃ©orÃ¨me

Le **prÃ©dicteur de Bayes** minimise le risque :

**RÃ©gression** (L quadratique) :
```
f*(x) = ğ”¼[Y|X = x]
```

**Classification** (perte 0-1) :
```
f*(x) = argmax_y P(Y = y|X = x)
```

### Risque de Bayes

Le **risque de Bayes** R* = R(f*) est le risque minimal atteignable.

**Erreur irrÃ©ductible** : MÃªme le meilleur modÃ¨le ne peut pas faire mieux que R*.

---

## 5.3 Exemples : Approche BasÃ©e sur des ModÃ¨les

### 5.3.1 ModÃ¨les Gaussiens et NaÃ¯ve Bayes

#### Classification avec Loi Normale

Si X|Y = k ~ N(Î¼â‚–, Î£) :
```
P(Y = k|X = x) âˆ Ï€â‚– Â· exp(-Â½(x - Î¼â‚–)áµ€Î£â»Â¹(x - Î¼â‚–))
```

**NaÃ¯ve Bayes** : Suppose l'indÃ©pendance conditionnelle :
```
P(X|Y = k) = âˆâ±¼ P(Xâ±¼|Y = k)
```

```python
from sklearn.naive_bayes import GaussianNB

# DonnÃ©es
X_train, y_train = ...  # Features et labels

# ModÃ¨le
model = GaussianNB()
model.fit(X_train, y_train)

# PrÃ©diction
y_pred = model.predict(X_test)
```

### 5.3.2 RÃ©gression par Noyaux

```
fÌ‚(x) = Î£áµ¢ wáµ¢(x) yáµ¢
```

oÃ¹ les poids dÃ©pendent de la distance :
```
wáµ¢(x) = K((x - xáµ¢)/h) / Î£â±¼ K((x - xâ±¼)/h)
```

---

## 5.4 Minimisation du Risque Empirique

### 5.4.1 Principes GÃ©nÃ©raux

On n'a pas accÃ¨s Ã  P(X, Y), mais aux donnÃ©es (xâ‚, yâ‚), ..., (xâ‚™, yâ‚™).

**Risque empirique** :
```
RÌ‚(f) = (1/n) Î£áµ¢ L(yáµ¢, f(xáµ¢))
```

**ERM** (Empirical Risk Minimization) :
```
fÌ‚ = argmin_{f âˆˆ â„±} RÌ‚(f)
```

### 5.4.2 Biais et Variance

**DÃ©composition de l'erreur** :
```
ğ”¼[L(Y, fÌ‚(X))] = R* + Approximation + Estimation
```

- **R*** : Risque de Bayes (irrÃ©ductible)
- **Approximation** : inf_{f âˆˆ â„±} R(f) - R*
- **Estimation** : R(fÌ‚) - inf_{f âˆˆ â„±} R(f)

---

## 5.5 Ã‰valuation de l'Erreur

### 5.5.1 Erreur de GÃ©nÃ©ralisation

**ProblÃ¨me** : RÌ‚(fÌ‚) est optimiste car fÌ‚ est ajustÃ© sur les mÃªmes donnÃ©es.

**Erreur de gÃ©nÃ©ralisation** : Ã‰valuer sur de nouvelles donnÃ©es.

### 5.5.2 Validation CroisÃ©e

#### Hold-out

```
Training set (70%) â†’ Ajuster le modÃ¨le
Test set (30%) â†’ Ã‰valuer
```

#### K-Fold Cross-Validation

```python
from sklearn.model_selection import cross_val_score

scores = cross_val_score(model, X, y, cv=5)
print(f"CV Score: {scores.mean():.3f} Â± {scores.std():.3f}")
```

#### Leave-One-Out (LOO)

```
Pour i = 1 Ã  n:
    EntraÃ®ner sur {1, ..., n} \ {i}
    Tester sur i
```

**Erreur LOOCV** :
```
CV = (1/n) Î£áµ¢ L(yáµ¢, fÌ‚^{(-i)}(xáµ¢))
```

---

## ğŸ’¡ Points ClÃ©s

1. **PrÃ©dicteur de Bayes** : Optimal en thÃ©orie
2. **ERM** : Minimiser l'erreur empirique
3. **Compromis biais-variance** : Choix de la classe de fonctions â„±
4. **Validation croisÃ©e** : Estimation non biaisÃ©e de l'erreur

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-04-biais-variance.md) | [Retour](../README.md) | [Suite â¡ï¸](./chapitre-06-noyaux.md)

