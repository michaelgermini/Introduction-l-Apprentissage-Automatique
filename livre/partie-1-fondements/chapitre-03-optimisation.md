# Chapitre 3 : Introduction √† l'Optimisation

## üìö Introduction

L'optimisation est au c≈ìur du machine learning. Entra√Æner un mod√®le revient √† r√©soudre un probl√®me d'optimisation : trouver les param√®tres qui minimisent une fonction de co√ªt. Ce chapitre couvre les algorithmes d'optimisation essentiels utilis√©s en ML.

---

## 3.1 Terminologie de Base

### Probl√®me d'Optimisation

Un probl√®me d'optimisation g√©n√©ral s'√©crit :

```
minimize    f(x)
subject to  x ‚àà C
```

o√π :
- **f : ‚Ñù‚Åø ‚Üí ‚Ñù** est la fonction objectif (fonction de co√ªt)
- **C ‚äÇ ‚Ñù‚Åø** est l'ensemble des contraintes
- **x ‚àà ‚Ñù‚Åø** est la variable d'optimisation

### D√©finitions

**Minimum global** : x* est un minimum global si f(x*) ‚â§ f(x) pour tout x ‚àà C

**Minimum local** : x* est un minimum local s'il existe Œµ > 0 tel que f(x*) ‚â§ f(x) pour tout x ‚àà C ‚à© B(x*, Œµ)

**Point stationnaire** : x tel que ‚àáf(x) = 0

---

## 3.2 Optimisation Sans Contrainte

### 3.2.1 Conditions d'Optimalit√©

#### Condition N√©cessaire du Premier Ordre

Si x* est un minimum local de f diff√©rentiable, alors :
```
‚àáf(x*) = 0
```

#### Condition N√©cessaire du Second Ordre

Si x* est un minimum local de f deux fois diff√©rentiable, alors :
```
‚àáf(x*) = 0  et  Hf(x*) ‚™∞ 0
```

#### Condition Suffisante du Second Ordre

Si ‚àáf(x*) = 0 et Hf(x*) ‚âª 0, alors x* est un minimum local strict.

**Exemple** : Pour f(x) = x¬≤ - 2x + 1
```
‚àáf(x) = 2x - 2 = 0  ‚üπ  x* = 1
H f(x) = 2 > 0  ‚üπ  x* est un minimum global
```

### 3.2.2 Ensembles et Fonctions Convexes

#### Ensemble Convexe

Un ensemble C est **convexe** si pour tous x, y ‚àà C et Œ∏ ‚àà [0,1] :
```
Œ∏x + (1-Œ∏)y ‚àà C
```

**Exemples** :
- ‚Ñù‚Åø est convexe
- {x : Ax ‚â§ b} est convexe (demi-espaces)
- Boules : {x : ‚Äñx - x‚ÇÄ‚Äñ ‚â§ r} est convexe

#### Fonction Convexe

Une fonction f est **convexe** si pour tous x, y et Œ∏ ‚àà [0,1] :
```
f(Œ∏x + (1-Œ∏)y) ‚â§ Œ∏f(x) + (1-Œ∏)f(y)
```

**Propri√©t√© fondamentale** : Pour f convexe diff√©rentiable :
```
f(y) ‚â• f(x) + ‚àáf(x)·µÄ(y - x)  pour tous x, y
```

**Test de convexit√©** : f est convexe ‚ü∫ Hf(x) ‚™∞ 0 pour tout x

**Exemples de fonctions convexes** :
- f(x) = x·µÄAx avec A ‚™∞ 0
- f(x) = ‚Äñx‚Äñ
- f(x) = exp(x)
- f(x) = -log(x) pour x > 0

### 3.2.3 Int√©rieur Relatif

L'**int√©rieur relatif** de C, not√© ri(C), est l'int√©rieur de C dans son affine hull.

**Importance** : Les conditions d'optimalit√© pour les fonctions convexes s'appliquent sur ri(dom f).

### 3.2.4 D√©riv√©es de Fonctions Convexes et Conditions d'Optimalit√©

#### Th√©or√®me Fondamental

Pour f convexe diff√©rentiable, x* est un minimum global si et seulement si :
```
‚àáf(x*) = 0
```

**Cons√©quence** : Pour les fonctions convexes, tout minimum local est global !

### 3.2.5 Direction de Descente et Descente la Plus Rapide

#### Direction de Descente

Un vecteur d est une **direction de descente** en x si :
```
‚àáf(x)·µÄd < 0
```

La **direction de descente la plus rapide** est :
```
d = -‚àáf(x) / ‚Äñ‚àáf(x)‚Äñ
```

### 3.2.6 Descente de Gradient

**Algorithme de descente de gradient** :
```
x_{k+1} = x_k - Œ±_k ‚àáf(x_k)
```

o√π Œ±_k > 0 est le pas d'apprentissage (learning rate).

**Impl√©mentation Python** :
```python
def gradient_descent(f, grad_f, x0, alpha=0.01, max_iter=1000, tol=1e-6):
    """
    Descente de gradient
    
    Args:
        f: fonction objectif
        grad_f: gradient de f
        x0: point initial
        alpha: pas d'apprentissage
        max_iter: nombre maximal d'it√©rations
        tol: tol√©rance pour la convergence
    """
    x = x0.copy()
    history = [x.copy()]
    
    for k in range(max_iter):
        grad = grad_f(x)
        
        # Test de convergence
        if np.linalg.norm(grad) < tol:
            print(f"Convergence √† l'it√©ration {k}")
            break
        
        # Mise √† jour
        x = x - alpha * grad
        history.append(x.copy())
    
    return x, np.array(history)

# Exemple : minimiser f(x) = x^T A x - b^T x
A = np.array([[4, 1], [1, 3]])
b = np.array([1, 2])

f = lambda x: 0.5 * x @ A @ x - b @ x
grad_f = lambda x: A @ x - b

x0 = np.array([0.0, 0.0])
x_opt, history = gradient_descent(f, grad_f, x0, alpha=0.1)
```

#### Th√©or√®me de Convergence

Pour f fortement convexe avec L-Lipschitz gradient :
```
f(x_k) - f(x*) ‚â§ (1 - Œº/L)^k (f(x_0) - f(x*))
```

o√π Œº est le param√®tre de convexit√© forte.

**Taux de convergence** : Lin√©aire (exponentiel en nombre d'it√©rations)

### 3.2.7 Recherche Lin√©aire (Line Search)

Au lieu de fixer Œ±, on peut le choisir √† chaque it√©ration :

#### R√®gle d'Armijo (Backtracking)

```python
def backtracking_line_search(f, x, d, grad, alpha=1.0, beta=0.5, sigma=0.1):
    """
    Recherche lin√©aire avec backtracking
    """
    while f(x + alpha * d) > f(x) + sigma * alpha * grad @ d:
        alpha *= beta
    return alpha
```

#### Recherche Exacte

Minimiser f(x + Œ±d) par rapport √† Œ± :
```
Œ±* = argmin_Œ± f(x + Œ±d)
```

---

## 3.3 Descente de Gradient Stochastique

### 3.3.1 M√©thodes d'Approximation Stochastique

En ML, on minimise souvent :
```
f(Œ∏) = ùîº[‚Ñì(Œ∏; Z)] = (1/n) Œ£·µ¢ ‚Ñì(Œ∏; z·µ¢)
```

**Probl√®me** : Calculer ‚àáf(Œ∏) n√©cessite n √©valuations.

**Solution** : Approximer le gradient par un √©chantillon :
```
‚àáf(Œ∏) ‚âà ‚àá‚Ñì(Œ∏; z_i)  o√π i est choisi al√©atoirement
```

### 3.3.2 Algorithme SGD

```
Œ∏_{k+1} = Œ∏_k - Œ±_k ‚àá‚Ñì(Œ∏_k; z_{i_k})
```

**Impl√©mentation** :
```python
def sgd(data, loss_grad, theta0, epochs=10, batch_size=32, alpha=0.01):
    """
    Descente de gradient stochastique
    """
    theta = theta0.copy()
    n = len(data)
    
    for epoch in range(epochs):
        # M√©langer les donn√©es
        np.random.shuffle(data)
        
        for i in range(0, n, batch_size):
            batch = data[i:i+batch_size]
            
            # Gradient sur le mini-batch
            grad = sum(loss_grad(theta, x) for x in batch) / len(batch)
            
            # Mise √† jour
            theta -= alpha * grad
        
        print(f"Epoch {epoch+1}/{epochs}")
    
    return theta
```

#### Choix du Pas d'Apprentissage

Conditions de Robbins-Monro pour la convergence :
```
Œ£ Œ±_k = ‚àû  et  Œ£ Œ±_k¬≤ < ‚àû
```

**Exemples** :
- Œ±_k = Œ±‚ÇÄ / (1 + k)
- Œ±_k = Œ±‚ÇÄ / ‚àök
- Learning rate schedule : r√©duction par paliers

### 3.3.3 L'Algorithme ADAM

**ADAM** (Adaptive Moment Estimation) combine :
- Momentum : moyenne mobile des gradients
- RMSprop : moyenne mobile des gradients au carr√©

**Algorithme** :
```
m_k = Œ≤‚ÇÅ m_{k-1} + (1-Œ≤‚ÇÅ) g_k
v_k = Œ≤‚ÇÇ v_{k-1} + (1-Œ≤‚ÇÇ) g_k¬≤

mÃÇ_k = m_k / (1 - Œ≤‚ÇÅ^k)
vÃÇ_k = v_k / (1 - Œ≤‚ÇÇ^k)

Œ∏_{k+1} = Œ∏_k - Œ± mÃÇ_k / (‚àövÃÇ_k + Œµ)
```

**Param√®tres standards** :
- Œ≤‚ÇÅ = 0.9
- Œ≤‚ÇÇ = 0.999
- Œµ = 10‚Åª‚Å∏

**Impl√©mentation** :
```python
def adam(data, loss_grad, theta0, epochs=10, alpha=0.001, 
         beta1=0.9, beta2=0.999, eps=1e-8):
    """
    Optimiseur ADAM
    """
    theta = theta0.copy()
    m = np.zeros_like(theta)
    v = np.zeros_like(theta)
    t = 0
    
    for epoch in range(epochs):
        for x in data:
            t += 1
            grad = loss_grad(theta, x)
            
            # Mise √† jour des moments
            m = beta1 * m + (1 - beta1) * grad
            v = beta2 * v + (1 - beta2) * grad**2
            
            # Correction du biais
            m_hat = m / (1 - beta1**t)
            v_hat = v / (1 - beta2**t)
            
            # Mise √† jour des param√®tres
            theta -= alpha * m_hat / (np.sqrt(v_hat) + eps)
    
    return theta
```

---

## 3.4 Optimisation Contrainte

### 3.4.1 Multiplicateurs de Lagrange

Pour minimiser f(x) sous contrainte h(x) = 0 :

**Lagrangien** :
```
‚Ñí(x, Œª) = f(x) + Œª·µÄh(x)
```

**Conditions KKT** (Karush-Kuhn-Tucker) :
```
‚àá_x ‚Ñí(x*, Œª*) = 0
h(x*) = 0
```

**Exemple** : Minimiser f(x, y) = x¬≤ + y¬≤ sous contrainte x + y = 1

```python
# Lagrangien: L = x¬≤ + y¬≤ + Œª(x + y - 1)
# ‚àáL = 0  ‚üπ  2x + Œª = 0, 2y + Œª = 0, x + y = 1
# Solution: x = y = 1/2, Œª = -1
```

### 3.4.2 Contraintes Convexes

Pour f convexe et contraintes convexes g(x) ‚â§ 0, les conditions KKT sont :

```
‚àáf(x*) + Œ£·µ¢ Œª·µ¢* ‚àág·µ¢(x*) = 0
g·µ¢(x*) ‚â§ 0
Œª·µ¢* ‚â• 0
Œª·µ¢* g·µ¢(x*) = 0  (compl√©mentarit√©)
```

**Propri√©t√©** : Si (x*, Œª*) satisfait KKT, alors x* est optimal.

### 3.4.3 Applications

#### SVM (Support Vector Machine)

Probl√®me primal :
```
minimize    (1/2)‚Äñw‚Äñ¬≤
subject to  y·µ¢(w·µÄx·µ¢ + b) ‚â• 1  pour i = 1, ..., n
```

Probl√®me dual :
```
maximize    Œ£·µ¢ Œ±·µ¢ - (1/2)Œ£·µ¢‚±º Œ±·µ¢Œ±‚±ºy·µ¢y‚±º(x·µ¢·µÄx‚±º)
subject to  0 ‚â§ Œ±·µ¢  et  Œ£·µ¢ Œ±·µ¢y·µ¢ = 0
```

### 3.4.4 Descente de Gradient Projet√©e

Pour minimiser f(x) avec x ‚àà C :

```
x_{k+1} = P_C(x_k - Œ±_k ‚àáf(x_k))
```

o√π P_C est la projection sur C.

**Projection sur simplexe** (Œ£·µ¢ x·µ¢ = 1, x·µ¢ ‚â• 0) :
```python
def project_simplex(v):
    """
    Projette v sur le simplexe
    """
    n = len(v)
    u = np.sort(v)[::-1]
    cssv = np.cumsum(u)
    rho = np.nonzero(u * np.arange(1, n+1) > (cssv - 1))[0][-1]
    theta = (cssv[rho] - 1) / (rho + 1)
    return np.maximum(v - theta, 0)
```

---

## 3.5 Probl√®mes Convexes G√©n√©raux

### 3.5.1 √âpigraphes

L'**√©pigraphe** de f est :
```
epi(f) = {(x, t) : f(x) ‚â§ t}
```

**Propri√©t√©** : f est convexe ‚ü∫ epi(f) est convexe.

### 3.5.2 Sous-Gradients

Pour f convexe (pas n√©cessairement diff√©rentiable), g est un **sous-gradient** de f en x si :
```
f(y) ‚â• f(x) + g·µÄ(y - x)  pour tout y
```

**Sous-diff√©rentiel** :
```
‚àÇf(x) = {g : g est un sous-gradient de f en x}
```

**Exemples** :
- f(x) = |x| : ‚àÇf(0) = [-1, 1]
- f(x) = max(x, 0) : ‚àÇf(0) = [0, 1]

### 3.5.3 D√©riv√©es Directionnelles

La **d√©riv√©e directionnelle** de f en x dans la direction v est :
```
f'(x; v) = lim_{t‚Üí0‚Å∫} [f(x + tv) - f(x)] / t
```

Pour f convexe :
```
f'(x; v) = max{g·µÄv : g ‚àà ‚àÇf(x)}
```

### 3.5.4 Descente de Sous-Gradient

Pour f convexe non diff√©rentiable :

```
x_{k+1} = x_k - Œ±_k g_k  o√π g_k ‚àà ‚àÇf(x_k)
```

**Note** : La convergence est plus lente que la descente de gradient (taux en O(1/‚àök)).

### 3.5.5 M√©thodes Proximales

L'**op√©rateur proximal** de f est :
```
prox_f(v) = argmin_x {f(x) + (1/2)‚Äñx - v‚Äñ¬≤}
```

**Algorithme du Gradient Proximal** :
Pour minimiser f(x) + g(x) o√π f est lisse et g convexe :

```
x_{k+1} = prox_{Œ±_k g}(x_k - Œ±_k ‚àáf(x_k))
```

**Exemple : Lasso** (f(x) = ‚ÄñAx - b‚Äñ¬≤, g(x) = Œª‚Äñx‚Äñ‚ÇÅ)

```python
def soft_threshold(x, threshold):
    """
    Op√©rateur proximal de ‚Äñ¬∑‚Äñ‚ÇÅ
    """
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

def lasso_proximal_gradient(A, b, lambda_param, alpha=0.01, max_iter=1000):
    """
    ISTA pour Lasso
    """
    n = A.shape[1]
    x = np.zeros(n)
    
    for k in range(max_iter):
        # Gradient de f
        grad = A.T @ (A @ x - b)
        
        # √âtape de gradient
        z = x - alpha * grad
        
        # √âtape proximale
        x = soft_threshold(z, alpha * lambda_param)
    
    return x
```

---

## 3.6 Dualit√©

### 3.6.1 Conditions KKT G√©n√©ralis√©es

Pour le probl√®me :
```
minimize    f(x)
subject to  g(x) ‚â§ 0, h(x) = 0
```

**Lagrangien** :
```
‚Ñí(x, Œª, ŒΩ) = f(x) + Œª·µÄg(x) + ŒΩ·µÄh(x)
```

### 3.6.2 Probl√®me Dual

**Fonction duale** :
```
q(Œª, ŒΩ) = inf_x ‚Ñí(x, Œª, ŒΩ)
```

**Probl√®me dual** :
```
maximize    q(Œª, ŒΩ)
subject to  Œª ‚â• 0
```

**Dualit√© faible** : q(Œª, ŒΩ) ‚â§ f(x*) pour tout Œª ‚â• 0

**Dualit√© forte** : Sous certaines conditions (Slater), q(Œª*, ŒΩ*) = f(x*)

---

## üí° Points Cl√©s

1. **Convexit√©** : Simplifie grandement l'optimisation
2. **Gradient** : Direction de descente la plus rapide
3. **SGD** : Essentiel pour les grands datasets
4. **ADAM** : Optimiseur adaptatif tr√®s efficace
5. **Proximal** : Pour les fonctions non diff√©rentiables
6. **Dualit√©** : Fournit des bornes et des algorithmes alternatifs

---

[‚¨ÖÔ∏è Chapitre pr√©c√©dent](./chapitre-02-analyse-matricielle.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](../partie-2-concepts/chapitre-04-biais-variance.md)

