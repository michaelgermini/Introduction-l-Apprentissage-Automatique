# Chapitre 8 : Mod√®les de Classification Lin√©aire

## üìö Introduction

Ce chapitre couvre les principales m√©thodes de classification lin√©aire : r√©gression logistique, analyse discriminante lin√©aire, et machines √† vecteurs de support.

## üó∫Ô∏è Carte Mentale : Classification Lin√©aire

```
                CLASSIFICATION LIN√âAIRE
                         ‚îÇ
        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
        ‚îÇ                ‚îÇ                ‚îÇ
   DISCRIMINATIVE    G√âN√âRATIVE       MARGIN-BASED
        ‚îÇ                ‚îÇ                ‚îÇ
    ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê        ‚îå‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îê          SVM
    ‚îÇ       ‚îÇ        ‚îÇ       ‚îÇ           ‚îÇ
 Logistic  Perceptron LDA    QDA    Max Margin
 Regression    ‚îÇ        ‚îÇ     ‚îÇ          ‚îÇ
    ‚îÇ      One-layer Fisher's Bayes  Hard/Soft
  Softmax              ‚îÇ    Optimal   Margin
                   Dimension         Kernel
                   Reduction         Trick
```

## üìä Tableau Comparatif des M√©thodes

| **M√©thode** | **Approche** | **Hypoth√®se** | **Fronti√®re** | **Probabilit√©s** | **Avantages** | **Inconv√©nients** |
|------------|-------------|--------------|--------------|-----------------|--------------|------------------|
| **Logistic Regression** | Discriminative | Aucune | Lin√©aire | ‚úì Oui | Simple, interpr√©table | Lin√©aire seulement |
| **LDA** | G√©n√©rative | Gaussien, Œ£ commune | Lin√©aire | ‚úì Oui | Efficace, r√©duction dim. | Hypoth√®se forte |
| **QDA** | G√©n√©rative | Gaussien, Œ£‚Çñ diff√©rentes | Quadratique | ‚úì Oui | Plus flexible | Plus de param√®tres |
| **SVM** | Margin-based | Aucune | Lin√©aire/Non-lin√©aire | ‚úó Non | Robuste, kernel trick | Pas de probabilit√©s |
| **Perceptron** | Discriminative | S√©parable | Lin√©aire | ‚úó Non | Simple, en ligne | Pas de convergence si non s√©parable |

## üìê Visualisation des Fronti√®res de D√©cision

### Classification Binaire : Espace 2D

```
    x‚ÇÇ
     ‚îÇ                    R√âGRESSION LOGISTIQUE
     ‚îÇ    Classe 1        ‚ï± Fronti√®re lin√©aire
     ‚îÇ  ‚óè  ‚óè  ‚óè  ‚óè      ‚ï±  P(Y=1|x) = 0.5
     ‚îÇ   ‚óè  ‚óè  ‚óè       ‚ï±
     ‚îÇ  ‚óè  ‚óè  ‚óè  ‚óè    ‚ï±
     ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ï±  ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚îÇ          ‚óã  ‚ï±‚óã  ‚óã
     ‚îÇ        ‚óã  ‚ï±  ‚óã  ‚óã  ‚óã
     ‚îÇ      ‚óã  ‚ï±  ‚óã  ‚óã  ‚óã
     ‚îÇ    ‚óã  ‚ï±  ‚óã  ‚óã        Classe 0
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí x‚ÇÅ

    x‚ÇÇ
     ‚îÇ                         LDA
     ‚îÇ    Classe 1        
     ‚îÇ  ‚óè  ‚óè  ‚óè  ‚óè      Œº‚ÇÅ‚óè  Centro√Øde
     ‚îÇ   ‚óè  ‚óè  ‚óè          ‚îÇ
     ‚îÇ  ‚óè  ‚óè  ‚óè  ‚óè        ‚îÇ
     ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚îÇ        ‚óã  ‚óã  ‚óã    ‚îÇ
     ‚îÇ      ‚óã  ‚óã  ‚óã  ‚óã   ‚îÇ
     ‚îÇ    ‚óã  ‚óã  ‚óã  ‚óã     ‚îÇ
     ‚îÇ  ‚óã  ‚óã  ‚óã      Œº‚ÇÄ‚óã  Centro√Øde
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí x‚ÇÅ
     
    Fronti√®re = {x : w·µÄx + b = 0}
    w ‚àù Œ£‚Åª¬π(Œº‚ÇÅ - Œº‚ÇÄ)

    x‚ÇÇ
     ‚îÇ                        SVM
     ‚îÇ    Classe 1        
     ‚îÇ  ‚óè  ‚óè  ‚óè  ‚óè      ‚ú± Support vector
     ‚îÇ   ‚óè  ‚ú±  ‚óè          
     ‚îÇ  ‚óè  ‚óè  ‚óè  ‚óè    ‚ï±‚îÄ‚îÄ‚îÄ‚ï≤  Marges
     ‚îÇ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ  ‚ï±  ‚îÇ  ‚ï≤ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
     ‚îÇ        ‚ú±  ‚óã ‚ï±   ‚îÇ   ‚ï≤‚óã  ‚óã
     ‚îÇ      ‚óã  ‚óã  ‚ï±    ‚îÇ    ‚ï≤  ‚óã  ‚óã
     ‚îÇ    ‚óã  ‚óã  ‚ï±     ‚îÇ     ‚ï≤  ‚óã
     ‚îÇ  ‚óã  ‚óã       ‚ú±           ‚óã
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí x‚ÇÅ
     
    Maximise la marge : 2/‚Äñw‚Äñ
```

## üéØ Comparaison G√©om√©trique

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LOGISTIC REGRESSION                                     ‚îÇ
‚îÇ  ‚Ä¢ Minimise la log-loss (cross-entropy)                 ‚îÇ
‚îÇ  ‚Ä¢ Fronti√®re probabiliste douce                         ‚îÇ
‚îÇ  ‚Ä¢ Tous les points contribuent                          ‚îÇ
‚îÇ    Loss = -Œ£·µ¢ [y·µ¢log(p) + (1-y·µ¢)log(1-p)]              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  LDA (Linear Discriminant Analysis)                      ‚îÇ
‚îÇ  ‚Ä¢ Suppose distributions gaussiennes                     ‚îÇ
‚îÇ  ‚Ä¢ Maximise s√©paration inter-classes / intra-classe     ‚îÇ
‚îÇ  ‚Ä¢ Fronti√®re = √©quiprobabilit√© bay√©sienne               ‚îÇ
‚îÇ    Fronti√®re : P(Y=1|x) = P(Y=0|x)                      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò

‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  SVM (Support Vector Machine)                            ‚îÇ
‚îÇ  ‚Ä¢ Maximise la marge (distance minimale)                ‚îÇ
‚îÇ  ‚Ä¢ Seuls les support vectors comptent                    ‚îÇ
‚îÇ  ‚Ä¢ Robuste aux outliers                                  ‚îÇ
‚îÇ    Marge = 2/‚Äñw‚Äñ,  min ‚Äñw‚Äñ¬≤  s.t.  y·µ¢(w·µÄx·µ¢+b) ‚â• 1      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

## üìà Fonctions de D√©cision

```
    f(x) = w·µÄx + b
     ‚îÇ
     ‚îÇ     LOGISTIC              LDA               SVM
     ‚îÇ      
  1  ‚îÇ      ‚ï±‚îÄ‚îÄ‚îÄ‚îÄ              ‚ï±‚îÇ‚ï≤              ‚ï±‚îÇ‚ï≤
     ‚îÇ     ‚ï±                  ‚ï± ‚îÇ ‚ï≤            ‚ï± ‚îÇ ‚ï≤
  0.5‚îÇ‚îÄ‚îÄ‚îÄ‚îÄ‚óè‚îÄ‚îÄ‚îÄ‚îÄ              ‚îÄ‚îÄ‚óè‚îÄ‚îÄ            ‚îÄ‚îÄ‚óè‚îÄ‚îÄ
     ‚îÇ   ‚ï±   œÉ(f)             Bayes         Hard margin
  0  ‚îÇ  ‚ï±                       ‚îÇ                ‚îÇ
     ‚îÇ                          ‚îÇ                ‚îÇ
     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚Üí x
             ‚îÇ                  ‚îÇ                ‚îÇ
         Seuil 0            Œº‚ÇÄ = Œº‚ÇÅ         Marge max

P(Y=1|x) = œÉ(f(x))      P(Y=k|x) ‚àù œÄ‚ÇñœÜ‚Çñ(x)    ≈∑ = sign(f(x))
```

---

## 8.1 R√©gression Logistique

### 8.1.1 Cadre G√©n√©ral

**Mod√®le** : Pour classification binaire (Y ‚àà {0, 1}) :
```
P(Y = 1|X = x) = œÉ(w·µÄx + b)
```

o√π œÉ(z) = 1/(1 + e‚Åª·∂ª) est la fonction sigmo√Øde.

### 8.1.2 Log-Vraisemblance Conditionnelle

**Fonction objectif** :
```
‚Ñì(w, b) = Œ£·µ¢ [y·µ¢ log p(x·µ¢) + (1-y·µ¢) log(1-p(x·µ¢))]
```

o√π p(x) = œÉ(w·µÄx + b)

### 8.1.3 Algorithme d'Entra√Ænement

**Gradient** :
```
‚àá‚Ñì = Œ£·µ¢ (y·µ¢ - p(x·µ¢)) x·µ¢
```

```python
from sklearn.linear_model import LogisticRegression

# Classification binaire
lr = LogisticRegression()
lr.fit(X_train, y_train)

# Pr√©diction
y_pred = lr.predict(X_test)
y_proba = lr.predict_proba(X_test)
```

### 8.1.4 R√©gression Logistique P√©nalis√©e

```
minimize  -‚Ñì(w, b) + Œª‚Äñw‚Äñ¬≤   (Ridge)
minimize  -‚Ñì(w, b) + Œª‚Äñw‚Äñ‚ÇÅ   (Lasso)
```

```python
# L2 penalty
lr_ridge = LogisticRegression(penalty='l2', C=1.0)

# L1 penalty (s√©lection de variables)
lr_lasso = LogisticRegression(penalty='l1', solver='liblinear', C=0.1)
```

### 8.1.5 R√©gression Logistique √† Noyaux

```
f(x) = Œ£·µ¢ Œ±·µ¢ K(x, x·µ¢)
P(Y = 1|X = x) = œÉ(f(x))
```

---

## 8.2 Analyse Discriminante Lin√©aire (LDA)

### 8.2.1 Mod√®le G√©n√©ratif

**Hypoth√®ses** :
- X|Y = k ~ N(Œº‚Çñ, Œ£)  (m√™me covariance)
- P(Y = k) = œÄ‚Çñ

**R√®gle de classification** :
```
≈∑ = argmax_k [log œÄ‚Çñ - ¬Ω(x - Œº‚Çñ)·µÄŒ£‚Åª¬π(x - Œº‚Çñ)]
```

**Fronti√®re de d√©cision** : Lin√©aire !

```python
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis

lda = LinearDiscriminantAnalysis()
lda.fit(X_train, y_train)
y_pred = lda.predict(X_test)
```

### 8.2.2 R√©duction de Dimension

LDA projette sur l'espace de dimension K-1 (K classes) qui maximise :
```
J(W) = |W·µÄS_B W| / |W·µÄS_W W|
```

o√π :
- S_B : matrice de covariance between-class
- S_W : matrice de covariance within-class

```python
# LDA pour r√©duction de dimension
lda = LinearDiscriminantAnalysis(n_components=2)
X_reduced = lda.fit_transform(X_train, y_train)
```

### 8.2.3 LDA de Fisher

Maximiser :
```
J(w) = (w·µÄ(Œº‚ÇÅ - Œº‚ÇÇ))¬≤ / (w·µÄ(Œ£‚ÇÅ + Œ£‚ÇÇ)w)
```

**Solution** :
```
w = (Œ£‚ÇÅ + Œ£‚ÇÇ)‚Åª¬π(Œº‚ÇÅ - Œº‚ÇÇ)
```

---

## 8.3 Notation Optimale

Trouver une transformation Y ‚Üí Z qui rend la classification plus facile.

**Objectif** : Maximiser la corr√©lation entre Z et les indicatrices de classes.

---

## 8.4 SVM (Support Vector Machines)

### 8.4.1 Perceptron et Marge

**Perceptron** : Trouver w tel que y·µ¢(w·µÄx·µ¢) > 0

**Marge** : Distance du point le plus proche √† l'hyperplan :
```
m = min_i |w·µÄx·µ¢| / ‚Äñw‚Äñ
```

### 8.4.2 Maximisation de la Marge

**Probl√®me primal** :
```
minimize    ¬Ω‚Äñw‚Äñ¬≤
subject to  y·µ¢(w·µÄx·µ¢ + b) ‚â• 1  pour tout i
```

**Version soft-margin** :
```
minimize    ¬Ω‚Äñw‚Äñ¬≤ + C Œ£·µ¢ Œæ·µ¢
subject to  y·µ¢(w·µÄx·µ¢ + b) ‚â• 1 - Œæ·µ¢
            Œæ·µ¢ ‚â• 0
```

### 8.4.3 Probl√®me Dual et Conditions KKT

**Dual** :
```
maximize    Œ£·µ¢ Œ±·µ¢ - ¬ΩŒ£·µ¢‚±º Œ±·µ¢Œ±‚±ºy·µ¢y‚±º(x·µ¢·µÄx‚±º)
subject to  0 ‚â§ Œ±·µ¢ ‚â§ C
            Œ£·µ¢ Œ±·µ¢y·µ¢ = 0
```

**Solution** :
```
w = Œ£·µ¢ Œ±·µ¢y·µ¢x·µ¢
```

**Vecteurs de support** : Points avec Œ±·µ¢ > 0

### 8.4.4 Version √† Noyaux

Remplacer x·µ¢·µÄx‚±º par K(x·µ¢, x‚±º) :

```
maximize    Œ£·µ¢ Œ±·µ¢ - ¬ΩŒ£·µ¢‚±º Œ±·µ¢Œ±‚±ºy·µ¢y‚±ºK(x·µ¢, x‚±º)
```

```python
from sklearn.svm import SVC

# SVM lin√©aire
svm_linear = SVC(kernel='linear', C=1.0)
svm_linear.fit(X_train, y_train)

# SVM avec noyau RBF
svm_rbf = SVC(kernel='rbf', gamma=0.1, C=1.0)
svm_rbf.fit(X_train, y_train)

# Vecteurs de support
n_support = svm_rbf.n_support_
print(f"Nombre de vecteurs de support: {n_support}")
```

---

## üí° Comparaison des M√©thodes

| M√©thode | Type | Fronti√®re | Probabilit√©s |
|---------|------|-----------|--------------|
| Logistic | Discriminatif | Lin√©aire | Oui |
| LDA | G√©n√©ratif | Lin√©aire | Oui |
| SVM | Discriminatif | Non lin√©aire (kernel) | Non (direct) |

---

[‚¨ÖÔ∏è Chapitre pr√©c√©dent](./chapitre-07-regression-lineaire.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](./chapitre-09-plus-proches-voisins.md)

