# Chapitre 11 : R√©seaux de Neurones

## üìö Introduction

Les r√©seaux de neurones sont la base du deep learning moderne. Ce chapitre couvre les fondements architecturaux et algorithmiques.

---

## 11.1 D√©finitions de Base

### Neurone Artificiel

Un **neurone** calcule :
```
z = w‚ÇÅx‚ÇÅ + w‚ÇÇx‚ÇÇ + ... + w‚Çöx‚Çö + b
a = œÉ(z)
```

o√π œÉ est une **fonction d'activation**.

### Fonctions d'Activation

**Sigmo√Øde** :
```
œÉ(z) = 1 / (1 + e‚Åª·∂ª)
```

**Tanh** :
```
tanh(z) = (e^z - e‚Åª·∂ª) / (e^z + e‚Åª·∂ª)
```

**ReLU** (Rectified Linear Unit) :
```
ReLU(z) = max(0, z)
```

**Leaky ReLU** :
```
LeakyReLU(z) = max(Œ±¬∑z, z)  o√π Œ± ‚âà 0.01
```

---

## 11.2 Architecture des R√©seaux

### 11.2.1 R√©seau Multicouche

**Couche cach√©e l** :
```
z‚ÅΩÀ°‚Åæ = W‚ÅΩÀ°‚Åæa‚ÅΩÀ°‚Åª¬π‚Åæ + b‚ÅΩÀ°‚Åæ
a‚ÅΩÀ°‚Åæ = œÉ(z‚ÅΩÀ°‚Åæ)
```

### 11.2.2 Couche de Sortie

**R√©gression** : Sortie lin√©aire
**Classification binaire** : Sigmo√Øde
**Classification multi-classe** : Softmax
```
P(Y = k|x) = exp(z_k) / Œ£‚±º exp(z_j)
```

### 11.2.3 R√©seaux Convolutifs (CNN)

**Convolution** :
```
(f * g)[i] = Œ£‚±º f[j] ¬∑ g[i - j]
```

**Pooling** : Max pooling, average pooling

```python
import torch.nn as nn

class SimpleCNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(32 * 14 * 14, 10)
    
    def forward(self, x):
        x = self.pool(nn.functional.relu(self.conv1(x)))
        x = x.view(-1, 32 * 14 * 14)
        x = self.fc(x)
        return x
```

---

## 11.3 Fonction Objectif

### Perte pour la R√©gression

**MSE** :
```
L = (1/n) Œ£·µ¢ (y·µ¢ - ≈∑·µ¢)¬≤
```

### Perte pour la Classification

**Cross-entropie** :
```
L = -(1/n) Œ£·µ¢ Œ£‚Çñ y·µ¢‚Çñ log(≈∑·µ¢‚Çñ)
```

**Avec PyTorch** :
```python
import torch
import torch.nn as nn

criterion = nn.CrossEntropyLoss()
loss = criterion(outputs, targets)
```

---

## 11.4 R√©tropropagation

### Algorithme

**Forward pass** : Calculer les activations
**Backward pass** : Calculer les gradients

**R√®gle de la cha√Æne** :
```
‚àÇL/‚àÇW‚ÅΩÀ°‚Åæ = (‚àÇL/‚àÇz‚ÅΩÀ°‚Åæ) ¬∑ (‚àÇz‚ÅΩÀ°‚Åæ/‚àÇW‚ÅΩÀ°‚Åæ)
```

**Impl√©mentation** :
```python
import torch

# R√©seau simple
class SimpleNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super().__init__()
        self.fc1 = nn.Linear(input_dim, hidden_dim)
        self.fc2 = nn.Linear(hidden_dim, output_dim)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Entra√Ænement
model = SimpleNN(784, 128, 10)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)

for epoch in range(num_epochs):
    for batch_x, batch_y in dataloader:
        # Forward
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)
        
        # Backward
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

---

## 11.5 Techniques d'Entra√Ænement

### 11.5.1 Mini-Batches

**SGD par mini-batches** :
```python
batch_size = 32
dataloader = torch.utils.data.DataLoader(
    dataset, 
    batch_size=batch_size,
    shuffle=True
)
```

### 11.5.2 Dropout

**R√©gularisation** : D√©sactiver al√©atoirement des neurones pendant l'entra√Ænement.

```python
class NNWithDropout(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.dropout = nn.Dropout(0.5)
        self.fc2 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.dropout(x)  # D√©sactiv√© en mode eval()
        x = self.fc2(x)
        return x
```

### Batch Normalization

```python
class NNWithBatchNorm(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(784, 256)
        self.bn = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 10)
    
    def forward(self, x):
        x = self.fc1(x)
        x = self.bn(x)
        x = torch.relu(x)
        x = self.fc2(x)
        return x
```

---

## 11.6 Architectures Modernes

### ResNet (Residual Networks)

**Connexions r√©siduelles** :
```
F(x) = H(x) - x
H(x) = F(x) + x
```

### Transformers

**Attention** :
```
Attention(Q, K, V) = softmax(QK^T / ‚àöd_k) V
```

---

## üí° Best Practices

1. **Initialisation** : Xavier/He initialization
2. **Learning rate** : Learning rate schedule
3. **R√©gularisation** : Dropout, L2, Batch norm
4. **Architecture** : Commencer simple, complexifier progressivement
5. **Monitoring** : TensorBoard, visualisation des pertes

```python
# Exemple complet avec TensorBoard
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter('runs/experiment_1')

for epoch in range(num_epochs):
    train_loss = train_one_epoch(model, train_loader)
    val_loss = validate(model, val_loader)
    
    writer.add_scalar('Loss/train', train_loss, epoch)
    writer.add_scalar('Loss/val', val_loss, epoch)

writer.close()
```

---

[‚¨ÖÔ∏è Chapitre pr√©c√©dent](./chapitre-10-arbres.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](../partie-4-modeles-probabilistes/chapitre-12-comparaison-distributions.md)

