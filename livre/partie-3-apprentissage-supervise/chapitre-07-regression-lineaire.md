# Chapitre 7 : RÃ©gression LinÃ©aire

## ğŸ“š Introduction

La rÃ©gression linÃ©aire est l'un des algorithmes les plus fondamentaux en machine learning. Ce chapitre couvre les moindres carrÃ©s, la rÃ©gularisation (Ridge, Lasso) et les SVM pour la rÃ©gression.

## ğŸ—ºï¸ Carte Mentale : RÃ©gression LinÃ©aire

```
                    RÃ‰GRESSION LINÃ‰AIRE
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
    MOINDRES            RÃ‰GULARISÃ‰E          ROBUSTE
     CARRÃ‰S                 â”‚                   â”‚
    (OLS)           â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”          SVM
        â”‚           â”‚               â”‚        RÃ©gression
   Non biaisÃ©    RIDGE           LASSO          â”‚
   Variance ++     â”‚               â”‚        Îµ-insensitive
                Lâ‚‚ penalty     Lâ‚ penalty
                RÃ©trÃ©cit       SÃ©lectionne
                              (SparsitÃ©)
```

## ğŸ“Š Tableau Comparatif : OLS vs Ridge vs Lasso

| **MÃ©thode** | **Formulation** | **Solution** | **PropriÃ©tÃ©s** | **Usage** |
|------------|----------------|-------------|---------------|-----------|
| **OLS** | min â€–Y-XÎ²â€–Â² | Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€Y | Non biaisÃ©, variance Ã©levÃ©e | p << n, faible collinÃ©aritÃ© |
| **Ridge** | min â€–Y-XÎ²â€–Â² + Î»â€–Î²â€–Â² | Î²Ì‚ = (Xáµ€X+Î»I)â»Â¹Xáµ€Y | BiaisÃ©, variance rÃ©duite | p â‰ˆ n, forte collinÃ©aritÃ© |
| **Lasso** | min â€–Y-XÎ²â€–Â² + Î»â€–Î²â€–â‚ | Pas de formule fermÃ©e | SÃ©lection de variables | p >> n, features redondantes |
| **Elastic Net** | min â€–Y-XÎ²â€–Â² + Î»â‚â€–Î²â€–â‚ + Î»â‚‚â€–Î²â€–Â² | ItÃ©ratif | Combine Ridge + Lasso | p >> n, groupes de features |

## ğŸ“ Visualisation GÃ©omÃ©trique

### Contraintes de RÃ©gularisation

```
Espace des paramÃ¨tres (Î²â‚, Î²â‚‚) :

    Î²â‚‚                          RIDGE (Lâ‚‚)
     â”‚                             â•­â”€â”€â”€â•®
     â”‚    â•­â”€â”€â”€â”€â”€â”€â”€â•®              â•±       â•²
     â”‚  â•±           â•²           â”‚    â—Î²* â”‚  RÃ©gion convexe
     â”‚ â”‚   Ellipses  â”‚          â”‚ solutionâ”‚  Cercle
     â”‚  â•²  de RSS   â•±            â•²       â•±
     â”€â—â”€â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î²â‚       â•°â”€â”€â”€â•¯
      â”‚  Î²Ì‚ (OLS)
      â”‚
    
    Î²â‚‚                          LASSO (Lâ‚)
     â”‚                             â•±â•²
     â”‚    â•­â”€â”€â”€â”€â”€â”€â”€â•®              â•±  â•²
     â”‚  â•±           â•²           â—†    â”‚     RÃ©gion en losange
     â”‚ â”‚   Ellipses  â”‚          â”‚  â—Î²*â”‚   Coins â†’ sparsitÃ©
     â”‚  â•²  de RSS   â•±            â•²  â•± 
     â”€â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Î²â‚    â•²â•±
      â”‚
      â”‚  Solution souvent sur un coin â†’ Î²â‚ ou Î²â‚‚ = 0
```

### ğŸ¯ Effet de Î» sur les Coefficients

```
    Coefficient Î²
         â”‚
    Î²Ì‚OLS â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  OLS (Î»=0)
         â”‚â•²
         â”‚ â•²                      Ridge : RÃ©trÃ©cissement progressif
         â”‚  â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€           (jamais exactement 0)
         â”‚   â•²      â•²
         â”‚    â•²â”€â”€â”€â”€â”€â”€â•²â”€â”€â”€        Lasso : Atteint 0
    0    â”‚â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â”€â”€â—â”€â”€â”€â”€â†’ Î»   (sÃ©lection de variables)
         â”‚    Î»â‚     Î»â‚‚
         â”‚     
    Optimal via CV
```

---

## 7.1 RÃ©gression par Moindres CarrÃ©s

### 7.1.1 Notation et Estimateur de Base

**ModÃ¨le** :
```
Y = XÎ² + Îµ
```

oÃ¹ X âˆˆ â„â¿Ë£áµ–, Î² âˆˆ â„áµ–, Îµ ~ N(0, ÏƒÂ²I)

**Estimateur des moindres carrÃ©s** :
```
Î²Ì‚ = argmin_Î² â€–Y - XÎ²â€–Â²
```

**Solution** :
```
Î²Ì‚ = (Xáµ€X)â»Â¹Xáµ€Y
```

```python
import numpy as np

def least_squares(X, y):
    """RÃ©gression par moindres carrÃ©s"""
    return np.linalg.solve(X.T @ X, X.T @ y)

# Avec sklearn
from sklearn.linear_model import LinearRegression

model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_test)
```

### 7.1.2 Comportement Limite

**ThÃ©orÃ¨me** : Si Xáµ€X/n â†’ Î£ et Xáµ€Y/n â†’ Î³, alors :
```
Î²Ì‚ â†’ Î£â»Â¹Î³  quand n â†’ âˆ
```

### 7.1.3 ThÃ©orÃ¨me de Gauss-Markov

Parmi tous les estimateurs **linÃ©aires non biaisÃ©s**, Î²Ì‚ a la variance minimale.

### 7.1.4 Version Noyau

```
fÌ‚(x) = Î£áµ¢ Î±áµ¢ K(x, xáµ¢)
```

avec Î± = (K + Î»I)â»Â¹y

---

## 7.2 Ridge Regression et Lasso

### 7.2.1 Ridge Regression

**ProblÃ¨me** :
```
minimize  â€–Y - XÎ²â€–Â² + Î»â€–Î²â€–Â²
```

**Solution** :
```
Î²Ì‚_ridge = (Xáµ€X + Î»I)â»Â¹Xáµ€Y
```

**Avantages** :
- StabilitÃ© numÃ©rique
- RÃ©duit la variance
- Toujours une solution unique

```python
from sklearn.linear_model import Ridge

ridge = Ridge(alpha=1.0)
ridge.fit(X_train, y_train)
```

**Choix de Î»** : Validation croisÃ©e

```python
from sklearn.linear_model import RidgeCV

ridge_cv = RidgeCV(alphas=[0.1, 1.0, 10.0])
ridge_cv.fit(X_train, y_train)
print(f"Best alpha: {ridge_cv.alpha_}")
```

### 7.2.2 Ã‰quivalence Formulations Contrainte/PÃ©nalisÃ©e

Les deux problÃ¨mes sont Ã©quivalents :

**Forme contrainte** :
```
minimize  â€–Y - XÎ²â€–Â²
subject to  â€–Î²â€–Â² â‰¤ t
```

**Forme pÃ©nalisÃ©e** :
```
minimize  â€–Y - XÎ²â€–Â² + Î»â€–Î²â€–Â²
```

Pour chaque t, il existe Î» tel que les solutions coÃ¯ncident.

### 7.2.3 RÃ©gression Lasso

**ProblÃ¨me** :
```
minimize  â€–Y - XÎ²â€–Â² + Î»â€–Î²â€–â‚
```

**PropriÃ©tÃ© clÃ©** : SÃ©lection de variables (certains Î²áµ¢ = 0)

```python
from sklearn.linear_model import Lasso, LassoCV

# Lasso
lasso = Lasso(alpha=0.1)
lasso.fit(X_train, y_train)

# Nombre de coefficients non nuls
n_nonzero = np.sum(lasso.coef_ != 0)
print(f"Variables sÃ©lectionnÃ©es: {n_nonzero}/{len(lasso.coef_)}")

# Cross-validation
lasso_cv = LassoCV(cv=5)
lasso_cv.fit(X_train, y_train)
```

**Algorithme ISTA** (Iterative Soft Thresholding) :
```python
def soft_threshold(x, threshold):
    return np.sign(x) * np.maximum(np.abs(x) - threshold, 0)

def ista(X, y, lambda_param, max_iter=1000, tol=1e-4):
    n, p = X.shape
    beta = np.zeros(p)
    L = np.linalg.norm(X.T @ X, 2)  # Constante de Lipschitz
    alpha = 1 / L
    
    for k in range(max_iter):
        grad = X.T @ (X @ beta - y)
        beta_new = soft_threshold(beta - alpha * grad, alpha * lambda_param)
        
        if np.linalg.norm(beta_new - beta) < tol:
            break
        
        beta = beta_new
    
    return beta
```

---

## 7.3 Autres Estimateurs de Parcimonie

### 7.3.1 LARS (Least Angle Regression)

Algorithme qui trouve tout le chemin de rÃ©gularisation du Lasso efficacement.

```python
from sklearn.linear_model import Lars

lars = Lars(n_nonzero_coefs=10)
lars.fit(X_train, y_train)
```

### 7.3.2 SÃ©lecteur de Dantzig

```
minimize  â€–Î²â€–â‚
subject to  â€–Xáµ€(Y - XÎ²)â€–_âˆ â‰¤ Î»
```

---

## 7.4 SVM pour la RÃ©gression

### 7.4.1 SVM LinÃ©aire

**Fonction de perte Îµ-insensible** :
```
L_Îµ(y, f(x)) = max(0, |y - f(x)| - Îµ)
```

**ProblÃ¨me** :
```
minimize  (1/2)â€–wâ€–Â² + C Î£áµ¢ (Î¾áµ¢ + Î¾áµ¢*)
subject to  yáµ¢ - (wáµ€xáµ¢ + b) â‰¤ Îµ + Î¾áµ¢
           (wáµ€xáµ¢ + b) - yáµ¢ â‰¤ Îµ + Î¾áµ¢*
           Î¾áµ¢, Î¾áµ¢* â‰¥ 0
```

```python
from sklearn.svm import SVR

svr = SVR(kernel='linear', epsilon=0.1, C=1.0)
svr.fit(X_train, y_train)
```

### 7.4.2 Kernel Trick pour SVM

```python
svr_rbf = SVR(kernel='rbf', gamma=0.1, epsilon=0.1)
svr_rbf.fit(X_train, y_train)
```

---

## ğŸ’¡ Comparaison des MÃ©thodes

| MÃ©thode | RÃ©gularisation | SÃ©lection | ComplexitÃ© |
|---------|---------------|-----------|------------|
| OLS | Aucune | Non | O(pÂ³) |
| Ridge | L2 | Non | O(pÂ³) |
| Lasso | L1 | Oui | ItÃ©ratif |
| Elastic Net | L1 + L2 | Oui | ItÃ©ratif |

---

[â¬…ï¸ Partie 2](../partie-2-concepts/chapitre-06-noyaux.md) | [Retour](../README.md) | [Suite â¡ï¸](./chapitre-08-classification-lineaire.md)

