# Chapitre 23 : Bornes de GÃ©nÃ©ralisation

## ğŸ“š Introduction

Ce chapitre prÃ©sente les outils thÃ©oriques pour analyser la capacitÃ© de gÃ©nÃ©ralisation des algorithmes d'apprentissage.

---

## 23.1 Notation

**Risque rÃ©el** : R(f) = ğ”¼[L(Y, f(X))]
**Risque empirique** : RÌ‚(f) = (1/n) Î£áµ¢ L(yáµ¢, f(xáµ¢))

**Objectif** : Borner |R(fÌ‚) - RÌ‚(fÌ‚)|

---

## 23.2 CritÃ¨res d'Information

### 23.2.1 AIC (Akaike Information Criterion)

```
AIC = -2 log L + 2k
```

oÃ¹ k est le nombre de paramÃ¨tres.

### 23.2.2 BIC (Bayesian Information Criterion)

```
BIC = -2 log L + k log n
```

**PropriÃ©tÃ©** : BIC pÃ©nalise plus la complexitÃ© pour n grand.

---

## 23.3 InÃ©galitÃ©s de Concentration

### 23.3.1 InÃ©galitÃ© de Hoeffding

Pour Xâ‚, ..., Xâ‚™ i.i.d. bornÃ©es dans [a, b] :
```
P(|XÌ„ - ğ”¼[X]| â‰¥ Îµ) â‰¤ 2 exp(-2nÎµÂ²/(b-a)Â²)
```

### 23.3.2 Variables Sous-Gaussiennes

X est **sous-gaussienne** si :
```
ğ”¼[exp(Î»X)] â‰¤ exp(Î»Â²ÏƒÂ²/2)
```

**PropriÃ©tÃ©** : Les variables bornÃ©es sont sous-gaussiennes.

---

## 23.4 Dimension VC

### DÃ©finition

La **dimension VC** d'une classe â„± est le plus grand n tel que â„± peut rÃ©aliser toutes les 2â¿ dichotomies sur n points.

**Exemples** :
- Demi-espaces en â„áµˆ : VC = d + 1
- Perceptron : VC = d + 1

### ThÃ©orÃ¨me de Vapnik

Avec probabilitÃ© 1 - Î´ :
```
R(fÌ‚) â‰¤ RÌ‚(fÌ‚) + O(âˆš(VC log(n/VC) / n))
```

---

## 23.5 Nombres de Couverture

### DÃ©finition

Le **nombre de couverture** N(Îµ, â„±, d) est le nombre minimal de boules de rayon Îµ pour couvrir â„±.

**Entropie mÃ©trique** : H(Îµ) = log N(Îµ)

### InÃ©galitÃ©

Avec probabilitÃ© 1 - Î´ :
```
|R(f) - RÌ‚(f)| â‰¤ O(âˆš(H(Îµ/n) / n))
```

---

## 23.6 ComplexitÃ© de Rademacher

### DÃ©finition

```
R_n(â„±) = ğ”¼_Ïƒ[sup_{fâˆˆâ„±} (1/n) Î£áµ¢ Ïƒáµ¢ f(xáµ¢)]
```

oÃ¹ Ïƒáµ¢ âˆˆ {-1, +1} alÃ©atoires.

### Borne de GÃ©nÃ©ralisation

Avec probabilitÃ© 1 - Î´ :
```
R(f) â‰¤ RÌ‚(f) + 2R_n(â„±) + O(âˆš(log(1/Î´) / n))
```

---

## 23.7 Application Ã  la SÃ©lection de ModÃ¨les

### Principe du Risque Structurel

Pour â„±â‚ âŠ‚ â„±â‚‚ âŠ‚ ... :
```
fÌ‚ = argmin_{fâˆˆâ„±_k} [RÌ‚(f) + Pen(k)]
```

**PÃ©nalitÃ©** : Pen(k) âˆ Complexity(â„±_k)

---

## ğŸ’¡ Points ClÃ©s

1. **Trade-off biais-variance** : FormalisÃ© par les bornes
2. **ComplexitÃ©** : VC-dimension, nombres de couverture, Rademacher
3. **Concentration** : Hoeffding, sous-gaussiennes
4. **SÃ©lection de modÃ¨les** : AIC, BIC, validation croisÃ©e

---

## ğŸ“ RÃ©sumÃ© du Livre

Ce livre a couvert :

### Partie I : Fondements
- AlgÃ¨bre linÃ©aire et optimisation
- Outils mathÃ©matiques essentiels

### Partie II : Concepts
- PrÃ©diction et Ã©valuation
- Noyaux et espaces de Hilbert

### Partie III : Apprentissage SupervisÃ©
- RÃ©gression et classification
- Arbres, forests, boosting
- RÃ©seaux de neurones

### Partie IV : ModÃ¨les Probabilistes
- Graphes et infÃ©rence
- MCMC et variational inference

### Partie V : MÃ©thodes GÃ©nÃ©ratives
- VAE, GAN, Flows

### Partie VI : Non SupervisÃ©
- Clustering et rÃ©duction de dimension
- Visualisation

### Partie VII : ThÃ©orie
- Bornes de gÃ©nÃ©ralisation
- Analyse thÃ©orique

---

## ğŸ“ Conclusion

Le machine learning est un domaine riche combinant thÃ©orie et pratique. Ce livre a fourni les fondements mathÃ©matiques et les algorithmes pour :

- Comprendre les principes sous-jacents
- ImplÃ©menter des algorithmes
- Analyser leur comportement
- DÃ©velopper de nouvelles mÃ©thodes

**Bonne continuation dans votre apprentissage du ML ! ğŸš€**

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](../partie-6-non-supervise/chapitre-22-visualisation.md) | [ğŸ  Retour Ã  l'accueil](../README.md)

