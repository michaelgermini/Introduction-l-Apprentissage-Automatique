# Chapitre 4 : Biais et Variance

## ğŸ“š Introduction

Le compromis biais-variance est un concept fondamental en machine learning qui explique pourquoi les modÃ¨les peuvent sous-apprendre (underfitting) ou sur-apprendre (overfitting). Ce chapitre illustre ce dilemme Ã  travers l'estimation de paramÃ¨tres et l'estimation de densitÃ©.

## ğŸ—ºï¸ Carte Mentale : Compromis Biais-Variance

```
                 COMPROMIS BIAIS-VARIANCE
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
    UNDERFITTING    OPTIMAL FIT      OVERFITTING
        â”‚                 â”‚                 â”‚
   Biais Ã©levÃ©      Ã‰quilibrÃ©        Variance Ã©levÃ©e
   Variance faible   Biais/Var       Biais faible
        â”‚                 â”‚                 â”‚
   ModÃ¨le trop      GÃ©nÃ©ralise       MÃ©morise
   simple           bien             donnÃ©es
        â”‚                                   â”‚
   â†“ CapacitÃ©                          â†‘ CapacitÃ©
```

## ğŸ“ Visualisation du Compromis

### DÃ©composition de l'Erreur

```
    Erreur
      â”‚
      â”‚     Erreur totale = BiaisÂ² + Variance + Bruit
      â”‚
      â”‚         â•±â•²
  1.0 â”‚        â•±  â•²
      â”‚       â•±    â•²              Variance
      â”‚      â•±      â•²â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
      â”‚     â•±        â•²â•²          â•±
      â”‚    â•±          â•²â•²        â•±
  0.5 â”‚   â•±            â•²â•²      â•±
      â”‚  â•±   BiaisÂ²     â•²â•²    â•±
      â”‚ â•±                â•²â•²  â•±
      â”‚â•±__________________â•²â•²â•±_____ Bruit irrÃ©ductible
  0   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ComplexitÃ©
      Simple          Optimal     Complexe
    (underfit)                   (overfit)
```

## ğŸ“Š Tableau Comparatif : Underfitting vs Overfitting

| **Aspect** | **Underfitting** | **Sweet Spot** | **Overfitting** |
|-----------|-----------------|---------------|----------------|
| **ComplexitÃ©** | Trop simple | AppropriÃ©e | Trop complexe |
| **Biais** | â¬†ï¸ Ã‰levÃ© | âœ“ Faible | âœ“ TrÃ¨s faible |
| **Variance** | âœ“ Faible | âœ“ Faible | â¬†ï¸ Ã‰levÃ©e |
| **Erreur Train** | Ã‰levÃ©e | Faible | TrÃ¨s faible |
| **Erreur Test** | Ã‰levÃ©e | Faible | Ã‰levÃ©e |
| **GÃ©nÃ©ralisation** | âœ— Mauvaise | âœ“âœ“ Excellente | âœ— Mauvaise |
| **Exemple** | Ligne droite | PolynÃ´me deg 3 | PolynÃ´me deg 15 |

## ğŸ“ˆ Courbes d'Apprentissage Typiques

```
    Erreur
      â”‚
      â”‚  UNDERFITTING        OPTIMAL          OVERFITTING
      â”‚  
      â”‚  Train â”€â”€â”€â”€â”€         Train â”€â”€â”€        Train â”€â”€â”€â”€â•²
      â”‚  Test  â”€â”€â”€â”€â”€         Test  â”€â”€â”€        Test  â”€â”€â”€â”€/â•²
      â”‚                                                    â•²
      â”‚  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€        â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€   â”€â”€â”€
      â”‚   Gap faible         Gap faible       Gap LARGE
      â”‚   Err. Ã©levÃ©e        Err. faible      Train â†’ 0
      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ Epochs

Diagnostic :
  â€¢ Gap faible + Err. Ã©levÃ©e â†’ Augmenter capacitÃ©
  â€¢ Gap large               â†’ RÃ©gulariser / Plus de donnÃ©es
  â€¢ Convergence             â†’ Sweet spot !
```

## ğŸ¯ Diagramme de DÃ©cision : Que Faire ?

```
         Analyser erreur Train vs Test
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                       â”‚
   Err Train       Err Train    Err Test
   Ã©levÃ©e?         faible?      Ã©levÃ©e?
        â”‚                       â”‚
      OUI                     OUI
        â”‚                       â”‚
    UNDERFITTING            OVERFITTING
        â”‚                       â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”               â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚               â”‚       â”‚
  â€¢ ModÃ¨le  â€¢ Features    â€¢ RÃ©gula- â€¢ Plus de
    plus      meilleures    risation  donnÃ©es
    complexe                â”‚         â”‚
  â€¢ Plus    â€¢ Feature     â€¢ Dropout â€¢ Data aug-
    layers    engineering   â”‚         mentation
                          â€¢ Early   â€¢ Simplifier
                            stopping  modÃ¨le
```

---

## 4.1 Estimation de ParamÃ¨tres et Sieves

### Le Dilemme Biais-Variance

Pour un estimateur Î¸Ì‚ d'un paramÃ¨tre Î¸ vrai :

**Erreur quadratique moyenne (MSE)** :
```
MSE(Î¸Ì‚) = ğ”¼[(Î¸Ì‚ - Î¸)Â²] = BiasÂ²(Î¸Ì‚) + Var(Î¸Ì‚)
```

oÃ¹ :
- **Biais** : Bias(Î¸Ì‚) = ğ”¼[Î¸Ì‚] - Î¸
- **Variance** : Var(Î¸Ì‚) = ğ”¼[(Î¸Ì‚ - ğ”¼[Î¸Ì‚])Â²]

### Illustration

```python
import numpy as np
import matplotlib.pyplot as plt

def generate_data(n, noise=0.1):
    """GÃ©nÃ¨re des donnÃ©es d'une fonction polynomiale"""
    x = np.linspace(0, 1, n)
    y = np.sin(2 * np.pi * x) + np.random.normal(0, noise, n)
    return x, y

# DiffÃ©rents modÃ¨les
def fit_polynomial(x, y, degree):
    """Ajuste un polynÃ´me de degrÃ© donnÃ©"""
    return np.polyfit(x, y, degree)

# Visualisation
x_train, y_train = generate_data(20)
x_test = np.linspace(0, 1, 100)

plt.figure(figsize=(15, 5))
for i, degree in enumerate([1, 5, 15], 1):
    plt.subplot(1, 3, i)
    coef = fit_polynomial(x_train, y_train, degree)
    y_pred = np.polyval(coef, x_test)
    
    plt.scatter(x_train, y_train, label='DonnÃ©es')
    plt.plot(x_test, y_pred, 'r-', label=f'DegrÃ© {degree}')
    plt.title(f'PolynÃ´me degrÃ© {degree}')
    plt.legend()
```

**InterprÃ©tation** :
- **DegrÃ© faible** : Biais Ã©levÃ©, variance faible (underfitting)
- **DegrÃ© optimal** : Compromis biais-variance
- **DegrÃ© Ã©levÃ©** : Biais faible, variance Ã©levÃ©e (overfitting)

### MÃ©thode des Sieves

**Principe** : Augmenter la complexitÃ© du modÃ¨le avec la taille des donnÃ©es.

Pour n observations, utiliser un espace de dimension d_n tel que :
```
d_n â†’ âˆ  mais  d_n/n â†’ 0  quand n â†’ âˆ
```

---

## 4.2 Estimation de DensitÃ© par Noyaux

### Estimateur de Parzen

Pour estimer la densitÃ© p(x) Ã  partir d'Ã©chantillons xâ‚, ..., xâ‚™ :

```
pÌ‚_h(x) = (1/nh) Î£áµ¢ K((x - xáµ¢)/h)
```

oÃ¹ :
- **K** : fonction noyau (K(u) â‰¥ 0, âˆ« K(u)du = 1)
- **h** : largeur de bande (bandwidth)

### Noyaux Communs

```python
def gaussian_kernel(u):
    """Noyau gaussien"""
    return (1/np.sqrt(2*np.pi)) * np.exp(-u**2/2)

def epanechnikov_kernel(u):
    """Noyau d'Epanechnikov"""
    return 0.75 * (1 - u**2) * (np.abs(u) <= 1)

def kernel_density_estimate(data, x, h, kernel=gaussian_kernel):
    """
    Estimation de densitÃ© par noyaux
    """
    n = len(data)
    density = np.zeros_like(x)
    
    for i, xi in enumerate(x):
        u = (xi - data) / h
        density[i] = np.mean(kernel(u)) / h
    
    return density

# Exemple
np.random.seed(42)
data = np.concatenate([
    np.random.normal(0, 1, 100),
    np.random.normal(5, 1.5, 100)
])

x = np.linspace(-5, 10, 1000)

plt.figure(figsize=(15, 5))
for i, h in enumerate([0.1, 0.5, 2.0], 1):
    plt.subplot(1, 3, i)
    density = kernel_density_estimate(data, x, h)
    plt.plot(x, density, label=f'h = {h}')
    plt.hist(data, bins=30, density=True, alpha=0.3)
    plt.title(f'Bandwidth h = {h}')
    plt.legend()
```

### Analyse Biais-Variance

**Biais** :
```
Bias(pÌ‚_h(x)) = O(hÂ²)  si p est deux fois dÃ©rivable
```

**Variance** :
```
Var(pÌ‚_h(x)) = O(1/(nh))
```

**MSE** :
```
MSE(pÌ‚_h(x)) = O(hâ´) + O(1/(nh))
```

**Bandwidth optimal** :
```
h_opt âˆ n^{-1/5}
```

donnant MSE = O(n^{-4/5})

### Choix de la Largeur de Bande

#### RÃ¨gle de Silverman

Pour des donnÃ©es approximativement normales :
```
h = 1.06 Â· ÏƒÌ‚ Â· n^{-1/5}
```

oÃ¹ ÏƒÌ‚ est l'Ã©cart-type empirique.

#### Validation CroisÃ©e

Minimiser :
```
CV(h) = âˆ« pÌ‚_hÂ²(x)dx - (2/n) Î£áµ¢ pÌ‚_h^{(-i)}(xáµ¢)
```

oÃ¹ pÌ‚_h^{(-i)} est l'estimateur sans la i-Ã¨me observation.

```python
def cross_validation_bandwidth(data, h_values):
    """
    SÃ©lection de h par validation croisÃ©e
    """
    n = len(data)
    cv_scores = []
    
    for h in h_values:
        score = 0
        for i in range(n):
            # Leave-one-out
            data_loo = np.delete(data, i)
            dens = kernel_density_estimate(data_loo, [data[i]], h)
            score += np.log(dens[0] + 1e-10)
        
        cv_scores.append(-score / n)
    
    best_h = h_values[np.argmin(cv_scores)]
    return best_h, cv_scores
```

---

## ğŸ’¡ Points ClÃ©s

1. **Compromis Biais-Variance** : MSE = BiasÂ² + Variance
2. **Underfitting** : ModÃ¨le trop simple (biais Ã©levÃ©)
3. **Overfitting** : ModÃ¨le trop complexe (variance Ã©levÃ©e)
4. **RÃ©gularisation** : ContrÃ´le le compromis
5. **Estimation de densitÃ©** : Le paramÃ¨tre h contrÃ´le le lissage

---

## ğŸ“ Exercices

### Exercice 1
GÃ©nÃ©rez des donnÃ©es et comparez les MSE de polynÃ´mes de diffÃ©rents degrÃ©s.

### Exercice 2
ImplÃ©mentez l'estimation de densitÃ© avec diffÃ©rents noyaux et comparez les rÃ©sultats.

### Exercice 3
Analysez thÃ©oriquement le biais et la variance de la moyenne empirique.

---

[â¬…ï¸ Partie 1](../partie-1-fondements/chapitre-03-optimisation.md) | [Retour](../README.md) | [Suite â¡ï¸](./chapitre-05-prediction-concepts.md)

