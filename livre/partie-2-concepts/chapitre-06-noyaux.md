# Chapitre 6 : Produits Internes et Noyaux Reproduisants

## ğŸ“š Introduction

Les noyaux sont un outil puissant qui permet d'Ã©tendre les mÃ©thodes linÃ©aires Ã  des espaces de dimension infinie. Ce chapitre introduit la thÃ©orie des noyaux reproduisants.

---

## 6.1 Introduction

Les **noyaux** permettent de calculer des produits scalaires dans des espaces de haute dimension sans calculer explicitement la transformation.

---

## 6.2 DÃ©finitions de Base

### 6.2.1 Espaces Ã  Produit Interne

Un **espace Ã  produit interne** (H, âŸ¨Â·,Â·âŸ©) est un espace vectoriel avec un produit scalaire.

**PropriÃ©tÃ©s** :
- LinÃ©aritÃ© : âŸ¨Î±u + Î²v, wâŸ© = Î±âŸ¨u,wâŸ© + Î²âŸ¨v,wâŸ©
- SymÃ©trie : âŸ¨u,vâŸ© = âŸ¨v,uâŸ©
- PositivitÃ© : âŸ¨u,uâŸ© â‰¥ 0

### 6.2.2 Espaces de CaractÃ©ristiques et Noyaux

**Fonction noyau** K : ğ’³ Ã— ğ’³ â†’ â„ est **dÃ©finie positive** si :
```
Î£áµ¢â±¼ Î±áµ¢Î±â±¼ K(xáµ¢, xâ±¼) â‰¥ 0
```
pour tous xâ‚, ..., xâ‚™ et Î±â‚, ..., Î±â‚™.

**ThÃ©orÃ¨me de Moore-Aronszajn** : Tout noyau dÃ©fini positif correspond Ã  un espace de Hilbert â„‹ et une carte Ï† : ğ’³ â†’ â„‹ tels que :
```
K(x, x') = âŸ¨Ï†(x), Ï†(x')âŸ©_â„‹
```

---

## 6.3 Exemples

### 6.3.1 Produit Scalaire

Le noyau linÃ©aire :
```
K(x, x') = xáµ€x'
```

### 6.3.2 Noyaux Polynomiaux

```
K(x, x') = (xáµ€x' + c)^d
```

**Exemple** : Pour d = 2, c = 0 en â„Â² :
```
K(x, x') = (xâ‚xâ‚' + xâ‚‚xâ‚‚')Â²
         = xâ‚Â²xâ‚'Â² + 2xâ‚xâ‚‚xâ‚'xâ‚‚' + xâ‚‚Â²xâ‚‚'Â²
```

Espace de caractÃ©ristiques : Ï†(x) = [xâ‚Â², âˆš2xâ‚xâ‚‚, xâ‚‚Â²]

### 6.3.3 Noyau Gaussien (RBF)

```
K(x, x') = exp(-â€–x - x'â€–Â²/(2ÏƒÂ²))
```

**PropriÃ©tÃ©** : Correspond Ã  un espace de dimension infinie !

```python
from sklearn.metrics.pairwise import rbf_kernel

K = rbf_kernel(X, gamma=1/(2*sigma**2))
```

### 6.3.4 ThÃ©orÃ¨mes de Construction

**ThÃ©orÃ¨me** : Si Kâ‚ et Kâ‚‚ sont des noyaux, alors :
- Î±Kâ‚ + Î²Kâ‚‚ (Î±, Î² â‰¥ 0)
- Kâ‚ Â· Kâ‚‚
- f(Kâ‚) oÃ¹ f est une sÃ©rie entiÃ¨re Ã  coefficients positifs

### 6.3.5 OpÃ©rations sur les Noyaux

**Somme** :
```
K(x, x') = Kâ‚(x, x') + Kâ‚‚(x, x')
```

**Produit** :
```
K(x, x') = Kâ‚(x, x') Â· Kâ‚‚(x, x')
```

**Composition avec fonction** :
```
K(x, x') = f(x)áµ€f(x')
```

---

## 6.4 Projection sur Sous-Espace

Dans un espace â„‹, la projection de f sur span{Ï†(xâ‚), ..., Ï†(xâ‚™)} est :

```
fÌ‚ = Î£áµ¢ Î±áµ¢ Ï†(xáµ¢)
```

**ThÃ©orÃ¨me du reprÃ©sentant** : La solution de minimiser
```
â€–fâ€–Â²_â„‹ + Î» Î£áµ¢ L(yáµ¢, f(xáµ¢))
```
est de la forme f = Î£áµ¢ Î±áµ¢ K(Â·, xáµ¢).

---

## ğŸ’¡ Applications

1. **SVM** : Classification dans espace de caractÃ©ristiques
2. **Kernel Ridge Regression**
3. **Kernel PCA** : PCA non linÃ©aire
4. **Kernel K-means** : Clustering non linÃ©aire

```python
from sklearn.svm import SVC
from sklearn.decomposition import KernelPCA

# SVM avec noyau RBF
svm = SVC(kernel='rbf', gamma=0.1)
svm.fit(X_train, y_train)

# Kernel PCA
kpca = KernelPCA(n_components=2, kernel='rbf')
X_transformed = kpca.fit_transform(X)
```

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-05-prediction-concepts.md) | [Retour](../README.md) | [Suite â¡ï¸](../partie-3-apprentissage-supervise/chapitre-07-regression-lineaire.md)

