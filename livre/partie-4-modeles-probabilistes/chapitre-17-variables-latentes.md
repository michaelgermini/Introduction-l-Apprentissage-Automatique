# Chapitre 17 : Variables Latentes et MÃ©thodes Variationnelles

## ğŸ“š Introduction

Ce chapitre traite des modÃ¨les avec variables non observÃ©es et des mÃ©thodes d'approximation variationnelle.

## ğŸ—ºï¸ Carte Mentale : Variables Latentes

```
              VARIABLES LATENTES
                      â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚             â”‚             â”‚
   MODÃˆLES       INFÃ‰RENCE      APPLICATIONS
        â”‚             â”‚             â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”     â”Œâ”€â”€â”€â”´â”€â”€â”€â”     â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚     â”‚       â”‚     â”‚       â”‚
  GMM    HMM     EM    VI/VB   Topic  PCA
 Mixture Hidden  â”‚     Mean   Model Probab.
    â”‚   Markov  ELBO  Field    LDA    â”‚
 Clusters  â”‚    Max  Approx.        Factor
         States                      Analysis
```

## ğŸ“Š Tableau : Algorithme EM vs InfÃ©rence Variationnelle

| **CritÃ¨re** | **EM** | **InfÃ©rence Variationnelle** |
|------------|--------|----------------------------|
| **Objectif** | Maximiser P(X\|Î¸) | Maximiser ELBO |
| **Distribution q** | Exacte P(Z\|X,Î¸) | Approximation q(Z) |
| **E-step** | âœ“ Calculer posteriors | âœ“ Optimiser q(Z) |
| **M-step** | âœ“ Optimiser Î¸ | âœ“ Optimiser Î¸ |
| **Convergence** | âœ“ Garantie (croissante) | âœ“ Garantie (ELBO â†‘) |
| **Calcul** | Exact si simple | Approximatif |
| **Usage** | GMM, HMM | Deep Learning (VAE) |

## ğŸ¯ Algorithme EM : Visualisation

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           EXPECTATION-MAXIMIZATION (EM)                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Exemple : Gaussian Mixture Model (GMM)

DonnÃ©es :     â—â—â—    â—‹â—‹â—‹    â– â– â– 
              â—â—      â—‹â—‹     â– â– 
              â—â—â—    â—‹â—‹â—‹    â– â– â– 

INITIALISATION :
  Î¸â½â°â¾ = {Î¼â‚–, Î£â‚–, Ï€â‚–}  (3 Gaussiennes)

ITÃ‰RATION t :

  E-STEP : Calculer responsabilitÃ©s
    Î³áµ¢â‚– = P(Záµ¢ = k | xáµ¢, Î¸â½áµ—â¾)
        = Ï€â‚– N(xáµ¢|Î¼â‚–,Î£â‚–) / Î£â±¼ Ï€â±¼ N(xáµ¢|Î¼â±¼,Î£â±¼)
    
    â— point xáµ¢ â†’ Î³áµ¢â‚=0.8, Î³áµ¢â‚‚=0.1, Î³áµ¢â‚ƒ=0.1
    (80% cluster 1)

  M-STEP : Mettre Ã  jour paramÃ¨tres
    Î¼â‚–â½áµ—âºÂ¹â¾ = Î£áµ¢ Î³áµ¢â‚– xáµ¢ / Î£áµ¢ Î³áµ¢â‚–
    (moyenne pondÃ©rÃ©e)
    
    Î£â‚–â½áµ—âºÂ¹â¾ = Î£áµ¢ Î³áµ¢â‚– (xáµ¢-Î¼â‚–)(xáµ¢-Î¼â‚–)áµ€ / Î£áµ¢ Î³áµ¢â‚–
    
    Ï€â‚–â½áµ—âºÂ¹â¾ = Î£áµ¢ Î³áµ¢â‚– / n

CONVERGENCE :
  log P(X|Î¸) augmente Ã  chaque itÃ©ration
  ArrÃªt : Î” log P(X|Î¸) < Îµ

RÃ©sultat :    â—â—â—    â—‹â—‹â—‹    â– â– â– 
              â—â—      â—‹â—‹     â– â– 
             â•±  â•²   â•±  â•²   â•±  â•²
           N(Î¼â‚) N(Î¼â‚‚) N(Î¼â‚ƒ)
           Clusters identifiÃ©s !
```

---

## 17.1 Introduction

**Variables latentes** Z : non observÃ©es mais influencent les donnÃ©es X.

**Exemple** : ModÃ¨les de mÃ©lange.

---

## 17.2 Principe Variationnel

**ELBO** (Evidence Lower Bound) :
```
log P(X) â‰¥ ğ”¼_q[log P(X, Z)] - ğ”¼_q[log q(Z)]
         = ğ”¼_q[log P(X, Z)/q(Z)]
```

**Objectif** : Maximiser ELBO par rapport Ã  q.

---

## 17.3 Approximations

### 17.3.1 Approximation de Mode

q(Z) = Î´(Z - áº‘) oÃ¹ áº‘ = argmax P(Z|X)

### 17.3.2 Approximation Gaussienne

q(Z) = N(Î¼, Î£)

### 17.3.3 Mean-Field

q(Z) = âˆáµ¢ q_i(Z_i) (indÃ©pendance)

---

## 17.4 Algorithme EM

**E-step** : Calculer Q(Î¸) = ğ”¼_{Z|X,Î¸_old}[log P(X, Z|Î¸)]

**M-step** : Î¸_new = argmax_Î¸ Q(Î¸)

```python
def em_algorithm(X, n_iter):
    # Initialisation
    theta = initialize_params()
    
    for t in range(n_iter):
        # E-step
        responsibilities = compute_responsibilities(X, theta)
        
        # M-step
        theta = update_parameters(X, responsibilities)
    
    return theta
```

---

## 17.5 ModÃ¨les de MÃ©lange Gaussien

```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=3)
gmm.fit(X)

# PrÃ©diction
labels = gmm.predict(X)
proba = gmm.predict_proba(X)
```

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-16-reseaux-bayesiens.md) | [Retour](../README.md) | [Suite â¡ï¸](./chapitre-18-apprentissage-graphiques.md)

