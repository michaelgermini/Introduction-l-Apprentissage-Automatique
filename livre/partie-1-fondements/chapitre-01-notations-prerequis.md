# Chapitre 1 : Notations GÃ©nÃ©rales et PrÃ©requis MathÃ©matiques

## ğŸ“š Introduction

Ce chapitre Ã©tablit les fondations mathÃ©matiques nÃ©cessaires pour comprendre l'apprentissage automatique. Nous couvrons les notations utilisÃ©es tout au long du livre et rÃ©visons les concepts essentiels d'algÃ¨bre linÃ©aire, de topologie, de calcul diffÃ©rentiel et de thÃ©orie des probabilitÃ©s.

## ğŸ—ºï¸ Carte Mentale du Chapitre

```
                    PRÃ‰REQUIS MATHÃ‰MATIQUES ML
                            â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                   â”‚                   â”‚
    ALGÃˆBRE            TOPOLOGIE           CALCUL           PROBABILITÃ‰S
    LINÃ‰AIRE                              DIFFÃ‰RENTIEL
        â”‚                   â”‚                   â”‚                   â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”           â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚           â”‚       â”‚           â”‚       â”‚           â”‚       â”‚
 Vecteurs Matrices   Ouverts Compacts   Gradient Hessienne  EspÃ©rance Variance
    â”‚       â”‚           â”‚       â”‚           â”‚       â”‚           â”‚       â”‚
 Normes  Valeurs    Boules  FermÃ©s    Jacobienne Taylor    Bayes   Ïƒ-algÃ¨bre
        Propres                                                    
```

## ğŸ“Š Vue d'Ensemble : Classification des Concepts

| **Domaine** | **Concepts ClÃ©s** | **Application ML** | **ComplexitÃ©** |
|-------------|-------------------|-------------------|----------------|
| **AlgÃ¨bre LinÃ©aire** | Vecteurs, Matrices, Valeurs propres | ReprÃ©sentation des donnÃ©es, PCA | â­â­ |
| **Topologie** | Ouverts, FermÃ©s, Compacts | Convergence, ContinuitÃ© | â­â­â­ |
| **Calcul DiffÃ©rentiel** | Gradient, Hessienne, Taylor | Optimisation, Backprop | â­â­â­â­ |
| **ProbabilitÃ©s** | EspÃ©rance, Variance, Bayes | ModÃ¨les gÃ©nÃ©ratifs | â­â­â­â­â­ |

---

## 1.1 AlgÃ¨bre LinÃ©aire

### 1.1.1 Ensembles et Fonctions

#### Notations de Base

- **â„** : Ensemble des nombres rÃ©els
- **â„â¿** : Espace vectoriel euclidien de dimension n
- **â„áµË£â¿** : Ensemble des matrices rÃ©elles mÃ—n
- **â„•** : Ensemble des entiers naturels {0, 1, 2, ...}
- **â„¤** : Ensemble des entiers relatifs

#### Fonctions

Une **fonction** f : A â†’ B associe Ã  chaque Ã©lÃ©ment de A un unique Ã©lÃ©ment de B.

- **Domaine** : L'ensemble A
- **Codomaine** : L'ensemble B
- **Image** : L'ensemble {f(x) | x âˆˆ A}

**Exemple** :
```
f : â„Â² â†’ â„
f(xâ‚, xâ‚‚) = xâ‚Â² + xâ‚‚Â²
```

### 1.1.2 Vecteurs

Un **vecteur** dans â„â¿ est un n-uplet de nombres rÃ©els :

```
v = [vâ‚, vâ‚‚, ..., vâ‚™]áµ€
```

#### ğŸ“ ReprÃ©sentation GÃ©omÃ©trique

```
Vecteur en 2D :          Vecteur en 3D :
    
    â”‚                         â”‚ z
  vâ‚‚â”‚   â—(vâ‚,vâ‚‚)              â”‚
    â”‚  /â”‚                     â”‚    â—(x,y,z)
    â”‚ / â”‚                     â”‚   /â”‚\
    â”‚/  â”‚                     â”‚  / â”‚ \
    â””â”€â”€â”€â”´â”€â”€                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€
      vâ‚                     x/      \y
```

#### OpÃ©rations sur les Vecteurs

##### 1. **Addition Vectorielle**

```
u + v = [uâ‚+vâ‚, uâ‚‚+vâ‚‚, ..., uâ‚™+vâ‚™]áµ€
```

**ğŸ“ Exemple dÃ©taillÃ© pas Ã  pas** :

```
Soit u = [2, 3, 1]áµ€  et  v = [1, -1, 4]áµ€

Ã‰tape 1 : Additionner composante par composante
    uâ‚ + vâ‚ = 2 + 1 = 3
    uâ‚‚ + vâ‚‚ = 3 + (-1) = 2
    uâ‚ƒ + vâ‚ƒ = 1 + 4 = 5

Ã‰tape 2 : Former le vecteur rÃ©sultat
    u + v = [3, 2, 5]áµ€

ReprÃ©sentation gÃ©omÃ©trique (rÃ¨gle du parallÃ©logramme) :
         u+v
      â—------â—
     /â”‚     /
    / â”‚    /
   /  â”‚   / v
  â—---â”‚--â—
     u
```

##### 2. **Multiplication Scalaire**

```
Î±v = [Î±vâ‚, Î±vâ‚‚, ..., Î±vâ‚™]áµ€
```

**ğŸ“ Exemple dÃ©taillÃ©** :

```
Soit Î± = 3  et  v = [2, -1, 4]áµ€

Ã‰tape 1 : Multiplier chaque composante par Î±
    Î±Â·vâ‚ = 3 Ã— 2 = 6
    Î±Â·vâ‚‚ = 3 Ã— (-1) = -3
    Î±Â·vâ‚ƒ = 3 Ã— 4 = 12

Ã‰tape 2 : RÃ©sultat
    3v = [6, -3, 12]áµ€

InterprÃ©tation : Le vecteur est Ã©tirÃ© par un facteur 3
```

##### 3. **Produit Scalaire** (Dot Product)

```
âŸ¨u, vâŸ© = uáµ€v = Î£áµ¢ uáµ¢váµ¢ = uâ‚vâ‚ + uâ‚‚vâ‚‚ + ... + uâ‚™vâ‚™
```

**ğŸ“ Calcul dÃ©taillÃ© pas Ã  pas** :

```
Soit u = [1, 2, 3]áµ€  et  v = [4, 5, 6]áµ€

Ã‰tape 1 : Multiplier terme Ã  terme
    uâ‚ Ã— vâ‚ = 1 Ã— 4 = 4
    uâ‚‚ Ã— vâ‚‚ = 2 Ã— 5 = 10
    uâ‚ƒ Ã— vâ‚ƒ = 3 Ã— 6 = 18

Ã‰tape 2 : Sommer tous les produits
    âŸ¨u, vâŸ© = 4 + 10 + 18 = 32

InterprÃ©tation gÃ©omÃ©trique :
    âŸ¨u, vâŸ© = â€–uâ€– Â· â€–vâ€– Â· cos(Î¸)
    oÃ¹ Î¸ est l'angle entre u et v
```

**ğŸ” Cas particuliers** :

| **Cas** | **âŸ¨u, vâŸ©** | **InterprÃ©tation** |
|---------|-----------|-------------------|
| u âŠ¥ v (orthogonal) | 0 | Vecteurs perpendiculaires |
| u âˆ¥ v (colinÃ©aire) | Â±â€–uâ€–Â·â€–vâ€– | MÃªme direction (Â±) |
| Î¸ = 90Â° | 0 | OrthogonalitÃ© |

##### 4. **Norme Euclidienne**

```
â€–vâ€– = âˆš(váµ€v) = âˆš(Î£áµ¢ váµ¢Â²) = âˆš(vâ‚Â² + vâ‚‚Â² + ... + vâ‚™Â²)
```

**ğŸ“ Calcul dÃ©taillÃ©** :

```
Soit v = [3, 4, 12]áµ€

Ã‰tape 1 : Calculer le carrÃ© de chaque composante
    vâ‚Â² = 3Â² = 9
    vâ‚‚Â² = 4Â² = 16
    vâ‚ƒÂ² = 12Â² = 144

Ã‰tape 2 : Sommer les carrÃ©s
    Î£ váµ¢Â² = 9 + 16 + 144 = 169

Ã‰tape 3 : Prendre la racine carrÃ©e
    â€–vâ€– = âˆš169 = 13

âœ“ Le vecteur v a une longueur de 13
```

**ğŸ¯ Autres normes importantes** :

| **Norme** | **DÃ©finition** | **Exemple pour v=[3,4]** |
|-----------|---------------|-------------------------|
| Lâ‚ (Manhattan) | â€–vâ€–â‚ = Î£\|váµ¢\| | \|3\|+\|4\| = 7 |
| Lâ‚‚ (Euclidienne) | â€–vâ€–â‚‚ = âˆš(Î£váµ¢Â²) | âˆš(9+16) = 5 |
| Lâˆ (Maximum) | â€–vâ€–âˆ = max\|váµ¢\| | max(3,4) = 4 |

#### PropriÃ©tÃ©s Importantes

##### **InÃ©galitÃ© de Cauchy-Schwarz**

```
|âŸ¨u, vâŸ©| â‰¤ â€–uâ€– Â· â€–vâ€–
```

**ğŸ“ DÃ©monstration avec exemple** :

```
Soit u = [1, 2]áµ€  et  v = [3, 4]áµ€

Calcul du membre de gauche :
    âŸ¨u, vâŸ© = 1Ã—3 + 2Ã—4 = 11
    |âŸ¨u, vâŸ©| = 11

Calcul du membre de droite :
    â€–uâ€– = âˆš(1Â² + 2Â²) = âˆš5 â‰ˆ 2.236
    â€–vâ€– = âˆš(3Â² + 4Â²) = âˆš25 = 5
    â€–uâ€–Â·â€–vâ€– = âˆš5 Ã— 5 = 5âˆš5 â‰ˆ 11.180

VÃ©rification : 11 â‰¤ 11.180 âœ“
```

##### **InÃ©galitÃ© Triangulaire**

```
â€–u + vâ€– â‰¤ â€–uâ€– + â€–vâ€–
```

**ğŸ“ Exemple numÃ©rique** :

```
Soit u = [3, 4]áµ€  et  v = [1, 2]áµ€

Ã‰tape 1 : Calculer u + v
    u + v = [3+1, 4+2]áµ€ = [4, 6]áµ€

Ã‰tape 2 : Calculer â€–u + vâ€–
    â€–u + vâ€– = âˆš(4Â² + 6Â²) = âˆš52 â‰ˆ 7.211

Ã‰tape 3 : Calculer â€–uâ€– + â€–vâ€–
    â€–uâ€– = âˆš(9 + 16) = 5
    â€–vâ€– = âˆš(1 + 4) = âˆš5 â‰ˆ 2.236
    â€–uâ€– + â€–vâ€– â‰ˆ 7.236

VÃ©rification : 7.211 â‰¤ 7.236 âœ“

InterprÃ©tation : Le chemin direct est toujours â‰¤ Ã  la somme des chemins
```

**Exemple Pratique en Python** :
```python
import numpy as np

# Vecteurs
u = np.array([1, 2, 3])
v = np.array([4, 5, 6])

# 1. Produit scalaire
dot_product = np.dot(u, v)
print(f"âŸ¨u, vâŸ© = {dot_product}")  # 32

# 2. Norme euclidienne
norm_u = np.linalg.norm(u)
norm_v = np.linalg.norm(v)
print(f"â€–uâ€– = {norm_u:.3f}")  # 3.742
print(f"â€–vâ€– = {norm_v:.3f}")  # 8.775

# 3. Angle entre les vecteurs
cos_theta = dot_product / (norm_u * norm_v)
theta_rad = np.arccos(cos_theta)
theta_deg = np.degrees(theta_rad)
print(f"Angle Î¸ = {theta_deg:.2f}Â°")  # 12.93Â°

# 4. VÃ©rification Cauchy-Schwarz
print(f"|âŸ¨u,vâŸ©| = {abs(dot_product)}")
print(f"â€–uâ€–Â·â€–vâ€– = {norm_u * norm_v:.3f}")
print(f"Cauchy-Schwarz vÃ©rifiÃ© : {abs(dot_product) <= norm_u * norm_v}")

# 5. Projection orthogonale de u sur v
projection = (dot_product / (norm_v ** 2)) * v
print(f"proj_v(u) = {projection}")
```

### 1.1.3 Matrices

Une **matrice** A âˆˆ â„áµË£â¿ est un tableau rectangulaire de m lignes et n colonnes :

```
        n colonnes
      â†â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’
    â”Œ             â”  â†‘
    â”‚ aâ‚â‚  aâ‚â‚‚ â€¦ aâ‚â‚™â”‚  â”‚
A = â”‚ aâ‚‚â‚  aâ‚‚â‚‚ â€¦ aâ‚‚â‚™â”‚  â”‚ m lignes
    â”‚  â‹®    â‹®  â‹±  â‹® â”‚  â”‚
    â”‚ aáµâ‚  aáµâ‚‚ â€¦ aáµâ‚™â”‚  â†“
    â””             â”˜

Notation : A âˆˆ â„áµË£â¿  (m=lignes, n=colonnes)
```

#### ğŸ”¤ Classification des Matrices

| **Type** | **PropriÃ©tÃ©** | **Exemple 2Ã—2** | **Application ML** |
|----------|--------------|----------------|-------------------|
| **CarrÃ©e** | m = n | [1 2; 3 4] | Transformations |
| **Rectangulaire** | m â‰  n | [1 2 3; 4 5 6] | DonnÃ©es (nÃ—p) |
| **Diagonale** | aáµ¢â±¼=0 si iâ‰ j | [2 0; 0 3] | Covariance |
| **IdentitÃ©** | I : Iáµ¢áµ¢=1, reste=0 | [1 0; 0 1] | Neutre |
| **SymÃ©trique** | A = Aáµ€ | [1 2; 2 3] | Hessienne |
| **Orthogonale** | Aáµ€A = I | [cos Î¸ -sin Î¸; sin Î¸ cos Î¸] | Rotation |
| **DÃ©finie positive** | xáµ€Ax > 0 âˆ€xâ‰ 0 | [2 1; 1 2] | ConvexitÃ© |

#### OpÃ©rations Matricielles

##### 1. **Multiplication Matricielle**

Si A âˆˆ â„áµË£â¿ et B âˆˆ â„â¿Ë£áµ–, alors C = AB âˆˆ â„áµË£áµ– avec :

```
cáµ¢â±¼ = Î£â‚–â‚Œâ‚â¿ aáµ¢â‚– bâ‚–â±¼ = aáµ¢â‚bâ‚â±¼ + aáµ¢â‚‚bâ‚‚â±¼ + ... + aáµ¢â‚™bâ‚™â±¼
```

**ğŸ“ Exemple dÃ©taillÃ© complet** :

```
Soit A = [1  2  3]  âˆˆ â„Â²Ë£Â³    et    B = [2  1]  âˆˆ â„Â³Ë£Â²
         [4  5  6]                        [0  3]
                                          [1  2]

Dimension de C = AB :  (2Ã—3) Ã— (3Ã—2) = (2Ã—2) âœ“ Compatible !

Calcul de chaque Ã©lÃ©ment c áµ¢â±¼ :

â”Œâ”€ Ã‰lÃ©ment câ‚â‚ (ligne 1 de A Ã— colonne 1 de B) â”€â”
â”‚ câ‚â‚ = aâ‚â‚bâ‚â‚ + aâ‚â‚‚bâ‚‚â‚ + aâ‚â‚ƒbâ‚ƒâ‚                â”‚
â”‚     = 1Ã—2 + 2Ã—0 + 3Ã—1                          â”‚
â”‚     = 2 + 0 + 3 = 5                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Ã‰lÃ©ment câ‚â‚‚ (ligne 1 de A Ã— colonne 2 de B) â”€â”
â”‚ câ‚â‚‚ = aâ‚â‚bâ‚â‚‚ + aâ‚â‚‚bâ‚‚â‚‚ + aâ‚â‚ƒbâ‚ƒâ‚‚                â”‚
â”‚     = 1Ã—1 + 2Ã—3 + 3Ã—2                          â”‚
â”‚     = 1 + 6 + 6 = 13                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Ã‰lÃ©ment câ‚‚â‚ (ligne 2 de A Ã— colonne 1 de B) â”€â”
â”‚ câ‚‚â‚ = aâ‚‚â‚bâ‚â‚ + aâ‚‚â‚‚bâ‚‚â‚ + aâ‚‚â‚ƒbâ‚ƒâ‚                â”‚
â”‚     = 4Ã—2 + 5Ã—0 + 6Ã—1                          â”‚
â”‚     = 8 + 0 + 6 = 14                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€ Ã‰lÃ©ment câ‚‚â‚‚ (ligne 2 de A Ã— colonne 2 de B) â”€â”
â”‚ câ‚‚â‚‚ = aâ‚‚â‚bâ‚â‚‚ + aâ‚‚â‚‚bâ‚‚â‚‚ + aâ‚‚â‚ƒbâ‚ƒâ‚‚                â”‚
â”‚     = 4Ã—1 + 5Ã—3 + 6Ã—2                          â”‚
â”‚     = 4 + 15 + 12 = 31                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

RÃ©sultat :
    C = AB = [5   13]
             [14  31]

SchÃ©ma visuel :
    [1 2 3]   [2 1]   [5  13]
    [4 5 6] Ã— [0 3] = [14 31]
              [1 2]
```

**âš ï¸ PropriÃ©tÃ©s importantes** :
- AB â‰  BA en gÃ©nÃ©ral (non-commutativitÃ©)
- (AB)C = A(BC) (associativitÃ©)
- A(B + C) = AB + AC (distributivitÃ©)

##### 2. **TransposÃ©e**

```
(Aáµ€)áµ¢â±¼ = Aâ±¼áµ¢    (Ã©change lignes â†” colonnes)
```

**ğŸ“ Exemple concret** :

```
Soit A = [1  2  3]  âˆˆ â„Â²Ë£Â³
         [4  5  6]

TransposÃ©e :
         [1  4]
    Aáµ€ = [2  5]  âˆˆ â„Â³Ë£Â²
         [3  6]

Visualisation :
    Ligne 1 de A  â†’  Colonne 1 de Aáµ€
    Ligne 2 de A  â†’  Colonne 2 de Aáµ€

PropriÃ©tÃ©s :
    (Aáµ€)áµ€ = A
    (AB)áµ€ = Báµ€Aáµ€  âš ï¸ Attention Ã  l'ordre inversÃ© !
    (A + B)áµ€ = Aáµ€ + Báµ€
```

##### 3. **Trace**

```
tr(A) = Î£áµ¢â‚Œâ‚â¿ aáµ¢áµ¢    (somme des Ã©lÃ©ments diagonaux, A carrÃ©e)
```

**ğŸ“ Exemple et propriÃ©tÃ©s** :

```
Soit A = [2  1  0]
         [1  3  2]
         [0  2  1]

Calcul de la trace :
    tr(A) = aâ‚â‚ + aâ‚‚â‚‚ + aâ‚ƒâ‚ƒ
          = 2 + 3 + 1
          = 6

PropriÃ©tÃ©s essentielles :
    1. tr(A + B) = tr(A) + tr(B)
    2. tr(Î±A) = Î±Â·tr(A)
    3. tr(AB) = tr(BA)  âš ï¸ Important !
    4. tr(Aáµ€) = tr(A)
    5. tr(Aáµ€A) = Î£áµ¢â±¼ aáµ¢â±¼Â²  (norme de Frobenius au carrÃ©)

Application ML : La trace est utilisÃ©e dans l'ACP et la rÃ©gularisation
```

##### 4. **DÃ©terminant**

Pour une matrice carrÃ©e A âˆˆ â„â¿Ë£â¿ :

**ğŸ“ Calcul dÃ©taillÃ© pour 2Ã—2** :

```
A = [a  b]    âŸ¹    det(A) = ad - bc
    [c  d]

Exemple :
A = [3  2]    det(A) = 3Ã—5 - 2Ã—1 = 15 - 2 = 13
    [1  5]

InterprÃ©tation gÃ©omÃ©trique :
    det(A) = aire du parallÃ©logramme formÃ© par les vecteurs colonnes

    det(A) > 0  â†’  Orientation prÃ©servÃ©e
    det(A) < 0  â†’  Orientation inversÃ©e
    det(A) = 0  â†’  Matrice singuliÃ¨re (non inversible)
```

**ğŸ“ Calcul pour 3Ã—3 (rÃ¨gle de Sarrus)** :

```
A = [a  b  c]
    [d  e  f]
    [g  h  i]

det(A) = aei + bfg + cdh - ceg - afh - bdi

Exemple numÃ©rique :
A = [1  2  3]
    [0  1  4]
    [5  6  0]

det(A) = 1Ã—1Ã—0 + 2Ã—4Ã—5 + 3Ã—0Ã—6 - 3Ã—1Ã—5 - 1Ã—4Ã—6 - 2Ã—0Ã—0
       = 0 + 40 + 0 - 15 - 24 - 0
       = 1

PropriÃ©tÃ©s :
    1. det(AB) = det(A) Ã— det(B)
    2. det(Aáµ€) = det(A)
    3. det(Aâ»Â¹) = 1/det(A)  si A inversible
    4. det(Î±A) = Î±â¿ det(A)  pour A âˆˆ â„â¿Ë£â¿
```

#### Matrices SpÃ©ciales

##### **Matrice IdentitÃ©** I

```
    [1  0  0  â€¦  0]
    [0  1  0  â€¦  0]
I = [0  0  1  â€¦  0]
    [â‹®  â‹®  â‹®  â‹±  â‹®]
    [0  0  0  â€¦  1]

PropriÃ©tÃ© : AI = IA = A  (Ã©lÃ©ment neutre)
```

##### **Matrice Diagonale** D

```
    [dâ‚  0   0  â€¦  0 ]
    [0   dâ‚‚  0  â€¦  0 ]
D = [0   0   dâ‚ƒ â€¦  0 ]
    [â‹®   â‹®   â‹®  â‹±  â‹® ]
    [0   0   0  â€¦  dâ‚™]

Multiplication rapide : (DA)áµ¢â±¼ = dáµ¢ aáµ¢â±¼
```

##### **Matrice SymÃ©trique** A = Aáµ€

```
    [1  2  3]       Ã‰lÃ©ment clÃ© :
A = [2  5  6]       aáµ¢â±¼ = aâ±¼áµ¢
    [3  6  9]

Application : Matrices de covariance, Hessienne
```

##### **Matrice Orthogonale** Q : Qáµ€Q = I

```
Exemple (rotation de 90Â°) :
    [0  -1]
Q = [1   0]

VÃ©rification :
    Qáµ€Q = [0  1] [0  -1] = [1  0] = I âœ“
          [-1 0] [1   0]   [0  1]

PropriÃ©tÃ©s :
    - det(Q) = Â±1
    - PrÃ©serve les normes : â€–Qxâ€– = â€–xâ€–
    - PrÃ©serve les angles
```

#### Valeurs Propres et Vecteurs Propres

Pour une matrice carrÃ©e A âˆˆ â„â¿Ë£â¿ :

```
Av = Î»v    oÃ¹ Î» âˆˆ â„ (valeur propre) et v â‰  0 (vecteur propre)
```

**ğŸ“ Calcul dÃ©taillÃ© pour une matrice 2Ã—2** :

```
Soit A = [4  2]
         [2  3]

Ã‰TAPE 1 : Trouver l'Ã©quation caractÃ©ristique
    det(A - Î»I) = 0

    A - Î»I = [4-Î»   2  ]
             [2    3-Î»]

    det(A - Î»I) = (4-Î»)(3-Î») - 2Ã—2
                = 12 - 4Î» - 3Î» + Î»Â² - 4
                = Î»Â² - 7Î» + 8 = 0

Ã‰TAPE 2 : RÃ©soudre l'Ã©quation caractÃ©ristique
    Î» = (7 Â± âˆš(49-32))/2 = (7 Â± âˆš17)/2

    Î»â‚ â‰ˆ 5.56    Î»â‚‚ â‰ˆ 1.44

Ã‰TAPE 3 : Trouver les vecteurs propres

Pour Î»â‚ â‰ˆ 5.56 :
    (A - Î»â‚I)vâ‚ = 0
    [4-5.56   2    ] [vâ‚â‚] = [0]
    [2      3-5.56] [vâ‚â‚‚]   [0]

    [-1.56   2   ] [vâ‚â‚] = [0]
    [2     -2.56] [vâ‚â‚‚]   [0]

    De la 1Ã¨re Ã©quation : -1.56vâ‚â‚ + 2vâ‚â‚‚ = 0
                          vâ‚â‚‚ = 0.78vâ‚â‚

    Normalisation (â€–vâ‚â€– = 1) :
    vâ‚ â‰ˆ [0.79]
         [0.62]

Pour Î»â‚‚ â‰ˆ 1.44 :
    vâ‚‚ â‰ˆ [-0.62]
         [0.79]

VÃ©rification :
    Avâ‚ = [4  2] [0.79]   [4.39]         [0.79]
          [2  3] [0.62] = [3.44] â‰ˆ 5.56  [0.62] = Î»â‚vâ‚ âœ“

InterprÃ©tation gÃ©omÃ©trique :
    vâ‚ : direction d'Ã©tirement maximal (facteur 5.56)
    vâ‚‚ : direction d'Ã©tirement minimal (facteur 1.44)
```

**PropriÃ©tÃ©s Fondamentales** :

| **PropriÃ©tÃ©** | **Matrice GÃ©nÃ©rale** | **Matrice SymÃ©trique** |
|--------------|---------------------|----------------------|
| Valeurs propres rÃ©elles | Non garanti | âœ“ Toujours |
| Vecteurs propres orthogonaux | Non garanti | âœ“ Toujours |
| Diagonalisable | Non garanti | âœ“ Toujours |
| det(A) | = Î  Î»áµ¢ | = Î  Î»áµ¢ |
| tr(A) | = Î£ Î»áµ¢ | = Î£ Î»áµ¢ |

**Applications en Machine Learning** :

```
1. PCA (Analyse en Composantes Principales)
   â†’ Les vecteurs propres de la covariance = directions principales

2. PageRank (Google)
   â†’ Vecteur propre dominant de la matrice de transition

3. Spectral Clustering
   â†’ Vecteurs propres de la matrice Laplacienne

4. StabilitÃ© des rÃ©seaux de neurones
   â†’ Valeurs propres de la Hessienne
```

**Exemple Python Complet** :
```python
import numpy as np

# Matrice symÃ©trique
A = np.array([[4, 2], 
              [2, 3]])

# Calcul des valeurs et vecteurs propres
eigenvalues, eigenvectors = np.linalg.eig(A)
print("Valeurs propres:", eigenvalues)
print("Vecteurs propres:\n", eigenvectors)

# VÃ©rification : Av = Î»v
for i in range(len(eigenvalues)):
    Î» = eigenvalues[i]
    v = eigenvectors[:, i]
    Av = A @ v
    Î»v = Î» * v
    print(f"\nVÃ©rification pour Î»_{i+1}:")
    print(f"Av = {Av}")
    print(f"Î»v = {Î»v}")
    print(f"Ã‰galitÃ© : {np.allclose(Av, Î»v)}")

# Diagonalisation : A = VÎ›Váµ€
V = eigenvectors
Î› = np.diag(eigenvalues)
A_reconstructed = V @ Î› @ V.T
print(f"\nReconstitution de A :")
print(f"Erreur : {np.linalg.norm(A - A_reconstructed)}")
```

### 1.1.4 Applications MultilinÃ©aires

Une **application bilinÃ©aire** B : â„â¿ Ã— â„â¿ â†’ â„ est linÃ©aire en chaque argument :

```
B(Î±u + Î²v, w) = Î±B(u,w) + Î²B(v,w)
B(u, Î±v + Î²w) = Î±B(u,v) + Î²B(u,w)
```

**Exemple** : Le produit scalaire âŸ¨u, vâŸ© = uáµ€v est bilinÃ©aire.

---

## 1.2 Topologie

### 1.2.1 Ensembles Ouverts et FermÃ©s dans â„áµˆ

#### Boule Ouverte

La **boule ouverte** de centre x et rayon r > 0 :
```
B(x, r) = {y âˆˆ â„áµˆ : â€–y - xâ€– < r}
```

#### Ensemble Ouvert

Un ensemble U âŠ‚ â„áµˆ est **ouvert** si pour tout x âˆˆ U, il existe r > 0 tel que B(x, r) âŠ‚ U.

#### Ensemble FermÃ©

Un ensemble F âŠ‚ â„áµˆ est **fermÃ©** si son complÃ©ment â„áµˆ \ F est ouvert.

**Exemples** :
- (a, b) est ouvert dans â„
- [a, b] est fermÃ© dans â„
- â„â¿ et âˆ… sont Ã  la fois ouverts et fermÃ©s

### 1.2.2 Ensembles Compacts

Un ensemble K âŠ‚ â„áµˆ est **compact** s'il est fermÃ© et bornÃ©.

**ThÃ©orÃ¨me de Heine-Borel** : Dans â„áµˆ, un ensemble est compact si et seulement si il est fermÃ© et bornÃ©.

**Importance en ML** : Les ensembles compacts garantissent l'existence de minima pour les fonctions continues (thÃ©orÃ¨me de Weierstrass).

### 1.2.3 Espaces MÃ©triques

Un **espace mÃ©trique** est un ensemble X muni d'une distance d : X Ã— X â†’ â„â‚Š telle que :

1. d(x, y) = 0 âŸº x = y
2. d(x, y) = d(y, x) (symÃ©trie)
3. d(x, z) â‰¤ d(x, y) + d(y, z) (inÃ©galitÃ© triangulaire)

**Exemples de distances** :
- Distance euclidienne : d(x, y) = â€–x - yâ€–
- Distance de Manhattan : d(x, y) = Î£áµ¢ |xáµ¢ - yáµ¢|
- Distance de Tchebychev : d(x, y) = maxáµ¢ |xáµ¢ - yáµ¢|

---

## 1.3 Calcul DiffÃ©rentiel

### 1.3.1 DiffÃ©rentielles

#### DÃ©rivÃ©e Directionnelle

La **dÃ©rivÃ©e directionnelle** de f : â„â¿ â†’ â„ en x dans la direction v est :

```
Dáµ¥f(x) = lim[hâ†’0] [f(x + hv) - f(x)] / h
```

#### Gradient

Le **gradient** de f : â„â¿ â†’ â„ en x est le vecteur :

```
âˆ‡f(x) = [âˆ‚f/âˆ‚xâ‚, âˆ‚f/âˆ‚xâ‚‚, ..., âˆ‚f/âˆ‚xâ‚™]áµ€
```

**PropriÃ©tÃ©s** :
- Le gradient pointe dans la direction de plus grande augmentation
- â€–âˆ‡f(x)â€– donne le taux de variation maximal

**Application en ML** : Le gradient est au cÅ“ur de l'algorithme de descente de gradient.

#### Matrice Jacobienne

Pour f : â„â¿ â†’ â„áµ, la **matrice Jacobienne** est :

```
Jf(x) = [âˆ‚fâ‚/âˆ‚xâ‚  ...  âˆ‚fâ‚/âˆ‚xâ‚™]
        [...       ...  ...]
        [âˆ‚fâ‚˜/âˆ‚xâ‚  ...  âˆ‚fâ‚˜/âˆ‚xâ‚™]
```

### 1.3.2 Exemples Importants

#### Fonction Quadratique

Pour f(x) = Â½xáµ€Ax - báµ€x :
```
âˆ‡f(x) = Ax - b
```

#### Fonction Exponentielle

Pour f(x) = exp(aáµ€x) :
```
âˆ‡f(x) = a exp(aáµ€x)
```

#### Fonction Logistique

Pour f(x) = log(1 + exp(aáµ€x)) :
```
âˆ‡f(x) = a / (1 + exp(-aáµ€x))
```

### 1.3.3 DÃ©rivÃ©es d'Ordre SupÃ©rieur

#### Matrice Hessienne

La **matrice Hessienne** de f : â„â¿ â†’ â„ est :

```
Hf(x) = [âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼]
```

**PropriÃ©tÃ©s** :
- Si f est deux fois continÃ»ment diffÃ©rentiable, H est symÃ©trique
- H dÃ©finit la courbure locale de f

**ConvexitÃ©** : f est convexe si H est semi-dÃ©finie positive partout.

### 1.3.4 ThÃ©orÃ¨me de Taylor

Le **dÃ©veloppement de Taylor** d'ordre 2 de f autour de x est :

```
f(x + h) â‰ˆ f(x) + âˆ‡f(x)áµ€h + Â½háµ€Hf(x)h
```

**Application** : UtilisÃ© pour l'analyse de convergence des algorithmes d'optimisation.

---

## 1.4 ThÃ©orie des ProbabilitÃ©s

### 1.4.1 HypothÃ¨ses GÃ©nÃ©rales et Notations

#### Espace de ProbabilitÃ©

Un **espace de probabilitÃ©** est un triplet (Î©, â„±, P) oÃ¹ :
- Î© est l'ensemble des rÃ©sultats possibles (univers)
- â„± est une Ïƒ-algÃ¨bre d'Ã©vÃ©nements
- P : â„± â†’ [0, 1] est une mesure de probabilitÃ©

#### Variable AlÃ©atoire

Une **variable alÃ©atoire** X est une fonction mesurable X : Î© â†’ â„.

**Notation** : X ~ P signifie que X suit la loi de probabilitÃ© P.

#### EspÃ©rance

L'**espÃ©rance** d'une variable alÃ©atoire X est :

```
ğ”¼[X] = âˆ« x dP(x)
```

**PropriÃ©tÃ©s** :
- LinÃ©aritÃ© : ğ”¼[Î±X + Î²Y] = Î±ğ”¼[X] + Î²ğ”¼[Y]
- Si X â‰¥ 0, alors ğ”¼[X] â‰¥ 0

#### Variance

La **variance** de X est :

```
Var(X) = ğ”¼[(X - ğ”¼[X])Â²] = ğ”¼[XÂ²] - (ğ”¼[X])Â²
```

**Ã‰cart-type** : Ïƒ(X) = âˆšVar(X)

### 1.4.2 ProbabilitÃ©s et EspÃ©rances Conditionnelles

#### ProbabilitÃ© Conditionnelle

La **probabilitÃ© conditionnelle** de A sachant B est :

```
P(A|B) = P(A âˆ© B) / P(B)
```

**Formule de Bayes** :
```
P(A|B) = P(B|A)P(A) / P(B)
```

**Application en ML** : La classification bayÃ©sienne utilise cette formule pour calculer P(classe|donnÃ©es).

#### EspÃ©rance Conditionnelle

L'**espÃ©rance conditionnelle** de X sachant Y est :

```
ğ”¼[X|Y] = g(Y) oÃ¹ g(y) = ğ”¼[X|Y=y]
```

**PropriÃ©tÃ© importante** :
```
ğ”¼[ğ”¼[X|Y]] = ğ”¼[X]  (Loi de l'espÃ©rance totale)
```

### 1.4.3 ThÃ©orie des ProbabilitÃ©s Mesurables

#### Ïƒ-AlgÃ¨bre

Une **Ïƒ-algÃ¨bre** â„± sur Î© est une famille d'ensembles telle que :
1. Î© âˆˆ â„±
2. Si A âˆˆ â„±, alors Aá¶œ âˆˆ â„±
3. Si Aâ‚, Aâ‚‚, ... âˆˆ â„±, alors â‹ƒáµ¢ Aáµ¢ âˆˆ â„±

#### Mesure de ProbabilitÃ©

Une **mesure de probabilitÃ©** P sur (Î©, â„±) satisfait :
1. P(Î©) = 1
2. P(A) â‰¥ 0 pour tout A âˆˆ â„±
3. Pour Aâ‚, Aâ‚‚, ... disjoints : P(â‹ƒáµ¢ Aáµ¢) = Î£áµ¢ P(Aáµ¢)

### 1.4.4 Produit de Mesures

Le **produit de mesures** Pâ‚ âŠ— Pâ‚‚ sur Î©â‚ Ã— Î©â‚‚ satisfait :

```
(Pâ‚ âŠ— Pâ‚‚)(A Ã— B) = Pâ‚(A) Â· Pâ‚‚(B)
```

**ThÃ©orÃ¨me de Fubini** : Pour une fonction intÃ©grable f :
```
âˆ«âˆ« f(x,y) d(Pâ‚âŠ—Pâ‚‚)(x,y) = âˆ«[âˆ« f(x,y) dPâ‚‚(y)] dPâ‚(x)
```

### 1.4.5 ContinuitÃ© Absolue et DensitÃ©s

Une mesure Q est **absolument continue** par rapport Ã  P (notÃ© Q << P) si :
```
P(A) = 0 âŸ¹ Q(A) = 0
```

**ThÃ©orÃ¨me de Radon-Nikodym** : Si Q << P, il existe une fonction f (densitÃ©) telle que :
```
Q(A) = âˆ«â‚ f dP
```

On note : f = dQ/dP

**Exemple** : La loi normale N(Î¼, ÏƒÂ²) a pour densitÃ© par rapport Ã  la mesure de Lebesgue :
```
f(x) = (1/âˆš(2Ï€ÏƒÂ²)) exp(-(x-Î¼)Â²/(2ÏƒÂ²))
```

### 1.4.6 ProbabilitÃ©s ThÃ©oriques Mesurables

#### IndÃ©pendance

Deux Ã©vÃ©nements A et B sont **indÃ©pendants** si :
```
P(A âˆ© B) = P(A) Â· P(B)
```

Deux variables alÃ©atoires X et Y sont **indÃ©pendantes** si :
```
P(X âˆˆ A, Y âˆˆ B) = P(X âˆˆ A) Â· P(Y âˆˆ B)
```
pour tous ensembles A et B.

### 1.4.7 EspÃ©rances Conditionnelles (Cas GÃ©nÃ©ral)

L'**espÃ©rance conditionnelle** ğ”¼[X|ğ’¢] est la projection de X sur l'espace LÂ²(ğ’¢) :

**PropriÃ©tÃ©s** :
1. ğ”¼[ğ”¼[X|ğ’¢]] = ğ”¼[X]
2. Si Y est ğ’¢-mesurable : ğ”¼[XY|ğ’¢] = Yğ”¼[X|ğ’¢]
3. InÃ©galitÃ© de Jensen : ğ”¼[Ï†(X)|ğ’¢] â‰¥ Ï†(ğ”¼[X|ğ’¢]) pour Ï† convexe

### 1.4.8 ProbabilitÃ©s Conditionnelles (Cas GÃ©nÃ©ral)

La **probabilitÃ© conditionnelle** P(A|ğ’¢) est dÃ©finie comme :
```
P(A|ğ’¢) = ğ”¼[1â‚|ğ’¢]
```

oÃ¹ 1â‚ est la fonction indicatrice de A.

---

## ğŸ’¡ Applications en Machine Learning

Les concepts de ce chapitre sont utilisÃ©s partout en ML :

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  CONCEPTS â†’ APPLICATIONS ML                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                 â”‚
â”‚  ğŸ“ ALGÃˆBRE LINÃ‰AIRE                                            â”‚
â”‚     Vecteurs & Matrices  â†’  ReprÃ©sentation des donnÃ©es          â”‚
â”‚     Valeurs propres      â†’  PCA, Spectral Clustering           â”‚
â”‚     Normes              â†’  Distance, SimilaritÃ©                â”‚
â”‚                                                                 â”‚
â”‚  ğŸ¯ CALCUL DIFFÃ‰RENTIEL                                         â”‚
â”‚     Gradient            â†’  Descente de gradient                â”‚
â”‚     Hessienne           â†’  Analyse de convergence              â”‚
â”‚     Jacobienne          â†’  Backpropagation                     â”‚
â”‚                                                                 â”‚
â”‚  ğŸ² PROBABILITÃ‰S                                                â”‚
â”‚     EspÃ©rance/Variance  â†’  Estimation, Incertitude             â”‚
â”‚     Bayes               â†’  Classification bayÃ©sienne           â”‚
â”‚     Conditionnelles     â†’  ModÃ¨les graphiques                  â”‚
â”‚                                                                 â”‚
â”‚  ğŸ“Š TOPOLOGIE                                                   â”‚
â”‚     Compacts            â†’  Existence de minima                 â”‚
â”‚     Ouverts/FermÃ©s      â†’  ContinuitÃ©, Convergence             â”‚
â”‚                                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“‹ RÃ©sumÃ© Visuel du Chapitre

### ğŸ”‘ Formules ClÃ©s Ã  Retenir

| **Concept** | **Formule** | **Application** |
|------------|------------|----------------|
| **Produit scalaire** | âŸ¨u, vâŸ© = Î£ uáµ¢váµ¢ | SimilaritÃ© |
| **Norme Lâ‚‚** | â€–vâ€– = âˆš(Î£ váµ¢Â²) | Distance |
| **Matrice Ã— Vecteur** | (Av)áµ¢ = Î£â±¼ aáµ¢â±¼vâ±¼ | Transformation |
| **Valeur propre** | Av = Î»v | PCA, Spectral |
| **Gradient** | âˆ‡f = [âˆ‚f/âˆ‚xáµ¢] | Optimisation |
| **Hessienne** | H = [âˆ‚Â²f/âˆ‚xáµ¢âˆ‚xâ±¼] | ConvexitÃ© |
| **Bayes** | P(A\|B) = P(B\|A)P(A)/P(B) | Classification |
| **Variance** | Var(X) = ğ”¼[XÂ²] - (ğ”¼[X])Â² | Incertitude |

### ğŸ¯ Checklist de ComprÃ©hension

AprÃ¨s ce chapitre, vous devriez pouvoir :

- âœ… Calculer le produit scalaire et la norme d'un vecteur
- âœ… Multiplier deux matrices (avec compatibilitÃ© des dimensions)
- âœ… Trouver les valeurs et vecteurs propres d'une matrice 2Ã—2
- âœ… Calculer le gradient d'une fonction simple
- âœ… Appliquer la formule de Bayes
- âœ… Distinguer les diffÃ©rents types de matrices (symÃ©trique, orthogonale, etc.)
- âœ… Comprendre l'interprÃ©tation gÃ©omÃ©trique des concepts

### ğŸ“Š Diagramme de DÃ©cision : Quelle OpÃ©ration Utiliser ?

```
            Dois-je faire un calcul ?
                     â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                         â”‚
    Sur VECTEURS            Sur MATRICES
        â”‚                         â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”               â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚       â”‚               â”‚           â”‚
 Distance  Angle      Transformation  Analyse
    â”‚       â”‚               â”‚           â”‚
 Norme Lâ‚‚  arccos      Multiplication Valeurs propres
  â€–vâ€–     (âŸ¨u,vâŸ©/      AÃ—v ou AÃ—B    Av = Î»v
         â€–uâ€–â€–vâ€–)
```

---

## ğŸ“ Exercices

### Exercice 1 : AlgÃ¨bre LinÃ©aire
Soit A = [[2, 1], [1, 3]]. Calculez ses valeurs propres et vecteurs propres.

### Exercice 2 : Calcul DiffÃ©rentiel
Pour f(x) = â€–xâ€–Â², calculez âˆ‡f(x) et Hf(x).

### Exercice 3 : ProbabilitÃ©s
DÃ©montrez que Var(X) = ğ”¼[XÂ²] - (ğ”¼[X])Â².

### Exercice 4 : Formule de Bayes
Un test mÃ©dical dÃ©tecte une maladie avec 95% de sensibilitÃ© et 90% de spÃ©cificitÃ©. Si 1% de la population a la maladie, quelle est la probabilitÃ© qu'une personne testÃ©e positive ait rÃ©ellement la maladie ?

---

## ğŸ”— RÃ©fÃ©rences et Lectures ComplÃ©mentaires

- **AlgÃ¨bre linÃ©aire** : Strang, G. "Linear Algebra and Its Applications"
- **Calcul** : Spivak, M. "Calculus on Manifolds"
- **ProbabilitÃ©s** : Billingsley, P. "Probability and Measure"

---

[â¬…ï¸ Retour Ã  la table des matiÃ¨res](../README.md) | [Chapitre suivant â¡ï¸](./chapitre-02-analyse-matricielle.md)

