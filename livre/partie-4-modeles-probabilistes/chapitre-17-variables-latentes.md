# Chapitre 17 : Variables Latentes et MÃ©thodes Variationnelles

## ğŸ“š Introduction

Ce chapitre traite des modÃ¨les avec variables non observÃ©es et des mÃ©thodes d'approximation variationnelle.

---

## 17.1 Introduction

**Variables latentes** Z : non observÃ©es mais influencent les donnÃ©es X.

**Exemple** : ModÃ¨les de mÃ©lange.

---

## 17.2 Principe Variationnel

**ELBO** (Evidence Lower Bound) :
```
log P(X) â‰¥ ğ”¼_q[log P(X, Z)] - ğ”¼_q[log q(Z)]
         = ğ”¼_q[log P(X, Z)/q(Z)]
```

**Objectif** : Maximiser ELBO par rapport Ã  q.

---

## 17.3 Approximations

### 17.3.1 Approximation de Mode

q(Z) = Î´(Z - áº‘) oÃ¹ áº‘ = argmax P(Z|X)

### 17.3.2 Approximation Gaussienne

q(Z) = N(Î¼, Î£)

### 17.3.3 Mean-Field

q(Z) = âˆáµ¢ q_i(Z_i) (indÃ©pendance)

---

## 17.4 Algorithme EM

**E-step** : Calculer Q(Î¸) = ğ”¼_{Z|X,Î¸_old}[log P(X, Z|Î¸)]

**M-step** : Î¸_new = argmax_Î¸ Q(Î¸)

```python
def em_algorithm(X, n_iter):
    # Initialisation
    theta = initialize_params()
    
    for t in range(n_iter):
        # E-step
        responsibilities = compute_responsibilities(X, theta)
        
        # M-step
        theta = update_parameters(X, responsibilities)
    
    return theta
```

---

## 17.5 ModÃ¨les de MÃ©lange Gaussien

```python
from sklearn.mixture import GaussianMixture

gmm = GaussianMixture(n_components=3)
gmm.fit(X)

# PrÃ©diction
labels = gmm.predict(X)
proba = gmm.predict_proba(X)
```

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-16-reseaux-bayesiens.md) | [Retour](../README.md) | [Suite â¡ï¸](./chapitre-18-apprentissage-graphiques.md)

