# Chapitre 21 : RÃ©duction de Dimension et Analyse Factorielle

## ğŸ“š Introduction

La rÃ©duction de dimension projette les donnÃ©es dans un espace de dimension rÃ©duite tout en prÃ©servant l'information.

## ğŸ—ºï¸ Carte Mentale : RÃ©duction de Dimension

```
                RÃ‰DUCTION DE DIMENSION
                          â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚                 â”‚                 â”‚
    LINÃ‰AIRE         NON-LINÃ‰AIRE       FACTORISATION
        â”‚                 â”‚                 â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”         â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚         â”‚       â”‚         â”‚       â”‚
  PCA    LDA      t-SNE  UMAP        NMF    ICA
    â”‚       â”‚         â”‚       â”‚         â”‚       â”‚
 SVD    Fisher   Manifold Graphe  Positive Independent
Variance  Discr.  Learning          Components
```

## ğŸ“Š Tableau Comparatif : MÃ©thodes

| **MÃ©thode** | **Type** | **LinÃ©aire** | **PrÃ©serve** | **ScalabilitÃ©** | **Usage** |
|------------|---------|-------------|-------------|----------------|-----------|
| **PCA** | Projection | âœ“ Oui | Variance | âœ“âœ“âœ“ Excellente | Standard, prÃ©traitement |
| **t-SNE** | Manifold | âœ— Non | Distances locales | âš ï¸ Faible | Visualisation 2D/3D |
| **UMAP** | Graphe | âœ— Non | Structure topologique | âœ“âœ“ Bonne | Visualisation moderne |
| **LDA** | Discriminant | âœ“ Oui | SÃ©parabilitÃ© classes | âœ“âœ“ Bonne | Classification |
| **Autoencoder** | Deep Learning | âœ— Non | Reconstruction | âœ“ Moyenne | ReprÃ©sentations |

---

## 21.1 Analyse en Composantes Principales (PCA)

### Principe

**Maximiser la variance** ou **minimiser l'erreur de reconstruction**.

**Solution** : Vecteurs propres de la matrice de covariance.

```python
from sklearn.decomposition import PCA

pca = PCA(n_components=2)
X_reduced = pca.fit_transform(X)

# Variance expliquÃ©e
print(pca.explained_variance_ratio_)

# Composantes principales
print(pca.components_)
```

---

## 21.2 Kernel PCA

**PCA non linÃ©aire** avec le kernel trick.

```python
from sklearn.decomposition import KernelPCA

kpca = KernelPCA(n_components=2, kernel='rbf', gamma=0.1)
X_reduced = kpca.fit_transform(X)
```

---

## 21.3 PCA Probabiliste

**ModÃ¨le** : x = Wz + Î¼ + Îµ oÃ¹ z ~ N(0, I)

**EM Algorithm** pour estimer W.

---

## 21.4 Robust PCA

**DÃ©composition** : X = L + S
- L : matrice de rang faible
- S : matrice creuse (outliers)

---

## 21.5 Analyse en Composantes IndÃ©pendantes (ICA)

### Objectif

Trouver sources indÃ©pendantes : x = As oÃ¹ s sont indÃ©pendants.

```python
from sklearn.decomposition import FastICA

ica = FastICA(n_components=3)
S = ica.fit_transform(X)  # Sources
A = ica.mixing_  # Matrice de mÃ©lange
```

**Applications** : SÃ©paration de sources (audio, EEG).

---

## 21.6 Factorisation Matricielle Non NÃ©gative (NMF)

**Contrainte** : X â‰ˆ WH avec W, H â‰¥ 0

```python
from sklearn.decomposition import NMF

nmf = NMF(n_components=10, init='random')
W = nmf.fit_transform(X)
H = nmf.components_
```

**Applications** : Traitement d'images, analyse de texte.

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-20-clustering.md) | [Retour](../README.md) | [Suite â¡ï¸](./chapitre-22-visualisation.md)

