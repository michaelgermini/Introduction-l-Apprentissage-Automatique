# Chapitre 22 : Visualisation de DonnÃ©es et Apprentissage de VariÃ©tÃ©s

## ğŸ“š Introduction

Les techniques de visualisation permettent de reprÃ©senter des donnÃ©es haute dimension en 2D ou 3D.

## ğŸ—ºï¸ Carte Mentale : Visualisation

```
              VISUALISATION & MANIFOLDS
                        â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚               â”‚               â”‚
    LINÃ‰AIRE       MANIFOLD         MODERNE
        â”‚               â”‚               â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”       â”Œâ”€â”€â”€â”´â”€â”€â”€â”       â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
  PCA    MDS    Isomap  LLE     t-SNE  UMAP
    â”‚       â”‚       â”‚       â”‚       â”‚       â”‚
Variance Distance GÃ©odÃ©sique Local  KL-div Topologique
Global  Global    Graph    Weights  Probas  Fuzzy Graph
```

## ğŸ“Š Tableau Comparatif : t-SNE vs UMAP vs PCA

| **CritÃ¨re** | **PCA** | **t-SNE** | **UMAP** |
|------------|---------|-----------|----------|
| **LinÃ©aire** | âœ“ Oui | âœ— Non | âœ— Non |
| **ScalabilitÃ©** | âœ“âœ“âœ“ O(npÂ²) | âš ï¸ O(nÂ²) | âœ“âœ“ O(n^1.14) |
| **Vitesse** | TrÃ¨s rapide | Lent | Rapide |
| **Structure globale** | âœ“âœ“ Bonne | âœ— Perdue | âœ“ PrÃ©servÃ©e |
| **Structure locale** | âš ï¸ Moyenne | âœ“âœ“âœ“ Excellente | âœ“âœ“âœ“ Excellente |
| **DÃ©terministe** | âœ“ Oui | âœ— Non | âš ï¸ Quasi |
| **InterprÃ©tabilitÃ©** | âœ“âœ“ Axes = composantes | âœ— Axes arbitraires | âš ï¸ Difficile |
| **Usage** | PrÃ©traitement | Visualisation finale | Moderne, polyvalent |

## ğŸ“ Comparaison Visuelle : PCA vs t-SNE vs UMAP

```
DonnÃ©es Originales (haute dim) :
  Clusters + Topologie complexe

     PCA (2D) :              t-SNE (2D) :            UMAP (2D) :

    â—â—â—    â—‹â—‹â—‹              â—â—â—      â—‹â—‹â—‹          â—â—â—      â—‹â—‹â—‹
     â—      â—‹                 â—        â—‹             â—        â—‹
      â—    â—‹                  â—        â—‹             â—        â—‹
       â—â—â—‹â—‹                    â—â—    â—‹â—‹               â—â—    â—‹â—‹
       â– â– â– â–                        â– â– â– â–                  â– â– â– â– 
        
âœ“ Rapide                    âœ“ Clusters nets        âœ“ Structure prÃ©servÃ©e
âœ— Overlap clusters          âœ— Distances globales   âœ“ Rapide
âœ“ InterprÃ©table             âœ— Non dÃ©terministe     âœ“ Ã‰quilibrÃ©

Observations :
  â€¢ PCA : Projete linÃ©airement, peut mÃ©langer clusters
  â€¢ t-SNE : SÃ©pare bien les clusters mais perd structure globale
  â€¢ UMAP : Meilleur compromis local/global
```

## ğŸ” t-SNE : Intuition

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 ALGORITHME t-SNE                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Ã‰TAPE 1 : ProbabilitÃ©s dans l'espace haute dimension
  Pour chaque paire (i,j), calculer similaritÃ© :
    p_{j|i} âˆ exp(-â€–xáµ¢ - xâ±¼â€–Â²/2Ïƒáµ¢Â²)
    
  SymÃ©trisÃ© : p_{ij} = (p_{j|i} + p_{i|j})/(2n)

Ã‰TAPE 2 : ProbabilitÃ©s dans l'espace 2D
  q_{ij} âˆ (1 + â€–yáµ¢ - yâ±¼â€–Â²)â»Â¹  (distribution t)

Ã‰TAPE 3 : Minimiser KL-divergence
  KL(P||Q) = Î£áµ¢â±¼ p_{ij} log(p_{ij}/q_{ij})
  
  Gradient descent pour ajuster les yáµ¢

RÃ©sultat :
  â€¢ Points proches en haute dim â†’ proches en 2D
  â€¢ Points Ã©loignÃ©s en haute dim â†’ Ã©loignÃ©s en 2D
  â€¢ PrÃ©serve structure locale (voisinage)
```

---

## 22.1 Multidimensional Scaling (MDS)

### Principe

PrÃ©server les distances entre points.

**Objectif** :
```
minimize Î£áµ¢â±¼ (d_ij - â€–y_i - y_jâ€–)Â²
```

```python
from sklearn.manifold import MDS

mds = MDS(n_components=2)
X_embedded = mds.fit_transform(X)
```

---

## 22.2 Apprentissage de VariÃ©tÃ©s

### 22.2.1 Isomap

**Principe** : Utiliser distances gÃ©odÃ©siques (plus court chemin sur le graphe de voisinage).

```python
from sklearn.manifold import Isomap

isomap = Isomap(n_components=2, n_neighbors=10)
X_embedded = isomap.fit_transform(X)
```

### 22.2.2 Locally Linear Embedding (LLE)

**Principe** : PrÃ©server reconstructions locales.

```python
from sklearn.manifold import LocallyLinearEmbedding

lle = LocallyLinearEmbedding(n_components=2, n_neighbors=10)
X_embedded = lle.fit_transform(X)
```

---

## 22.3 t-SNE

**t-Distributed Stochastic Neighbor Embedding**

**Objectif** : PrÃ©server similaritÃ©s locales.

```python
from sklearn.manifold import TSNE

tsne = TSNE(n_components=2, perplexity=30, learning_rate=200)
X_embedded = tsne.fit_transform(X)

plt.scatter(X_embedded[:, 0], X_embedded[:, 1], c=y)
plt.show()
```

**ParamÃ¨tres importants** :
- perplexity : taille de voisinage (5-50)
- learning_rate : pas d'apprentissage

---

## 22.4 UMAP

**Uniform Manifold Approximation and Projection**

**Avantages** :
- Plus rapide que t-SNE
- PrÃ©serve structure globale

```python
import umap

reducer = umap.UMAP(n_components=2, n_neighbors=15, min_dist=0.1)
X_embedded = reducer.fit_transform(X)
```

---

## ğŸ’¡ Comparaison

| MÃ©thode | Temps | PrÃ©serve | Usage |
|---------|-------|----------|--------|
| PCA | Rapide | Global | LinÃ©aire |
| t-SNE | Lent | Local | Visualisation |
| UMAP | Moyen | Local+Global | Visualisation |
| Isomap | Moyen | Distances | VariÃ©tÃ©s |

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](./chapitre-21-reduction-dimension.md) | [Retour](../README.md) | [Suite â¡ï¸](../partie-7-theorie/chapitre-23-generalisation.md)

