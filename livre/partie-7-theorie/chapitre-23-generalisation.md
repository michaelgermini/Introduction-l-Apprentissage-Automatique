# Chapitre 23 : Bornes de GÃ©nÃ©ralisation

## ğŸ“š Introduction

Ce chapitre prÃ©sente les outils thÃ©oriques pour analyser la capacitÃ© de gÃ©nÃ©ralisation des algorithmes d'apprentissage.

## ğŸ—ºï¸ Carte Mentale : ThÃ©orie de l'Apprentissage

```
              GÃ‰NÃ‰RALISATION
                    â”‚
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚           â”‚           â”‚
   COMPLEXITÃ‰   BORNES     SÃ‰LECTION
   (CapacitÃ©)   Probab.    ModÃ¨le
        â”‚           â”‚           â”‚
    â”Œâ”€â”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”€â”´â”€â”€â”€â”   â”Œâ”€â”€â”€â”´â”€â”€â”€â”
    â”‚       â”‚   â”‚       â”‚   â”‚       â”‚
   VC    Rademacher Hoeffding AIC   CV
  Dim.   Complexity  Bounds   BIC   â”‚
    â”‚       â”‚          â”‚      MDL  LOO
 Shatter  Empirical  PAC           k-Fold
           Process
```

## ğŸ“Š Tableau : Bornes de GÃ©nÃ©ralisation

| **MÃ©thode** | **Borne** | **DÃ©pendance** | **UtilitÃ©** |
|------------|-----------|---------------|-------------|
| **Hoeffding** | O(âˆš(log(1/Î´)/n)) | Borne fixe | Classe simple |
| **VC-dimension** | O(âˆš(d log n/n)) | d = VC-dim | Classique |
| **Rademacher** | O(Râ‚™(â„±)) | ComplexitÃ© empirique | Moderne, tight |
| **PAC-Bayes** | O(âˆš(KL(Qâ€–P)/n)) | Prior/Posterior | BayÃ©sien |

## ğŸ“ Visualisation : VC-dimension

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           DIMENSION VC : EXEMPLES                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Demi-espaces en â„Â² : VC = 3

3 points :               4 points :
  â— â— â—                    â— â—
                           â— â—
Toutes les 2Â³=8           Impossible de sÃ©parer
dichotomies rÃ©alisables   â—â—‹  (XOR)
                          â—‹â—

Conclusion : VC(lignes 2D) = 3

Perceptron â„áµˆ : VC = d + 1

PolynÃ´mes degrÃ© p : VC = O(p^d)

RÃ©seaux neurones : VC = O(W log W)
  oÃ¹ W = nombre de poids
```

## ğŸ¯ Borne de GÃ©nÃ©ralisation : Visualisation

```
Erreur R(f)
     â”‚
     â”‚   â•±â”€â”€â”€â”€â”€ Borne supÃ©rieure
  1.0â”‚  â•±       R(f) â‰¤ RÌ‚(f) + âˆš(d log n / n)
     â”‚ â•±   â•±â”€â”€â”€
     â”‚â•±   â•±      Risque rÃ©el R(f)
  0.5â”‚   â•±
     â”‚  â•±â”€â”€â”€â”€â”€â”€â”€â”€â”€ Risque empirique RÌ‚(f)
     â”‚
   0 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ n (taille)
       10   100   1000  10000

Observations :
  â€¢ Gap â†“ quand n â†‘  (plus de donnÃ©es)
  â€¢ Gap â†‘ quand d â†‘  (modÃ¨le complexe)
  â€¢ Trade-off biais-variance
```

---

## 23.1 Notation

**Risque rÃ©el** : R(f) = ğ”¼[L(Y, f(X))]
**Risque empirique** : RÌ‚(f) = (1/n) Î£áµ¢ L(yáµ¢, f(xáµ¢))

**Objectif** : Borner |R(fÌ‚) - RÌ‚(fÌ‚)|

---

## 23.2 CritÃ¨res d'Information

### 23.2.1 AIC (Akaike Information Criterion)

```
AIC = -2 log L + 2k
```

oÃ¹ k est le nombre de paramÃ¨tres.

### 23.2.2 BIC (Bayesian Information Criterion)

```
BIC = -2 log L + k log n
```

**PropriÃ©tÃ©** : BIC pÃ©nalise plus la complexitÃ© pour n grand.

---

## 23.3 InÃ©galitÃ©s de Concentration

### 23.3.1 InÃ©galitÃ© de Hoeffding

Pour Xâ‚, ..., Xâ‚™ i.i.d. bornÃ©es dans [a, b] :
```
P(|XÌ„ - ğ”¼[X]| â‰¥ Îµ) â‰¤ 2 exp(-2nÎµÂ²/(b-a)Â²)
```

### 23.3.2 Variables Sous-Gaussiennes

X est **sous-gaussienne** si :
```
ğ”¼[exp(Î»X)] â‰¤ exp(Î»Â²ÏƒÂ²/2)
```

**PropriÃ©tÃ©** : Les variables bornÃ©es sont sous-gaussiennes.

---

## 23.4 Dimension VC

### DÃ©finition

La **dimension VC** d'une classe â„± est le plus grand n tel que â„± peut rÃ©aliser toutes les 2â¿ dichotomies sur n points.

**Exemples** :
- Demi-espaces en â„áµˆ : VC = d + 1
- Perceptron : VC = d + 1

### ThÃ©orÃ¨me de Vapnik

Avec probabilitÃ© 1 - Î´ :
```
R(fÌ‚) â‰¤ RÌ‚(fÌ‚) + O(âˆš(VC log(n/VC) / n))
```

---

## 23.5 Nombres de Couverture

### DÃ©finition

Le **nombre de couverture** N(Îµ, â„±, d) est le nombre minimal de boules de rayon Îµ pour couvrir â„±.

**Entropie mÃ©trique** : H(Îµ) = log N(Îµ)

### InÃ©galitÃ©

Avec probabilitÃ© 1 - Î´ :
```
|R(f) - RÌ‚(f)| â‰¤ O(âˆš(H(Îµ/n) / n))
```

---

## 23.6 ComplexitÃ© de Rademacher

### DÃ©finition

```
R_n(â„±) = ğ”¼_Ïƒ[sup_{fâˆˆâ„±} (1/n) Î£áµ¢ Ïƒáµ¢ f(xáµ¢)]
```

oÃ¹ Ïƒáµ¢ âˆˆ {-1, +1} alÃ©atoires.

### Borne de GÃ©nÃ©ralisation

Avec probabilitÃ© 1 - Î´ :
```
R(f) â‰¤ RÌ‚(f) + 2R_n(â„±) + O(âˆš(log(1/Î´) / n))
```

---

## 23.7 Application Ã  la SÃ©lection de ModÃ¨les

### Principe du Risque Structurel

Pour â„±â‚ âŠ‚ â„±â‚‚ âŠ‚ ... :
```
fÌ‚ = argmin_{fâˆˆâ„±_k} [RÌ‚(f) + Pen(k)]
```

**PÃ©nalitÃ©** : Pen(k) âˆ Complexity(â„±_k)

---

## ğŸ’¡ Points ClÃ©s

1. **Trade-off biais-variance** : FormalisÃ© par les bornes
2. **ComplexitÃ©** : VC-dimension, nombres de couverture, Rademacher
3. **Concentration** : Hoeffding, sous-gaussiennes
4. **SÃ©lection de modÃ¨les** : AIC, BIC, validation croisÃ©e

---

## ğŸ“ RÃ©sumÃ© du Livre

Ce livre a couvert :

### Partie I : Fondements
- AlgÃ¨bre linÃ©aire et optimisation
- Outils mathÃ©matiques essentiels

### Partie II : Concepts
- PrÃ©diction et Ã©valuation
- Noyaux et espaces de Hilbert

### Partie III : Apprentissage SupervisÃ©
- RÃ©gression et classification
- Arbres, forests, boosting
- RÃ©seaux de neurones

### Partie IV : ModÃ¨les Probabilistes
- Graphes et infÃ©rence
- MCMC et variational inference

### Partie V : MÃ©thodes GÃ©nÃ©ratives
- VAE, GAN, Flows

### Partie VI : Non SupervisÃ©
- Clustering et rÃ©duction de dimension
- Visualisation

### Partie VII : ThÃ©orie
- Bornes de gÃ©nÃ©ralisation
- Analyse thÃ©orique

---

## ğŸ“ Conclusion

Le machine learning est un domaine riche combinant thÃ©orie et pratique. Ce livre a fourni les fondements mathÃ©matiques et les algorithmes pour :

- Comprendre les principes sous-jacents
- ImplÃ©menter des algorithmes
- Analyser leur comportement
- DÃ©velopper de nouvelles mÃ©thodes

**Bonne continuation dans votre apprentissage du ML ! ğŸš€**

---

[â¬…ï¸ Chapitre prÃ©cÃ©dent](../partie-6-non-supervise/chapitre-22-visualisation.md) | [ğŸ  Retour Ã  l'accueil](../README.md)

