# Chapitre 12 : Comparaison de Distributions de Probabilit√©

## üìö Introduction

Ce chapitre pr√©sente diff√©rentes fa√ßons de mesurer la similarit√© ou la distance entre distributions de probabilit√©.

---

## 12.1 Distance de Variation Totale

**D√©finition** :
```
TV(P, Q) = sup_{A} |P(A) - Q(A)|
         = (1/2) ‚à´ |p(x) - q(x)| dx
```

**Propri√©t√©s** :
- 0 ‚â§ TV(P, Q) ‚â§ 1
- TV(P, Q) = 0 ‚ü∫ P = Q

---

## 12.2 Divergences

### Divergence de Kullback-Leibler

```
KL(P || Q) = ‚à´ p(x) log(p(x)/q(x)) dx
           = ùîº_P[log(p(X)/q(X))]
```

**Propri√©t√©s** :
- KL ‚â• 0 (in√©galit√© de Gibb's)
- KL = 0 ‚ü∫ P = Q
- **Non sym√©trique** !

**Applications** :
- Maximum de vraisemblance
- VAE (Variational Autoencoders)

### Divergence de Jensen-Shannon

```
JS(P || Q) = (1/2)KL(P || M) + (1/2)KL(Q || M)
```

o√π M = (P + Q)/2

**Propri√©t√©s** :
- Sym√©trique
- 0 ‚â§ JS ‚â§ log 2

---

## 12.3 Distance de Wasserstein

**D√©finition** (1-Wasserstein) :
```
W_1(P, Q) = inf_{Œ≥ ‚àà Œì(P,Q)} ‚à´‚à´ d(x,y) dŒ≥(x,y)
```

**Interpr√©tation** : Co√ªt minimal pour transformer P en Q.

**Applications** :
- GANs (Wasserstein GAN)
- Optimal transport

---

## 12.4 Distances Duales

**f-divergence** : Famille g√©n√©rale de divergences.

---

[‚¨ÖÔ∏è Partie 3](../partie-3-apprentissage-supervise/chapitre-11-reseaux-neurones.md) | [Retour](../README.md) | [Suite ‚û°Ô∏è](./chapitre-13-monte-carlo.md)

